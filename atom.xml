<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浙江大学超算队</title>
  
  <subtitle>Zhejiang University Supercomputer Team</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjusct.github.io/"/>
  <updated>2019-05-31T15:25:27.713Z</updated>
  <id>https://zjusct.github.io/</id>
  
  <author>
    <name>ZJU · SCT</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>生成式对抗网络_基础</title>
    <link href="https://zjusct.github.io/2019/05/31/GAN_Introduction/"/>
    <id>https://zjusct.github.io/2019/05/31/GAN_Introduction/</id>
    <published>2019-05-30T16:00:00.000Z</published>
    <updated>2019-05-31T15:25:27.713Z</updated>
    
    <content type="html"><![CDATA[<p>Reference :  <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Machine Learning and having it deep and structured (2018,Spring)</a> （李宏毅）</p><h1 id="Introduction-of-GAN"><a href="#Introduction-of-GAN" class="headerlink" title="Introduction of GAN"></a>Introduction of GAN</h1><p>生成式对抗网络（Generative Adversarial Network，GAN），是2014年被提出来的一种生成式深度学习网络技术。</p><p>由于在众多应用场合中的出色表现，近年来GAN的应用和研究已经成为一个非常热门的方向。</p><p><img src="0.png" alt="研究热度"></p><p>这篇文章将会对GAN做一个直观的介绍，帮助读者理解GAN的Basic Idea。</p><p>第一节中，我们将简单对机器学习的Basic Ideas做一个回顾。第二节和第三节会简单介绍一下什么是生成（Generation）。第四节和第五节会介绍两种生成模型。第六节我们将会看到GAN是如何结合这两种模型从而达到优秀的表现。</p><h2 id="Review-for-ML"><a href="#Review-for-ML" class="headerlink" title="Review for ML"></a>Review for ML</h2><p>在之前的文章（ <a href="http://zjuqxy.xyz/?p=1081" target="_blank" rel="noopener">机器学习</a> / <a href="http://122.152.198.128/?p=1532>" target="_blank" rel="noopener">卷积神经网络</a>）中，我们介绍了一些机器学习（Machine Learning）的思想和例子。</p><p>简单回顾一下在这些文章中的讨论，我们知道，<font color="red">机器学习本质上就是一个映射拟合 </font>:</p><p>在这个过程中，模型（Model）是一个函数，我们希望对它输入一个自变量x，能从它那里得到一个我们想要的输出y。以房价预测为例，我们希望告诉这个模型一些当前的经济，政治等社会环境，它能反馈一个房价数值。以图像分类为例，我们希望告诉这个模型一张图片，它能反馈给我们这张图片属于什么类别。</p><p>在最简单的形式中，我们用一个简单的线性模型（Linear Model） $w*x=y$ 来建模这个函数。高级一点，选择一个更复杂的核函数（另一种说法也可以称特征函数）的话，这个线性模型可以表达更加复杂的关系。再高级一点，把这些线性模型多层复合起来，形成神经网络（Neural Network），表达力就更强，而这个用深度神经网络来建模的方法自成一派，也就是所谓的深度学习（Deep Learning）。</p><p>而常常所说的训练数据（Training Data）无非就是从真实的世界中采样的一组（x，y）样本。训练（Training）则是让我们的模型从训练数据中学习x和y之间存在的某种规律的模式（Pattern），模型的参数在<font color="red">学习过程中</font>不断调整，最后模型代表的函数尽可能的去模拟了真实世界中的映射关系。</p><p>这个学习过程又是怎样的呢？通常就是定义了一个损失函数（Loss Function），这个损失函数的值是基于模型预测出来的结果和我们采样出来的真实结果的差别。<font color="red">所谓学习其实就是最小化这个差别。因此，所谓的学习，本质上就是这个函数最优化问题的一种拟人化表达而已。</font></p><p>因此机器学习实际上就是给定一个训练数据集，产生一个预测函数。这个预测函数能对输入的x给出期望的y，简单的可以表示为这样：</p><p><img src="2.png" alt="ML示意图"></p><h2 id="What-Is-Generation"><a href="#What-Is-Generation" class="headerlink" title="What Is Generation"></a>What Is Generation</h2><p>机器学习中，有很多种类型的任务，比如最简单的回归（Regression），分类（Classification）等。</p><p>它们是机器学习入门时肯定会接触的例子，常见的回归/分类任务比如：给定一张图片，输出一个标量或者向量描述这个图片的性质。</p><p>而生成（Generation）实际上就可以简单的理解为上述任务的一种逆向过程。给机器一张图片，它给你一个向量描述它的类别或者特征，似乎很无聊？那么如果给机器一个向量后，它给你生成一张图片(<del>老婆</del>)呢？</p><p><img src="1.png" alt="用向量生成图片"></p><p><img src="4.png" alt="生成器根据向量变化产生不同图片"></p><p>这就是生成任务最直观最简单的例子。（除了生成图片以外，当然它也可以用来生成句子/音频等其它任何你能想到的东西，只要能保证可训练。）</p><p>如上图所示，只需要调整对生成器的输入向量，它就会相应的生成不同的图片。P2中增大了第一维的输入，生成的人物头发变长了；P3中改变向量倒数第二维的输入，人物头发变成了蓝色；P4中增大了向量的最后一维，生成的人像就变得笑口常开了。</p><h2 id="Why-Generation"><a href="#Why-Generation" class="headerlink" title="Why Generation"></a>Why Generation</h2><p>一个很自然的问题是：为什么我们需要生成？给我一个向量，我生成一张图片有什么用？</p><p>如果只是给一个很枯燥的向量，让机器生成一张图片确实没啥用。但是如果再组合上其它的功能模块，这个技术可能会非常有用。</p><p><img src="21.png" alt></p><p>比如说，我们可以不从向量开始，而从一个句子开始。训练这样一个模型，自然语言先被转成向量编码，向量编码再被转成这个自然语言句子描述的画面。</p><p><img src="22.png" alt></p><p>又比如说，我们可以从图片中修改一部分区域，画出一个简单的草图，它就自动帮我们生成更加真实的修改内容。</p><p><img src="23.png" alt></p><p>你甚至还可以根据马云的脸的表情信息的编码生成一张小罗伯特·唐尼同样表情的脸，然后替换上去，产生真实的换脸效果。</p><p>生成，在某种意义上来说，是人工智能最有意义的任务。回归和分类这样的任务仅仅是去理解和识别我们熟知的对象，比如识别一句话是不是脏话，识别一张图片是不是猫。但是生成却是化腐朽为神奇，仅仅根据一些非常空洞的描述就自动生成了我们人所熟知的对象，这才是我们真正最想让人工智能为我们做的。仅仅是理解和识别，人工智能终究只是作为一种辅助工具。但是当它能创造的时候，它就翻身做了主人。</p><p><img src="9.png" alt="连续性"></p><p>上图是一个向量生成图片的例子。向量按照一种顺序在逐渐变化，我们而可以看到第四行生成的人脸也在变化。惊人的是，它的生成竟然是一个连续的将脸变换一个朝向的过程。这说明，这个模型确实学到了这个人的脸的深刻特征，能根据向量的微调，对产生的结果也生成连贯一致的微调。</p><p>不只是绘画，当它还能生成语言，生成电影，生成代码，生成电路设计，生成科学理论时，它体现出来的智能性是非常让人震撼的。学习到对象/概念的深层次内涵，并且能够使用这些学到的知识去创造未知的东西，这是人工智能的究极目标之一。</p><h2 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p><img src="7.png" alt="Generator"></p><p>上图是生成模型的一种基本模式，我们希望输入一个向量得到一个目标输出作为图形。</p><p>一个简单问题是，我们如何训练它？容易想到的是事先准备一堆向量和图片之间的映射对，让这个生成器学得某种映射模式，如下图所示：</p><p><img src="8.png" alt="训练数据样例"></p><p>但面临的一个现实问题是，用于训练的图片很容易得到，但是这个向量code如何得到？需要我们手工设置吗？对于一些大型的项目，这个向量一般维度很高，样例也很多，手工设置这些向量标签可操作性并不强。并且，如果设计的向量不好，也很难保证模型收敛。</p><p>自编码器在一定程度上解决了这个问题：</p><p><img src="5.png" alt></p><p><img src="6.png" alt="Auto-encoder"></p><p>如图，自编码器接收一张图片作为输入，先丢入一个神经网络中产生一个编码作为输出（一个高维向量），这个产生的编码再被丢入另外一个神经网络，恢复成图片。我们希望恢复出来的图片尽可能和输入的图片相近，于是可以定义一个loss来衡量图片在这一次编码和解码过程中的损失，根据这个loss，再使用像梯度下降之类的方法，就可以调整encoder和decoder的参数，使得其尽可能减少损失。</p><p>在这样一个自编码，自解码的过程中，我们就既有了编码器和解码器。我们可以只对解码器输入向量，就能得到一个生成图。需要注意到的是，在这样一个自编码器模型中，code和image的映射完全是由模型生成的。模型会自动帮我们调优这个映射的划分。</p><p><font color="red">PS : </font>这个code的长度是一个超参数，我们需要设定一个适合的编码长度。一方面，它的长度小于原图才能达到我们希望的提取特征的效果。另一方面，如果它的长度太小，可能无法容纳图片中必须的一些关键信息，导致无法还原。</p><h3 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>自动编码器的优点很显然，只需要给模型展示一些正例，它就能学到编码与图片之间的映射，非常易于训练和生成。</p><p>上面我们说了，我们在训练一个自编码器的时候，希望的是输出图片和输入图片尽可能的相近，减少损失。这个相似如何去衡量呢？一般采用的方法都是把图片当作高维向量处理，求两个向量之间距离的L1或者L2范数。这个范数就表征了两张图片之间的差别，而这样的衡量方式局限性很明显：</p><p><img src="10.png" alt="简单范数的局限性"></p><p>如上图所示，如果我们让机器生成了四个版本的“2”的图片。其中v1和v2与原图只差了一个像素点，v3和v4查了足足6个像素点。但是v1和v2破绽很明显，v1在尾巴上出现了一个孤立的橘色块，而v2在中间留了一块白出来，这都是不自然的。而v3和v4虽然像素点差距更大，但是其实只是尾巴和头分别拉长了一点罢了，这都是很正常的情况。所以，用这种标准训练出来的模型，它显然缺少一个大局观。只是机械的分别关注全局对象的每一个小组成部分，很难学到组成部分之间的关联。</p><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><h3 id="Basic-Idea-1"><a href="#Basic-Idea-1" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p><img src="11.png" alt="评估器"></p><p>评估器接收一个对象输入，产生一个标量输出。（其实就是一个回归问题）如上图所示，以生成图片的任务为例，一个评估器会评估机器生成的图片是否和人类作出来的图有一样的效果，根据生成质量，评估器会给出一个0~1的评分。</p><p>假设我们已经有了一个很好的评估器，我们也可以用拿它来生成图片。给定一个向量，我们通过一些事先构造的假设，将其可能对应的图片锁定在一个集合（可能会很大）中。图片生成就成为了在这个集合中枚举，筛选出一个得分最高的图片：</p><p><img src="12.png" alt="枚举筛选"></p><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>一个实际的问题是，如何训练一个好的评估器？它怎么能分辨出哪些是画得好的，哪些是画得糟糕的？</p><p><img src="13.png" alt></p><p><img src="14.png" alt></p><p>在网上，我们可以下载一大堆艺术家绘制的图片，他们都可以拿来作为正例，用来让模型学习什么是“好”图片。但是如果全都是正例，那么评估器将会收敛为一个恒输出1的常函数。所以，要训练一个评估器，面临的一个实际问题是：如何获得好的反例？</p><p>如果反例的采样分布没有选好，那么模型对反例的识别能力也会很差。就会像上图那样，一个画得很一般的图片却获得了0.9的高分。</p><p>我们可以通过一个迭代算法来不断强化这个评估器：</p><p><img src="15.png" alt="迭代优化"></p><p>如上图所示。一开始，我们可以随机生成一批反例，然后我们用当前的正例和反例训练出来一个评估器。评估器是个什么东西呢？它本质上也是一个定义在图片域上的函数。我们现在就从这个定义域上采集一批评分很高的图片来作为反例，用新的反例结合之前的正例重新训练，得到下一个版本的评估器。最后不断迭代。</p><p><img src="16.png" alt="迭代示意图"></p><p>上图是一个迭代过程的示意图，绿色为真实图片的分布域，蓝色为生成的效果糟糕的图片的分布域，红色曲线是我们的评估函数。在第一轮的训练中，真实域的函数估值会被拉高，我们随机生成那些噪音图片所在的域的股指会被压低。但是这个估值函数可能还不够，它可能没有发现其它很糟糕的图片的分布域，给了它们很高的估值，我们称为“虚高”。所以我们会进行第二轮训练，第二轮中用到的反例是从第一轮训练出来的估值函数中采样的估值高的点，那些“虚高”的点因为被作为反例被压低，而那些本来就该得分高的点即使被采样为反例，但是因为有同区域的正例在支撑，所以这一区域的估值不会被压低。最后，多轮迭代后，那些虚高的点就会逐渐被压平，只有正例分布的区域还依旧坚挺。此时我们就认为得到的评估函数是优秀的。</p><p>这就像是制定一部法律。我们希望人们的行为被约束在一个我们预先定义的可接受的范围内。但是法律有漏洞，总有人钻空子，在制定者期望之外的区域获得很高的收益。法律制定者就会锁定这些突出的领域专门指定新的条令来打击。经过不断的实践和修订，这部法律也就完善起来。</p><h3 id="Pros-and-Cons-1"><a href="#Pros-and-Cons-1" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>估值器的优点正好弥补了自动编码器的缺点。自动编码器是一个Button Up的模型，它根据编码对每一个像素点单独生成，组成一个完整的更高层的结果，这样的Button Up模型很难学到不同组成部分之间的关联。而估值器是一个Top Down模型，它直接获得正常图片作为输入，用全连接网络或者卷积网络很容易学习图片中的一些局部以及全局特征。</p><p>但是缺点也很明显。从上面看到，我们发现评估器很依赖于一个argmax的计算。如果要用它来生成模型，我们需要在一个域上找到得分最高的候选图。如果要用迭代法来训练一个评估器，我们也需要在当前的评估函数域采样估值高的点来作为新的反例。然而这个argmax怎么算呢？很多用discriminator来做生成的，都要事先做一些不准确的假设，根据这些假设对不同的输入向量在图片域中划分一个候选域，在这个可行域上做argmax。并且有的时候需要大量的枚举，非常耗时。并且要训练评估器也不容易，光有正例还不行，还得自己去合理的生成反例。</p><p>所以，显而易见，评估器虽然有了自编码器难以学得的全局观，但是却没了自编码器那种易于生成结果的特性。</p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>讨论了这么多，终于轮到主人公GAN登场了。</p><p>前面我们先讨论了Auto-encoder和Discriminator，分别介绍了用它们来做生成的基本想法。在讨论中，我们已经意识到了：Auto-encoder很容易产生生成图像，但是由于是button-up的模型，根据编码分别对每一个像素点预测，很难学得像素点之间的关系；Discriminator很容易学得图像中一些局部和全局的特征细节，可以很好的处理像素点之间的correlation，但是其主要能力还是在做评估上，在生成方面它还是缺乏一个有效的生成手段。</p><p>GAN集成了这两个方法：一方面，GAN使用一个类似Auto-encoder结构的Generator来生成图像，另一方面它用一个Discriminator来评估生成图片的质量。</p><h3 id="Basic-Idea-2"><a href="#Basic-Idea-2" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>考虑这样的一个过程：</p><p>一开始我们已经有了一个generator和discriminator，它们的参数都是随机设置的。generator生成的图片很糟糕，discriminator也无法判别什么图片是好的。</p><p>然后我们可以以一个适当的概率分布随机向这个generator中输入一组向量，然后得到一堆生成的图片，用这些图片作为反例，用艺术家绘制的图片作为正例训练discriminator。这轮训练后，得到的discriminator的能力得到了提升，能够学会给一些好的图片打高分，给一些差的图片打低分。</p><p><img src="18.png" alt="固定Generator，强化Discriminator"></p><p>这之后，我们再固定这个discriminator的参数。此时如果我们给generator输入一个向量，再把它产生的图片送入discriminator中，我们会得到一个反馈的分数。这个反馈分数就可以作为LOSS，我们根据LOSS FUNCTION的梯度调整generator的参数，使得它尽可能产生可以骗过这个版本的discriminator，从它手下得到一个高分。这轮训练后，得到的generator的能力也得到了提升，能够产生一些像样的图片了。</p><p><img src="19.png" alt="固定Discriminator，强化Generator"></p><p>然后我们又重复上面的过程，强化discriminator，discriminator强化后再强化generator。。。可以期望的是，多轮迭代后，我们的generator和discriminator都可以变得很强。</p><p><img src="17.png" alt="GAN的哲学"></p><p>上图很好的从生物进化的角度很好的揭示了GAN的哲学。一开始蝴蝶颜色五颜六色的，停在树上的时候，捕食它的鸟根据颜色是否是棕色来区分它和叶子。在这个过程中，那些五颜六色的蝴蝶被淘汰了，棕色的蝴蝶脱颖而出，成功骗过了初代鸟。但是那些被骗过的初代鸟也会被淘汰，于是鸟也在这个过程中进化，学会了通过判别是否有叶脉来寻找猎物。而蝴蝶在这一过程中再次进化，成为了枯叶蝶。。。这是自然界中经典的良性竞争，互相强化的例子。而Generative Adversarial Network中的“Adversarial”也就是这样来的。在GAN中，我们让一个generator和一个discriminator互相对抗，generator努力的生成逼真的图片试图骗过discriminator，discriminator努力强化自己的辨别能力对抗generator的欺骗。模型的学习就是在两者的对抗之中互相强化而完成的。</p><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="20.png" alt="GAN"></p><p>上图是GAN的基本训练模式。$\theta_d$ 和 $\theta_g$ 分别是Discriminator和Generator的参数，一开始是随意初始化的。</p><p>每一轮中：</p><p>首先从数据库中采样m个真实样本${x_i}$，再让Generator随机生成m个虚假样本${\hat{x_i}}$。</p><p>定义 $V=\frac{1}{m}[;\Sigma_{i=1}^{m}logD(x_i)+\Sigma_{i=1}^{m}log(1-D(\hat{x}_i));]$</p><p>真实样本得分越高，虚假样本得分越低，这个V的值越大。于是我们只需要通过梯度上升法，最大化这个V，我们的Discriminator就能尽可能的对真实样本给出高分，尽可能的给虚假样本给出低分。这实际上就是完成了一个对Discriminator的强化。</p><p>在这之后，我们再取样m个随机向量${z_i}$，丢入Generator，再把生成的结果给Discriminator评估。</p><p>定义$V=\frac{1}{m}\Sigma_{i=1}^mlog(D(G(z_i)))$</p><p>同样的，我们希望最大化这个V值，使得产生的结果尽可能骗过Discriminator。将V值对 $\theta_g $ 求偏导，使用梯度上升来最大化这个V，从而实现对Generator的调优。</p><h3 id="Comparision"><a href="#Comparision" class="headerlink" title="Comparision"></a>Comparision</h3><p>在GAN的设计中，我们可以看到其明显优于Auto-encoder和Discriminator的地方。Auto-encoder生成之后，缺乏一个强大评估反馈，简单的采用原图和恢复生成图的差向量范数来评估，无法考虑到各个component之间的correlation，缺乏一个大局观。Discriminator虽然能对图片产生很深的特征理解，但是缺少一个高效准确的proposal手段，训练时它非常依赖于反例的生成，生成时又需要选择一个候选区域来执行argmax，这都不是容易解决的问题。</p><p>GAN将这两者的特性做了一个结合。虽然还是采用button-up的生成方式，根据向量对每一个像素点单独生成，但是多了一个up-down评估模式的discriminator来对生成内容进行更加细致的评估反馈，从而generator能得到更好的训练指引。另一方面，由于有了一个更加强大的generator，训练discriminator时，能获得质量更高的反例，从而discriminator也能得到更好的训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Reference :  &lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Machine Learning and having
      
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>2019.5.26 会议记录</title>
    <link href="https://zjusct.github.io/2019/05/27/MeetingReport20190526/"/>
    <id>https://zjusct.github.io/2019/05/27/MeetingReport20190526/</id>
    <published>2019-05-27T03:24:31.000Z</published>
    <updated>2019-05-31T15:04:27.471Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="输入密码以查看文章" />    <label for="pass">输入密码以查看文章</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      这是一篇加密文章，仅供超算队内部阅读。
    
    </summary>
    
    
      <category term="MeetingReport" scheme="https://zjusct.github.io/tags/MeetingReport/"/>
    
      <category term="ZJUSCT" scheme="https://zjusct.github.io/tags/ZJUSCT/"/>
    
  </entry>
  
  <entry>
    <title>基于0-1乘性噪声的朴素图片降噪</title>
    <link href="https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/"/>
    <id>https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/</id>
    <published>2019-05-10T16:00:00.000Z</published>
    <updated>2019-05-31T15:04:27.426Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h2><p>给定3张受损图像，尝试恢复他们的原始图像。</p><ol><li>原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。</li><li>受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。</li><li>噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。</li></ol><p><strong>评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。</strong></p><a id="more"></a><br><h2 id="实现介绍"><a href="#实现介绍" class="headerlink" title="实现介绍"></a>实现介绍</h2><br><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>由于图片的像素点在空间上满足局部相似的特征，相邻的像素点通道值变化往往是平滑且有一定规则的。因此，我们可以用一个模型来拟合像素点通道值在空间上的关系。具体实现中，我们将图片切割成若干个小矩形块，然后使用一个二维线性回归模型来回归每个小矩形块中位置和像素通道值的函数关系。为了使结果更加平滑和可靠，我们采用了高斯函数作为基函数。</p><br><h3 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h3><p>当我们对一个 ss * ss 的像素矩阵块做回归的时候，我们要把所有没有被噪音损坏的点都提取出来。点位置用一个高斯核函数处理，这样原来的点坐标(x,y)就被转换成了$(e^{-\frac{(x-mid)^2}{2}},e^{-\frac{(y-mid)^2}{2}})$，在特征空间中用来刻画这个点与矩阵块中心的距离。这样，我们实际要回归的就是点心距与像素通道值的关系。</p><p>这样做，主要是因为我们的局部性原理本身就是不带方向性的，所谓的局部性就是指临近的点存在某种平滑的变化关系。使用这样一个衡量距离的核函数，可以使得我们的回归结果更加平滑：</p><p><img src="1.png" alt="Non-Gaussian"><img src="2.png" alt="Gaussian"></p><p>上图左边是用坐标直接回归，右图是坐标经过高斯核处理后回归的结果。可以看到左边有大量的不平滑的交错的黑白点，看起来很“脏”。右边由于采用了高斯函数处理过的表征距离关系的核函数，结果更加平滑，清晰。</p><p>CODE : </p><p><img src="0.png" alt="CODE OF GAUSSIAN KERNEL"></p><br><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>提取完特征后的数据点拟合，就是一个简单的线性回归任务而已，我们采用最小二乘法回归。<br>$$<br>Loss = \frac {\Sigma_{i=0}^n(y_i-\phi(x_i) * w^T)^2}{n}=\frac{\Sigma_{i=1}^{n}Loss_i}{n}<br>$$<br>使用随机梯度下降来最优化损失函数：<br>$$<br>w\leftarrow w-\eta * \triangledown Loss_i=w+2\eta(y_i-\phi(x_i) * w^T) * \phi(x_i)<br>$$<br>具体实现中，我们取步长=0.005，进行100轮随机梯度下降。</p><p>CODE : </p><p><img src="3.png" alt="CODE OF TRAINING"></p><p>​    <br></p><h3 id="迭代降噪"><a href="#迭代降噪" class="headerlink" title="迭代降噪"></a>迭代降噪</h3><p>我们前面提到了，我们要把图片切成若干个小矩形块，对每个小矩形块分别进行回归，那么这个小矩形块的尺寸取多少比较合适呢？</p><p>我们先取ss=2尝试一下：</p><p><img src="4.png" alt="SS=2"></p><p>噪音为0.8的时候，我们可以看到，如果取一个2*2的块，期望其中没损坏的通道只有0.8个，所以势必有大量的矩阵块里面都是全损坏的，这会使得一些全损坏的块得不到修复，产生大量的黑点。</p><p>直接取ss=5：</p><p><img src="5.png" alt="SS=5"></p><p>显然，黑块的数量变少了，但是实际上图片给人的颗粒赶很明显，更像是一堆模糊的马赛克拼图拼凑而成的。</p><p>我们的解决办法是先取ss=2，对图片做恢复，然后将恢复的图像再用更大的ss来恢复。</p><p>下面是用ss={2，3，4，5}迭代恢复四次的过程：</p><p><img src="6.png" alt="SS-&gt;2"><img src="7.png" alt="SS-&gt;2-&gt;3">    </p><p><img src="8.png" alt="SS-&gt;2-&gt;3-&gt;4"><img src="9.png" alt="SS-&gt;2-&gt;3-&gt;4-&gt;5"></p><p>可以看到黑点逐渐被消除，并且最后经过ss=5的回归后得到的结果在视觉效果上明显优于直接用ss=5进行回归。这种想法本质上是一种<font color="red">贪心算法</font>。先将损失密度小的局部块恢复好，再充分利用之前修复出来的信息将损失密度更大的块修复。</p><br><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>A （noise rate = 0.8）</p><p><img src="10.png" alt="NR=0.8_原图"><img src="9.png" alt="NR=0.8_修复图"></p><p>B（noise rate = 0.4）</p><p><img src="11.png" alt="NR=0.4_原图"><img src="12.png" alt="NR=0.4_修复图"></p><p>C（noise rate = 0.6）</p><p><img src="13.png" alt="NR=0.6_原图"><img src="14.png" alt="NR=0.6_修复图"></p><p>D（根据原图自己生成，测试迭代过程中损失的减少）</p><p><img src="15.png" alt="误差测试"></p><br><h2 id="潜在的优化展望"><a href="#潜在的优化展望" class="headerlink" title="潜在的优化展望"></a>潜在的优化展望</h2><ol><li>由于每个块的修复是独立的，可以考虑使用CPU多线程计算或者在GPU上用CUDA进行并行优化，加速整个修复过程。</li><li>使用更复杂的网络来拟合。</li><li>使用马尔科夫随机场的方法来做图像降噪。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目内容&quot;&gt;&lt;a href=&quot;#项目内容&quot; class=&quot;headerlink&quot; title=&quot;项目内容&quot;&gt;&lt;/a&gt;项目内容&lt;/h2&gt;&lt;p&gt;给定3张受损图像，尝试恢复他们的原始图像。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。&lt;/li&gt;
&lt;li&gt;受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。&lt;/li&gt;
&lt;li&gt;噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Virtual Memory and TLB</title>
    <link href="https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/"/>
    <id>https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/</id>
    <published>2019-03-31T06:05:11.000Z</published>
    <updated>2019-05-31T15:04:27.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h2><p>x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址</p><ul><li>其中高2G<code>0x80000000 ~ 0xFFFFFFFF</code>为内和空间，由操作系统调用；</li><li>低2G<code>0x00000000 ~ 0x7FFFFFFF</code>为用户空间，由用户使用。</li></ul><p>在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。</p><a id="more"></a><p>虚拟地址空间是一种通过机制映射出来的空间，与实际物理空间大小无必然联系，在x86保护模式下，无论计算及实际主存是512MB还是8GB，虚拟地址空间总是4GB，<strong>这是由CPU和操作系统的宽度决定的</strong>，即：</p><blockquote><p>CPU地址总线宽度 → 物理地址范围<br>CPU的ALU宽度 → 操作系统位数 → 虚拟地址范围</p></blockquote><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>虚拟地址空间 = 主存 + 虚拟内存(交换空间 Swap Space)</p><p>虚拟内存：将硬盘的一部分作为存储器使用，来扩充物理内存。</p><p>利用了自动覆盖、交换技术。内存中存不下、暂时不用的内容会存在硬盘中。</p><blockquote><p>Assume: 32位操作系统，32位寻址总线宽度 → 4G线性空间</p></blockquote><h2 id="保护模式下的进程运行"><a href="#保护模式下的进程运行" class="headerlink" title="保护模式下的进程运行"></a>保护模式下的进程运行</h2><p>虚拟地址空间是硬件行为，CPU自动完成(同时与操作系统协作)虚拟地址到物理地址(可能差熬过实际内存，这样会产生一个异常中断，揭晓来有操作系统处理(如从虚拟内存中调出对应的页框内容))。</p><p>所以，一个程序若运行在保护模式下，其汇编级、机器语言级的寻址都是用的虚拟地址，即在一般的编程中不会接触到物理一层。</p><p>在进程被加载时，系统为进程建立唯一的数据结构<code>进程控制块(PCB = Process Control Block)</code>，直至进程结束。</p><p>PCB中描述了该进程的现状以及控制运行的全部信息，有了PCB，一个进程才可以在保护模式下和其他进程一起被并发地运行起来，操作系统通过PCB对进程进行控制。</p><p>PCB中的程序ID(PID(unix、linux)、句柄(windows))是进程的唯一标识；PCB中的一个指针指向 <strong>页表</strong> ，这些都与地址转化有关。</p><h2 id="地址转化"><a href="#地址转化" class="headerlink" title="地址转化"></a>地址转化</h2><p>地址转化的全过程可以用以下这张图来概括：</p><p><img src="OG.png" alt="OG"></p><p>以下是具体步骤介绍。</p><h3 id="1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留"><a href="#1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留" class="headerlink" title="1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)"></a>1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)</h3><ul><li><p>段内偏移地址(32位)</p></li><li><p>段选择符：16位长的序列，是索引值，定位段描述符；结构：<br><img src="%232.png" alt="#2"></p><ul><li>高13位为表内索引号 —— 但注意由于GDT第一项留空，所以索引要先加1；</li><li>而2位为TI表指示器，0是指GDT，1是指LDT；</li><li>0、1位是RPL请求者特权级，00最高，11最低 —— 在x86保护模式下修改寄存器是系统之灵，必须有对应的权限才能修改(当前执行权限和段寄存器中(被修改的)的RPL均不低于目标段的RPL)</li></ul></li><li><p>段描述符：8x8=64位长的结构，用来描述一个段的各种属性。结构：<br> <img src="%231.png" alt="#1"></p><ul><li>0、1字节+6字节低4位(20位) 段边界/段长度：最大1MB或者4G(看粒度位的单位)</li><li>2、3、4、7字节(32位) 段基址：4G线性地址的任意位置(不一定非要被16整除)</li><li>6、7字节的奇怪设计是为了兼容80286(24位地址总线)</li><li>剩下的那些是段属性，详见<code>20180819143434</code></li></ul></li><li><p>段描述表：多任务操作系统中，含有多个任务，而每个人物都有多个段，其段描述符存于段描述表中。<br>IA-32处理器有3个段描述表：GDT、LDT和IDT。</p><ul><li>GDT(Global Descripter Table) 全局段描述符表：一个系统一般只有一个GDT，含有每一个任务都可以访问的段；通常包含操作系统所使用的代码段、数据段和堆栈段，GDT同时包含各进程LDT数据段项，以及进程间通讯所需要的段。<br>GDTR是CPU提供的寄存器，存储GDT的位置和边界；在32位模式下RGDT有48位长(高32位基地址+低16位边界)，在32e模式下有80位长(高64位基地址+低16位边界)。<br>GDT的第一个表项留空不用，是空描述符，所以索引号要加1。<br>GDT最多128项。</li><li>LDT(Local Descripter Table) 局部段描述符表：16位长，属于某个进程。一个进程一个LDT，对应有RLDT寄存器，进程切换时RLDT改变。<br>RLDT和RGDT不一样，RLDT是一个索引值而不是实际指向，指向GDT中某一个LDT描述项。所以如果要获取LDT中的某一项，先要访问GDT找到对应LDT，再找到LDT中的一项。<br>编译程序时，程序内赋予了虚拟页号。在程序运行时，通过对应LDT转译成物理地址。故虚拟页号是局部性的、不同进程的页号会有冲突。<br>LDT没有空选择子。</li><li>IDT(Interrupt Descripter Table) 中断段描述符表；一个系统一般也只有一个。</li><li>以下这个图能做一点解释：<br><img src="%237.png" alt="#7"></li></ul></li></ul><h3 id="2-线性地址-→-物理地址-页式内存管理"><a href="#2-线性地址-→-物理地址-页式内存管理" class="headerlink" title="2. 线性地址 → 物理地址 (页式内存管理)"></a>2. 线性地址 → 物理地址 (页式内存管理)</h3><p>这一步由CPU的页式管理单元来负责转换。——MMU(内存管理单元)。</p><ul><li><p>线性地址可以拆分为三部分(或者两部分)：<br><img src="%233.png" alt="#3"></p></li><li><p>页(Page)：线性地址被划分为大小一致的若干内存区域，其对应映射到大小相同的与物理空间区域页框(Frame)上。这个映射不一定是连贯而有序的。</p></li><li><p>CR3：页目录基址寄存器。对于每一个进程，CR3的内容不同(有点像RLDT)，页目录基址也不同，线性地址-物理地址的映射也不同。</p></li><li><p>页目录：占用一个4kb的内存页，最多存储1024个页目录表项(PDE)，一个PDE有4字节。在没启用PAE时，有两种PDE，规格不同。</p></li><li><p>页目录表项(PDE)：每个程序有多个页表，即拥有多个PDE。PDE的结构如下：<br><img src="%234.png" alt="#4"><br>12~31位(20位)表示页表起始物理地址的高20位(页表基址低12位为0，即一定以4kb对齐)。</p></li><li><p>页表：一个页表占4kb的内存页，最多存储1024个页表项(PTE)，一个PTE是4字节。页表的基址是4kb对齐的，低12位是0。</p></li></ul><p>采用对页表项的二级管理模式(也目录→页表→页)能够节约空间。因为不存在的页表就可以不分配空间，并且对于Windows来说只有一级页表才会存在主存中，二级可以存在辅存中——不过Linux中它们都常驻主存。</p><p>一些CPU会提供更多级的架构，如三级、四级。Linux中，有对应的高层次抽象，提供了一个四层页管理架构：<br><img src="%236.png" alt="#6"><br>把中间的某几个定为长度为0，就可以调整架构级数。如“四化二”：某地址0x08147258，对应的PUD、PMD里只有一个表项为PUD→PMD，PMD→PT；划分的时候，PGD=0000100000，PUD=PMD=0，PT=0101000111.</p><h3 id="3-TLB-转换检测缓冲区、快表、转译后被缓冲区"><a href="#3-TLB-转换检测缓冲区、快表、转译后被缓冲区" class="headerlink" title="3. TLB (转换检测缓冲区、快表、转译后被缓冲区)"></a>3. TLB (转换检测缓冲区、快表、转译后被缓冲区)</h3><p>处理器中，一个具有并行朝赵能力的特殊高速缓存器，存储最近访问过的一些页表项(时空局部性原理，减少页映射的内存访问次数)。</p><p>TLB较贵，通常能够存放16~512个页表项。</p><ul><li><p>TLB命中：直接取出对应的页表项</p></li><li><p>TLB缺失：先淘汰TLB中的某一项(TLB替换策略，一些算法，可以由硬件或软件来实现)</p><ul><li>硬件处理TLB Miss：CPU会遍历页表，找到正确的PTE；如果没有找到，CPU就会发起一个页错误并将控制权交给操作系统。</li><li>软件处理TLB Miss：CPU直接发出未命中错误，让操作系统来处理。</li></ul></li><li><p>脏记录：当TLB中某个PTE项失效(如切换进程、进程退出、虚拟页换出到磁盘)，PTE标记为不存在，此时映射已经不成立了。<br>操作系统要保证即时刷新掉这些脏记录，不同的CPU有不同的刷新TLB方法，但每次都完全刷新TLB会很慢，所以现在有一些策略，扩展对一个PTE的描述(如针对某个进程、空间的标识，如果目前进程与PTE相关，就会忽略掉)，这样可以让多个进程同时共存TLB</p></li></ul><h2 id="Linux-段式管理"><a href="#Linux-段式管理" class="headerlink" title="Linux 段式管理"></a>Linux 段式管理</h2><p>Linux似乎没有理会Intel的那一套段的机制，而是做了一个高级的抽象。<br>Linux对所有的进程使用了相同的段来对指令和数据寻址，让每个段寄存器都指向同一个段描述符，让这个段描述符的基址为0，长度为4G。即用这种方式略去了段式内存管理。<br>对应多有用户代码段、用户数据段、内核代码段和内核数据段。可以在<code>segment.h</code>中看到，四种段对应的段基址都是0，这就是“平坦内存模型”，这样就有<code>段内偏移地址=逻辑地址</code></p><p>且，四种段对应的都为GDT。即Linux大多数情况都不使用LDT，除非使用wine等Windows防真程序。</p><p>Linux 0.11中每个进程划分64MB的虚拟内存空间。故逻辑地址范围为0~0x4000000</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;虚拟地址空间&quot;&gt;&lt;a href=&quot;#虚拟地址空间&quot; class=&quot;headerlink&quot; title=&quot;虚拟地址空间&quot;&gt;&lt;/a&gt;虚拟地址空间&lt;/h2&gt;&lt;p&gt;x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中高2G&lt;code&gt;0x80000000 ~ 0xFFFFFFFF&lt;/code&gt;为内和空间，由操作系统调用；&lt;/li&gt;
&lt;li&gt;低2G&lt;code&gt;0x00000000 ~ 0x7FFFFFFF&lt;/code&gt;为用户空间，由用户使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Operating System" scheme="https://zjusct.github.io/tags/Operating-System/"/>
    
  </entry>
  
  <entry>
    <title>如何在墙内快速部署CentOS 7的MySQL</title>
    <link href="https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/"/>
    <id>https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/</id>
    <published>2019-03-31T02:45:19.000Z</published>
    <updated>2019-05-31T15:04:27.470Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。</p><p>其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。</p><p>本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。</p><a id="more"></a><h2 id="传统方案……慢到怀疑人生"><a href="#传统方案……慢到怀疑人生" class="headerlink" title="传统方案……慢到怀疑人生"></a>传统方案……慢到怀疑人生</h2><p>根据<a href="https://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="noopener">官方指南</a>，我们执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源</span></span><br><span class="line">wget <span class="string">"https://dev.mysql.com/get/mysql80-community-release-el7-2.noarch.rpm"</span></span><br><span class="line"><span class="comment"># 安装源</span></span><br><span class="line">sudo rpm -ivh mysql80-community-release-el7-2.noarch.rpm</span><br><span class="line"><span class="comment"># 检查源是否成功安装</span></span><br><span class="line">sudo yum repolist enabled | grep <span class="string">"mysql80-community*"</span></span><br></pre></td></tr></table></figure><p>接下来就是正常的安装步骤：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install mysql-community-server mysql</span><br></pre></td></tr></table></figure><p>但是由于一些原因，下载速度基本是几Byte/s，MySQL 服务器的大小(加上依赖服务)差不多有600MB，这种方法基本不可取。手头没有特别好的而且很新的软件源，就打算手动安装。</p><h2 id="手动安装法"><a href="#手动安装法" class="headerlink" title="手动安装法"></a>手动安装法</h2><p>首先依然需要下载并安装官方源。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql-community-server</span><br></pre></td></tr></table></figure><p>利用该命令我们可以获取一些 MySQl Server 以来安装顺序及其版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">=================================================================================================</span><br><span class="line"> Package                       Arch          Version              Repository                Size</span><br><span class="line">=================================================================================================</span><br><span class="line">Reinstalling:</span><br><span class="line"> mysql-community-client        x86_64        8.0.15-1.el7         mysql80-community         25 M</span><br><span class="line"></span><br><span class="line"> mysql-community-libs          x86_64        8.0.15-1.el7         mysql80-community          2 M</span><br><span class="line"></span><br><span class="line"> mysql-community-common        x86_64        8.0.15-1.el7         mysql80-community        570 K</span><br><span class="line"></span><br><span class="line"> mysql-community-server        x86_64        8.0.15-1.el7         mysql80-community        360 M</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">=================================================================================================</span><br></pre></td></tr></table></figure><p>解压并分析rpm源包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm2cpio mysql80-community-release-el7-2.noarch.rpm | cpio -div</span><br><span class="line">vim /etc/yum.repos.d/mysql-community.repo</span><br></pre></td></tr></table></figure><p>从中我们可以找到对应版本的网络路径为<code>http://repo.mysql.com/yum/mysql-8.0-community/el/7/x86_64/</code>。</p><p>打开该地址，找到对应的几个安装包：</p><ul><li>mysql-community-client-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-libs-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-common-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-server-8.0.15-1.el7.x86_64.rpm</li></ul><p>使用某种下载工具(我使用的是迅雷)下载，然后使用<code>scp</code>指令上传到服务器上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp mysql-community-client-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-libs-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-common-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-server-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>按照先后顺序依次执行<code>yum</code>本地安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo yum localinstall mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">sudo yum -y install mysql</span><br></pre></td></tr></table></figure><p>安装成功，启动并测试服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br><span class="line">systemctl status mysqld.service</span><br></pre></td></tr></table></figure><p>找出默认密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep &quot;password&quot; /var/log/mysqld.log &gt;&gt; defalut_mysql_passwd.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。&lt;/p&gt;
&lt;p&gt;其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。&lt;/p&gt;
&lt;p&gt;本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="MySQL" scheme="https://zjusct.github.io/tags/MySQL/"/>
    
      <category term="CentOS" scheme="https://zjusct.github.io/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>BP算法</title>
    <link href="https://zjusct.github.io/2018/12/24/bp/"/>
    <id>https://zjusct.github.io/2018/12/24/bp/</id>
    <published>2018-12-24T13:03:59.000Z</published>
    <updated>2019-05-31T15:04:27.488Z</updated>
    
    <content type="html"><![CDATA[<p>本文探讨BP算法。</p><a id="more"></a><h1 id="Feed-forward-neural-network-and-back-propagation"><a href="#Feed-forward-neural-network-and-back-propagation" class="headerlink" title="Feed forward neural network and back propagation"></a>Feed forward neural network and back propagation</h1><h2 id="1-Neuron-structure"><a href="#1-Neuron-structure" class="headerlink" title="1. Neuron structure"></a>1. Neuron structure</h2><p><img src="neuron.png" alt="Neuron structure"></p><p>上图是一种典型的神经元结构，$x_n$是神经元的输入，将输入加权求和后再通过激活函数即可得到此神经元的输出：<br>$$t = \sum_{i=1}^{n}{w_ix_i} + b$$<br>$$a = f(t)$$</p><p>为计算方便，可将偏置$b$提到求和符号里面，相当于加入一个恒为1的输入值，对应的权重为$b$：<br>$$t = \sum_{i=0}^{n}{w_ix_i},(x_0 = 1, w_0 = b)$$<br>$$a = f(t)$$<br>此即为上图神经元结构对应的表达式</p><p>常用的激活函数有sigmoid, ReLU, tanh等。</p><h2 id="2-Network-structure"><a href="#2-Network-structure" class="headerlink" title="2. Network structure"></a>2. Network structure</h2><p><img src="bp_net.jpg" alt="Network Structure"></p><p>这是一个简单的3层网络，输入层有3个输入值，隐藏层包含3个隐藏神经元，最后是两个输出值<br>隐藏层神经元的前向计算过程：</p><p>$$z_i^{l} = \sum_{i=0}^{n}w_{ij}^{l}x_j, (x_0 = 1, w_0 = b)$$</p><p>$$a_i^l = f(z_i^l)$$</p><p>$l$表示第几层。</p><p>这个网络的抽象数学表达式为：<br>$$F(x) = f_3(f_2(x * W_2 + b_2) * W_3 + b_3)$$</p><p>事实上，深度神经网络一般都能够抽象为一个复合的非线性多元函数，有多少隐藏层就有多少层复合函数：<br>$$F(x) = f_n\left(\dots f_3(f_2(f_1(x) * w_1 + b_1) * w_2 + b_2)\dots\right)$$</p><h2 id="3-Loss"><a href="#3-Loss" class="headerlink" title="3. Loss"></a>3. Loss</h2><p>Loss，即损失，用来衡量神经网络的输出值与实际值的误差，对于不同的问题，通常会定义不同的loss函数</p><p>回归问题常用的均方误差：<br>$$MSE = \frac{1}{n}\sum_{i=1}^{n}(Y - f(x))^2$$<br>$Y$为实际值，$f(x)$为网络预测值</p><p>分类问题常用的交叉熵(m类)：<br>$$L = \sum_{k=1}^{n}\sum_{i=1}^{m}l_{ki}log(p_{ki})$$<br>$l_{ki}$表示第k个样本实际是否属于第i类（0，1编码），$p_{ki}$表示第k个样本属于第i类的概率值</p><p>特别地，二分类问题的交叉熵损失函数形式为：<br>$$L = \sum_{i=1}^{n}[y_ilog(p_i) + (1 - y_i)log(1 - p_i)]$$<br>$y_i$为第i个样本所属类别，$p_i$为第i个样本属于$y_i$类的概率</p><h2 id="4-Back-propagation"><a href="#4-Back-propagation" class="headerlink" title="4. Back propagation"></a>4. Back propagation</h2><p>BP 是用来将loss反向传播的算法，用来调整网络中神经元间连接的权重和偏置，整个训练的过程就是：前向计算网络输出<br>-&gt;;根据当前网络输出计算loss-&gt;BP算法反向传播loss调整网络参数，不断循环这样的三步直到loss达到最小或达到指定停止条件</p><p>BP算法的本质是求导的链式法则，对于上面的三层网络，假设其损失函数为$C$，激活函数为$\sigma$，第$l$第$i$个神经元的输入为$z_i^{(l)}$，输出为$a_i^{(l)}$</p><p>则通过梯度下降来更新权值和偏置的公式如下：<br>$$W_{ij}^{(l)} = W_{ij}^{(l)} - \eta\frac{\partial}{\partial W_{ij}^{(l)}}C\tag1$$<br>$$b_{i}^{(l)} = b_{i}^{(l)} - \eta\frac{\partial}{\partial b_{i}^{(l)}}C\tag2$$</p><p>$W_{ij}^{(l)}$表示第$l$层第$i$个神经元与第$l - 1$层第$j$个神经元连接的权值，$b_i^{(l)}$表示第$l$层第$i$个神经元的偏置</p><p>$\eta$表示学习率</p><p>由更新公式可见主要问题在于求解损失函数关于权值和偏置的偏导数</p><p>第$l$层第$i$个神经元的输入$z_i^{(l)}$为：<br>$$z_i^{(l)} = \sum_{j=1}^{n^{(l-1)}}{W_{ij}^{(l)}a_j^{(l-1)}} + b_i^{l}\tag3$$</p><p>则更新公式中偏导项可化为:</p><p>$$\frac{\partial}{\partial W_{ij}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial W_{ij}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}} \bullet a_i^{(l-1)}\tag4$$</p><p>$$\frac{\partial}{\partial b_{i}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial b_{i}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}}\tag5$$</p><p>定义</p><p>$$\delta_i^{(l)} = \frac{\partial}{\partial z_i^{(l)}}C\tag6$$</p><p>现在问题转化为求解$\delta_i^{(l)}$，对第$l$层第$j$个神经元有：<br>$$<br>\delta_j^{(l)} = \frac{\partial C}{\partial z_j^{(l)}} = \sum_{i=1}^{n^{(l+1)}}\frac{\partial C}{\partial z_i^{(l+1)}} \bullet \frac{\partial z_i^{(l+1)}}{\partial a_j^{(l)}} \bullet \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet \frac{\partial(W_{ij}^{l+1} + b_i^{(l+1)})}{\partial a_j^{(l)}} \bullet \sigma^\prime(z_j^{(l)})\<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet W_{ij}^{(l+1)} \bullet \sigma^\prime(z_j^{(l)})\tag7<br>$$</p><p>则：<br>$$\delta^{(l)} = ((W^{(l+1)})^T\delta^{(l+1)})\odot\sigma^\prime(z^{(l)})\tag8$$</p><p>损失函数关于权重和偏置的偏导分别为：<br>$$\frac{\partial C}{\partial W_{ij}^{(l)}} = a_i^{(l-1)}\delta_i^{(l)}\tag9$$<br>$$\frac{\partial C}{\partial b_{i}^{(l)}} =\delta_i^{(l)}\tag{10}$$</p><p>误差根据8式由输出层向后传播，再结合1，2，9，10四式对权重和偏置进行更新</p><h2 id="5-实现"><a href="#5-实现" class="headerlink" title="5.实现"></a>5.实现</h2><p>下面是一个简单3隐层神经网络的实现</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum((pred - y) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_prime</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pred - y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">network</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_layers, output_size, loss = loss, loss_prime = loss_prime)</span>:</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation function</span></span><br><span class="line">        self.activation = self.sigmoid</span><br><span class="line">        <span class="comment"># derivative of activation function</span></span><br><span class="line">        self.activation_prime = self.sigmoid_prime</span><br><span class="line">        <span class="comment"># loss funciton</span></span><br><span class="line">        self.loss = loss</span><br><span class="line">        <span class="comment"># derivative of loss function</span></span><br><span class="line">        self.loss_prime = loss_prime</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input-&gt;hidden</span></span><br><span class="line">        self.w_ih = np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.b_ih = np.random.randn(<span class="number">1</span>, hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        self.W_hh = [np.random.randn(hidden_size, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line">        self.B_hh = [np.random.randn(<span class="number">1</span>, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden-&gt;output</span></span><br><span class="line">        self.w_ho = np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.b_ho = np.random.randn(<span class="number">1</span>, output_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assemble w and b</span></span><br><span class="line">        self.W = [self.w_ih]</span><br><span class="line">        self.W.extend(self.W_hh)</span><br><span class="line">        self.W.append(self.w_ho)</span><br><span class="line"></span><br><span class="line">        self.B = [self.b_ih]</span><br><span class="line">        self.B.extend(self.B_hh)</span><br><span class="line">        self.B.append(self.b_ho)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># activation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(x) * (<span class="number">1</span> - self.sigmoid(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass, calculate the output of the network</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            a = self.activation(np.dot(a, w) + b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backpropagate error</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get output of each layer in forward pass</span></span><br><span class="line">        out = x</span><br><span class="line">        outs = []</span><br><span class="line">        zs = []</span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            z = np.dot(out, w) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            out = self.activation(z)</span><br><span class="line">            outs.append(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># δ of last layer</span></span><br><span class="line">        delta = self.loss_prime(outs[<span class="number">-1</span>], y) * self.activation_prime(zs[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        delta_b[<span class="number">-1</span>] = delta</span><br><span class="line">        delta_w[<span class="number">-1</span>] = np.dot(outs[<span class="number">-2</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(delta_w)):</span><br><span class="line">            delta = np.dot(delta, self.W[-i+<span class="number">1</span>].transpose()) * self.activation_prime(zs[-i])</span><br><span class="line">            delta_b[-i] = delta</span><br><span class="line">            delta_w[-i] = np.dot(outs[-i<span class="number">-1</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> delta_w, delta_b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update w and b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, batch, lr)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> batch:</span><br><span class="line">            d_w, d_b = self.backward(x, y)</span><br><span class="line">            delta_w = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_w, d_w)]</span><br><span class="line">            delta_b = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_b, d_b)]</span><br><span class="line"></span><br><span class="line">        self.W = [w - lr * t <span class="keyword">for</span> w, t <span class="keyword">in</span> zip(self.W, delta_w)]</span><br><span class="line">        self.B = [b - lr * t <span class="keyword">for</span> b, t <span class="keyword">in</span> zip(self.B, delta_b)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SGD training</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, epochs, batch_size, lr)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">            np.random.shuffle(train_data)</span><br><span class="line">            batches = [train_data[t : t + batch_size] <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, len(train_data), batch_size)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">                self.update(batch, lr)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> train_data:</span><br><span class="line">                loss += self.loss(self.forward(x), y)</span><br><span class="line">            loss /= len(train_data)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"Epoch %d done, loss: %f"</span> % (i + <span class="number">1</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.forward(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># use it for handwriting digits classification</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onehot</span><span class="params">(y)</span>:</span></span><br><span class="line">    arr = np.zeros([y.shape[<span class="number">0</span>], <span class="number">10</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        arr[i][y[i]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line">x_train = x_train.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">x_test = x_test.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">y_train = onehot(y_train)</span><br><span class="line">y_test = onehot(y_test)</span><br><span class="line"></span><br><span class="line">train_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_train, y_train)]</span><br><span class="line">test_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_test, y_test)]</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">3</span></span><br><span class="line">output_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">net = network(input_size, hidden_size, num_layers, output_size)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.005</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">net.train(train_data, epochs, batch_size, lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    exp = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> exp / np.sum(exp)</span><br><span class="line"></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> test_data:</span><br><span class="line">    ret = net.forward(x)</span><br><span class="line">    pred = softmax(ret)</span><br><span class="line">    <span class="keyword">if</span> np.argmax(pred) == np.argmax(y):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">acc = float(correct) / len(test_data)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, acc)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文探讨BP算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Machine Learning" scheme="https://zjusct.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Tensorflow</title>
    <link href="https://zjusct.github.io/2018/12/23/tensorflow/"/>
    <id>https://zjusct.github.io/2018/12/23/tensorflow/</id>
    <published>2018-12-23T11:57:25.000Z</published>
    <updated>2019-05-31T15:04:27.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-单机"><a href="#1-单机" class="headerlink" title="1.单机"></a>1.单机</h2><h3 id="log-device-placement"><a href="#log-device-placement" class="headerlink" title="log_device_placement"></a>log_device_placement</h3><p>单机情况比较简单，不需要特殊配置，TensorFlow会自动将计算任务分配到可用的GPU上，在定义session时，可以通过<em>log_device_placement</em>参数来打印具体的计算任务分配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="1.png"><h3 id="指定设备"><a href="#指定设备" class="headerlink" title="指定设备"></a>指定设备</h3><p>如果需要让一些运算在特定的设备上执行，可以使用tf.device:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="2.png"><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>尽管上面一个例子中我们只给CPU和GPU0指定了计算任务，但是两块显卡的显存都被占满了：</p><img src="3.png"><p>因为TensorFlow会默认占满所有可见GPU的显存，对于简单的计算任务，这样显然非常浪费，我们可以通过修改环境变量<em>CUDA_VISIBLE_DEVICES</em>解决这个问题:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 运行时指定环境变量</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python demo.py</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python 代码中修改环境变量</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>]=<span class="string">'0'</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="2-多机"><a href="#2-多机" class="headerlink" title="2.多机"></a>2.多机</h2><h3 id="In-graph-amp-Between-graph"><a href="#In-graph-amp-Between-graph" class="headerlink" title="In-graph &amp; Between-graph"></a>In-graph &amp; Between-graph</h3><p>TensorFlow的分布式训练有两种模式：In-graph和Between-graph</p><p>In-graph: 不同的机器执行计算图的不同部分，和单机多GPU模式类似，一个节点负责模型数据分发，其他节点等待接受任务，通过<em>tf.device(“/job:worker/task:n”)</em>来指定计算运行的节点</p><img src="5.png"><p>Between-graph:每台机器执行相同的计算图</p><blockquote><p>Author: 陈岩<br>PostDate: 2018.12.21</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-单机&quot;&gt;&lt;a href=&quot;#1-单机&quot; class=&quot;headerlink&quot; title=&quot;1.单机&quot;&gt;&lt;/a&gt;1.单机&lt;/h2&gt;&lt;h3 id=&quot;log-device-placement&quot;&gt;&lt;a href=&quot;#log-device-placement&quot; cla
      
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Tensorflow" scheme="https://zjusct.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Quick Guide to CUDA Profiling</title>
    <link href="https://zjusct.github.io/2018/12/07/cuprof/"/>
    <id>https://zjusct.github.io/2018/12/07/cuprof/</id>
    <published>2018-12-07T15:48:33.000Z</published>
    <updated>2019-05-31T15:04:27.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Brief-Introduction"><a href="#1-Brief-Introduction" class="headerlink" title="1. Brief Introduction"></a>1. Brief Introduction</h2><p>在并行计算领域，很难通过纯理论的分析来确定程序的性能，<code>GPGPU</code>这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。</p><p><code>Nvidia</code>提供了<code>nvprof</code>，<code>nvvp</code>，<code>Nsight</code>三种cuda可用的性能分析工具，本文将简述配合使用<code>nvprof</code>与<code>nvvp</code>的cuda程序性能分析方法。</p><a id="more"></a><h2 id="2-Check-Out-Device-Properties"><a href="#2-Check-Out-Device-Properties" class="headerlink" title="2. Check Out Device Properties"></a>2. Check Out Device Properties</h2><p>由于cuda程序的线程/块分配方案与程序运行的的硬件高度相关，故对目标平台的硬件参数有一定程度的了解是相当有必要的。我们可以使用<code>cudaGetDeviceProperties()</code>函数获取设备的各项属性，下述代码可以结合<code>cuda_runtime_api.h#1218</code>处<code>struct cudaDeviceProp</code>的定义和各属性的相应注解自行理解。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nDevices;</span><br><span class="line">cudaDeviceProp prop;</span><br><span class="line">cudaGetDeviceCount( &amp;nDevices );</span><br><span class="line"><span class="keyword">for</span> ( <span class="keyword">auto</span> i = <span class="number">0</span>; i != nDevices; ++i )</span><br><span class="line">&#123;</span><br><span class="line">cudaGetDeviceProperties( &amp;prop, i );</span><br><span class="line"><span class="comment">// check out interesting property</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-Profile-Using-Nvprof"><a href="#3-Profile-Using-Nvprof" class="headerlink" title="3. Profile Using Nvprof"></a>3. Profile Using Nvprof</h2><h3 id="3-1-Quick-Start"><a href="#3-1-Quick-Start" class="headerlink" title="3.1. Quick Start"></a>3.1. Quick Start</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="3-2-Metrics"><a href="#3-2-Metrics" class="headerlink" title="3.2. Metrics"></a>3.2. Metrics</h3><ul><li>使用<code>--query-metrics</code>列出所有可测试的性能指标。</li><li>使用<code>--metrics sm_efficiency,warp_execution_efficiency,...</code>指定要测试的性能指标。</li></ul><h3 id="3-3-PC-Sampling"><a href="#3-3-PC-Sampling" class="headerlink" title="3.3. PC Sampling"></a>3.3. PC Sampling</h3><p>在CC5.2或更高的设备上支持使用PC采样(PC sampling)技术。</p><p>PC采样技术通过<code>Round-Robin</code>方法对SM中所有活动线程束的PC状态进行采样，采样结果包含如下两种可能：</p><ul><li>线程束完成了当前指令。</li><li>线程束被<code>stall</code>，不能完成当前指令，并可以给出<code>stall</code>的原因。</li></ul><p>事实上线程束被<code>stall</code>并不代表指令流水线处于<code>stall</code>状态，因为其他正常运行的线程束可以利用计算资源。</p><p>CC6.0以上的设备对PC采样方法进行了改进，通过检查线程束调度器是否执行指令来确定指令流水线是否真正处于<code>stall</code>状态，从而能正确指示指令<code>stall</code>的原因。</p><h2 id="4-Data-Visualize-Using-Nvvp"><a href="#4-Data-Visualize-Using-Nvvp" class="headerlink" title="4. Data Visualize Using Nvvp"></a>4. Data Visualize Using Nvvp</h2><p><code>nvvp</code>可以导入<code>nvprof</code>的分析结果，可视化显示统计图表，并且建议性地指出程序可能存在的瓶颈。</p><p><em>以饼状图显示各类stall比重</em></p><img src="a.jpg"><p><em>以频谱显示各类指令比例</em></p><img src="b.jpg"><p><em>通过source file mapping可视化指令stall状态，需要在编译选项中指定<code>-lineinfo</code></em></p><img src="d.jpg"><h3 id="4-1-Usage"><a href="#4-1-Usage" class="headerlink" title="4.1. Usage"></a>4.1. Usage</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvprof -f --kernels <span class="string">"kernelName"</span> --analysis-metrics -o a.nvvp &lt;task&gt; &lt;args&gt;</span><br><span class="line">nvvp a.nvvp</span><br></pre></td></tr></table></figure><p>这里我使用的方法是在集群上用<code>nvprof</code>做性能测试，之后将分析结果<code>*.nvvp</code>传回本地用<code>nvvp</code>做可视化。</p><h2 id="Ext-Remarks"><a href="#Ext-Remarks" class="headerlink" title="Ext. Remarks"></a>Ext. Remarks</h2><h3 id="Tradeoff-Between-Registers-and-Threads"><a href="#Tradeoff-Between-Registers-and-Threads" class="headerlink" title="Tradeoff Between Registers and Threads"></a>Tradeoff Between Registers and Threads</h3><p>在实际Profiling中重新认识了这个问题。</p><p>在默认情况下，<code>nvcc</code>为每个线程分配<code>maxRegsPerThread</code>个数的寄存器，在<em>Tesla K40</em>上，这个值为64。同时，每个SM持有为65536个寄存器，这意味着单个SM中的线程数最多不超过1024。通过检查参数表，我们发现该设备单个SM可容纳线程数为2048。这意味着我们计算任务的GPU利用率最大只有50%（所有SM均满载的状态下）。</p><p>在这种情况下，如果我们将分配给单个线程的寄存器数目减半，则最大GPU利用率可以达到100%。但若发生寄存器溢出（register spilling），溢出的存储空间被放到片外的local memory，访问速度在（同在片外的）global memory级别。</p><p>在实际的CUDA核函数中，能全部利用64个寄存器的情况很少。寄存器的使用情况可以在nvvp中检查，如果发现有大量寄存器浪费，可以立即减少寄存器数量。在大多数情况下，可以结合计算任务的量级和性质来调节线程最大寄存器数，从而达到有针对性的性能调优。</p><p>在<code>nvcc</code>中指定单个线程最大寄存器数，可以添加编译选项<code>-maxrregcount=N</code>。如果限定不修改编译选项或需要逐核函数指定，则需要使用<code>__launch_bounds__</code>限定符，如下（隐式地指定了最大寄存器个数）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)</span><br><span class="line">MyKernel(...)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在我的<code>path tracer</code>中对上述方法进行测试，将每线程的寄存器数减半为32，SM线程数加倍并满载，GPU利用率由30+提升到70+，执行速度有1.5倍左右的提升。</p><h3 id="Tradeoff-Between-BlockDim-and-BlockPerSM"><a href="#Tradeoff-Between-BlockDim-and-BlockPerSM" class="headerlink" title="Tradeoff Between BlockDim and BlockPerSM"></a>Tradeoff Between BlockDim and BlockPerSM</h3><p>当一个块（block）中的所有线程束（warp）全部完成时，这个块才可以被SM调度。如果块的大小过大，则块的运行速度受单个线程束约束的开销就越大（如果算法并行度很高，增大块的大小不失为一个好选择）；如果块的大小过小，则一方面SM可能无法达到其最大利用率（受<code>maxBlocksPerSM</code>的限制），另一方面SM调度块的额外开销也会增大。尤其是针对不同特点的计算任务有不同的更优选择，如<code>divergency</code>较高的任务更适合较小的BlockDim。所以在选择BlockDim时不仅要在算法的适应性上做考虑，还要通过多次性能测试来进行针对性的优化。</p><h3 id="Beware-of-Ladder-Effects"><a href="#Beware-of-Ladder-Effects" class="headerlink" title="Beware of Ladder Effects"></a>Beware of Ladder Effects</h3><p>注意计算资源分配时要注意分配的资源量要能够被组别整除，否则会出现断层状的资源浪费现象。</p><p><em>每块线程数与SM中最大线程束数的关系</em></p><img src="c.jpg">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Brief-Introduction&quot;&gt;&lt;a href=&quot;#1-Brief-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Brief Introduction&quot;&gt;&lt;/a&gt;1. Brief Introduction&lt;/h2&gt;&lt;p&gt;在并行计算领域，很难通过纯理论的分析来确定程序的性能，&lt;code&gt;GPGPU&lt;/code&gt;这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Nvidia&lt;/code&gt;提供了&lt;code&gt;nvprof&lt;/code&gt;，&lt;code&gt;nvvp&lt;/code&gt;，&lt;code&gt;Nsight&lt;/code&gt;三种cuda可用的性能分析工具，本文将简述配合使用&lt;code&gt;nvprof&lt;/code&gt;与&lt;code&gt;nvvp&lt;/code&gt;的cuda程序性能分析方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
      <category term="Profile" scheme="https://zjusct.github.io/tags/Profile/"/>
    
  </entry>
  
  <entry>
    <title>CUDA内存管理总结(一)</title>
    <link href="https://zjusct.github.io/2018/11/25/cuda/"/>
    <id>https://zjusct.github.io/2018/11/25/cuda/</id>
    <published>2018-11-24T16:46:08.000Z</published>
    <updated>2019-05-31T15:04:27.493Z</updated>
    
    <content type="html"><![CDATA[<p>本文将探讨CUDA中的内存管理机制。</p><a id="more"></a><h2 id="一、寄存器"><a href="#一、寄存器" class="headerlink" title="一、寄存器"></a>一、寄存器</h2><p>​    GPU的每个SM（流多处理器）都有上千个寄存器，每个SM都可以看作是一个多线程的CPU核，但与一般的CPU拥有二、四、六或八个核不同，一个GPU可以有<strong>N个SM核</strong>；同样，与一般的CPU核支持一到两个硬件线程不同，每个SM核可能有<strong>8~192个SP</strong>（流处理器），亦即每个SM能同时支持这么多个硬件线程。事实上，一台GPU设备的所有SM中活跃的线程数目通常数以万计。</p><h3 id="1-1-寄存器映射方式"><a href="#1-1-寄存器映射方式" class="headerlink" title="1.1 寄存器映射方式"></a>1.1 寄存器映射方式</h3><p>​    <strong>CPU处理多线程</strong>：进行上下文切换，使用寄存器重命名机制，将当前所有寄存器的状态保存到栈（系统内存），再从栈中恢复当前需要执行的新线程在上一次的执行状态。这些操作通常花费上百个CPU时钟周期，有效工作吞吐量低。</p><p>​    <strong>GPU处理多线程</strong>：与CPU相反，GPU利用多线程隐藏了内存获取与指令执行带来的延迟；此外，GPU不再使用寄存器重命名机制，而是尽可能为每个线程分配寄存器，从而上下文切换就变成了寄存器组选择器（或指针）的更新，几乎是零开销。</p><h3 id="1-2-寄存器空间大小"><a href="#1-2-寄存器空间大小" class="headerlink" title="1.2 寄存器空间大小"></a>1.2 寄存器空间大小</h3><p>​    每个SM可提供的寄存器空间大小分别有8KB、16KB、32KB和64KB，每个线程中的每个变量占用一个寄存器，因而总共会占用N个寄存器，N代表调度的线程数量。当线程块上的寄存器数目是允许的最大值时，每个SM会只处理一个线程块。</p><h3 id="1-3-SM调度线程、线程块"><a href="#1-3-SM调度线程、线程块" class="headerlink" title="1.3 SM调度线程、线程块"></a>1.3 SM调度线程、线程块</h3><p>​    由于大多数内核对寄存器的需求量很低，所以可以通过降低寄存器的需求量来增加SM上线程块的调度数量，从而提高运行的线程总数，根据线程级并行<strong>“占用率越高，程序运行越快”</strong>，可以实现运行效率的优化。当线程级并行（<em>Thread-Level Parallelism</em>，TLP）足以隐藏存储延迟时会达到一个临界点，此后想要继续提高程序性能，可以在单个线程中实现指令级的并行（<em>Instruction-Level Parallelism</em>，ILP），即单线程处理多数据。</p><p>​    但在另一方面，每个SM所能调度的线程总量是有限制的，因此当线程总量达到最大时，再减少寄存器的使用量就无法达到提高占有率的目的（如下表中寄存器数目由20减小为16，线程块调度数量不变），所以在这种情况下，应增加寄存器的使用量到临界值。</p><img src="1.png"><h3 id="1-4-寄存器优化方式"><a href="#1-4-寄存器优化方式" class="headerlink" title="1.4 寄存器优化方式"></a>1.4 寄存器优化方式</h3><p>​    1）将中间结果累积在寄存器而非全局内存中。尽量避免全局内存的写操作，因为如果操作聚集到同一块内存上，就会强制硬件对内存的操作序列化，导致严重的性能降低；</p><p>​    2）循环展开。循环一般非常低效，因为它们会产生分支，造成流水线停滞。</p><h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h3><p>​    使用寄存器可以有效消除内存访问，或提供额外的ILP，以此实现GPU内核函数的加速，这是最为有效的方法之一。</p><h2 id="二、共享内存"><a href="#二、共享内存" class="headerlink" title="二、共享内存"></a>二、共享内存</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p>​    1、共享内存实际上是可以受用户控制的一级缓存，每个SM中的一级缓存和共享内存共用一个64KB的内存段。</p><p>​    2、共享内存的延迟很低，大约有1.5TB/s的带宽，而全局内存仅为160GB/s，换言之，有效利用共享内存有可能获得7倍的加速比。但它的速度依然只有寄存器的十分之一，并且共享内存的速度几乎在所有GPU中都相同，因为它由核时钟频率驱动。</p><p>​    3、只有当数据重复利用、全局内存合并，或者线程之间有共享数据（例如同时访问相同地址的存储体）的时候使用共享内存才更合适，否则将数据直接从全局内存加载到寄存器性能会更好。</p><p>​    4、共享内存是基于存储体切换的架构（<em>bank-switched architecture</em>），费米架构的设备上有32个存储体。无论有多少线程发起操作，<strong>每个</strong>存储体<strong>每个</strong>周期只执行<strong>一次</strong>操作。因此，如果线程束中的每个线程各访问一个存储体，那么所有线程的操作都可以在一个周期内同时执行，且所有操作都是独立互不影响的。此外，如果所有线程同时访问同一地址的存储体，会触发一个广播机制到线程束中的每个线程中。但是，如果是其他的访问方式，线程访问共享内存就需要排队，即一个线程访问时，其他线程将阻塞闲置。因此很重要的一点时，应该尽可能地获得<strong>零存储体冲突</strong>的共享内存访问。</p><h3 id="2-2-Example：使用共享内存排序"><a href="#2-2-Example：使用共享内存排序" class="headerlink" title="2.2 Example：使用共享内存排序"></a>2.2 Example：使用共享内存排序</h3><h2 id="2-2-1-归并排序"><a href="#2-2-1-归并排序" class="headerlink" title="2.2.1 归并排序"></a>2.2.1 归并排序</h2><p>​    假设待排序的数据集大小为N，现将数据集进行划分。根据归并排序的划分原则，最后每个数据包中只有两个数值需要排序，因此，在这一阶段，最大并行度可达到 $N \over 2$ 个独立线程。例如，处理一个大小为512KB的数据集，共有128K个32位的元素，那么最多可以使用的线程个数为64K个（N=128K，N/2=64K），假设GPU上有16个SM，每个SM最多支持1536个线程，那么每个GPU上最多可以支持24K个线程，因此，按照这样划分，64K的数据对只需要2.5次迭代即可完成排序操作。</p><p>​    但是，如果采用上述划分排序方式再进行合并，我们需要从每个排好序的数据集中读出元素，对于一个64K的集合，需要64K次读操作，即从内存中获取256MB的数据，显然当数据集很大的时候不合适。</p><p>​    因此，我们采用通过限制对原始问题的迭代次数，通过基于共享内存的分解方式来获得更好的合并方案。因为在费米架构的设备上有32个存储体，即对应32个线程，所以当需要的线程数量减少为32（一个线程束）时，停止迭代，于是共需要线程束4K个（128K/32=4K），又因为GPU上有16个SM，所以这将为每个SM分配到256个线程束。然而由于费米架构设备上的每个SM最多只能同时执行48个线程束，因此多个块将被循环访问。</p><p>​    通过将数据集以每行32个元素的方式在共享内存中进行分布，每列为一个存储体，即可得到零存储体冲突的内存访问，然后对每一列实施相同的排序算法。（或者也可以理解为桶排序呀）</p><p>​    然后再进行列表的合并。</p><h2 id="2-2-2-合并列表"><a href="#2-2-2-合并列表" class="headerlink" title="2.2.2 合并列表"></a>2.2.2 合并列表</h2><p>​    先从串行合并任意数目的有序列表看起：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, <span class="comment">//待排序数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 u32 *<span class="keyword">const</span> dest_array, <span class="comment">//排序后的数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_lists, <span class="comment">//列表总数</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_elements)</span> <span class="comment">//数据总数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);<span class="comment">//每个列表中的数据个数</span></span><br><span class="line">    u32 list_indexes[MAX_NUM_LISTS]; <span class="comment">//所有列表当前所在的元素下标</span></span><br><span class="line">    <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">    &#123;</span><br><span class="line">list_indexes[<span class="built_in">list</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">dest_array[i] = find_min(scr_array, </span><br><span class="line">                                 list_indexes, </span><br><span class="line">                                 num_lists, </span><br><span class="line">                                 num_elements_per_list);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">u32 <span class="title">find_min</span><span class="params">(<span class="keyword">const</span> u32*cosnt src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">             u32 *<span class="keyword">const</span> list_indexes, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_elements_per_list)</span><span class="comment">//寻找num_lists个元素中的最小值</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">    u32 min_idx = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_lists; i++)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[i] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line"><span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                min_idx = i;</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    list_indexes[min_idx]++;</span><br><span class="line">    <span class="keyword">return</span> min_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    将上述算法用GPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">gpu_sort_array_array</span><span class="params">(u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_elements)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 tid = (blockIdx.x * blockDim.x) + threadIdx.x;</span><br><span class="line">    __shared__ u32 sort_tmp[NUM_ELEM];</span><br><span class="line">    __shared__ u32 sort_tmp_1[NUM_ELEM];</span><br><span class="line">    </span><br><span class="line">    copy_data_to_shared(data, sort_tmp, num_lists, num_elements, tid);</span><br><span class="line">    radix_sort2(sort_tmp, num_lists, num_elements, tid, sort_tmp_1);</span><br><span class="line">    merge_array6(sort_tmp, data, num_lists, num_elements, tid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    第一个函数的实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">copy_data_to_shared</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    u32 *sort_tmp, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">sort_tmp[i+tid] = data[i+tid]; </span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    该函数中，程序按行将数据从全局内存读入共享内存。当函数调用一个子函数并传入参数时，这些参数必须以某种方式提供给被调用的函数，有两种方法可以采用。一种是通过寄存器传递所需的值，另一种方法是创建一个名为“栈帧”的内存区，但这种方法非常地不高效。出于这一原因，我们需要重新修改合并的程序(merge_array)，以避免函数调用，修改后程序如下（单线程）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array1</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    lists_indexes[tid] = <span class="number">0</span>;<span class="comment">//从每个列表的第一个元素开始</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//单线程</span></span><br><span class="line">    <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">        <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">        &#123;</span><br><span class="line">u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             u32 min_idx = <span class="number">0</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">      &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[<span class="built_in">list</span>] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line">   <span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                 min_idx = i;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          list_indexes[min_idx]++;</span><br><span class="line">          dest_array[i]=min_val;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    这里只用一个线程进行合并，但显然，为了获得更好的性能，一个线程是远远不够的。因为数据被写到一个单一的列表中，所以多个线程必须进行某种形式的合作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array6</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个列表分到的元素个数</span></span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建一个共享列表数组，用来储存当前线程所访问的列表元素下标</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建所有线程共享的最小值与最小值线程号</span></span><br><span class="line">    __shared__ u32 min_val;</span><br><span class="line">    __shared__ u32 min_tid;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        u32 data;</span><br><span class="line">        <span class="comment">//如果当前列表还未被读完，则从中读取数据</span></span><br><span class="line">        <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list);</span><br><span class="line">        &#123;</span><br><span class="line">             <span class="comment">//计算出当前元素在原数组中的下标</span></span><br><span class="line"><span class="keyword">const</span> u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//用零号线程来初始化最小值与最小值线程号</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//让所有线程都尝试将它们现在手上有的值写入min_val，但只有最小的数据会被保留</span></span><br><span class="line">        <span class="comment">//利用__syncthreads()确保每个线程都执行了该操作</span></span><br><span class="line">        atomicMin(&amp;min_val, data);</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在所有data==min_val的线程中，选取最小线程号写入min_tid</span></span><br><span class="line">        <span class="keyword">if</span>(min_val == data)</span><br><span class="line">        &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将满足要求的线程所在列表的当前元素往后移一位，进行下一轮比较</span></span><br><span class="line">        <span class="comment">//并将筛选结果存入结果数组dest_array</span></span><br><span class="line">        <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">             dest_array[i] = data;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    上面的函数中将num_lists个线程进行合并操作，但只用了一个线程一次将结果写入结果数据数组中，保证了结果的正确性，不会引起线程间的冲突。</p><p>​    其中使用到了 atomicMin 函数。每个线程以从列表中获取的数据作为入参调用该函数，取代了原先单线程访问列表中所有元素并找出最小值的操作。当每个线程调用 atomicMin 函数时，线程读取保存在共享内存中的最小值并于当前线程中的值进行比较，然后把比较结果重新写回最小值对应的共享内存中，同时更新最小值对应的线程号。然而，由于列表中的数据可能会重复，因此可能出现多个线程的值均为最小值的情况，保留的线程号却各不相同。因此需要执行第二步操作，保证保留的线程号为最小线程号。</p><p>​    虽然这种方法的优化效果很显著，但它也有一定的劣势。例如，atomicMin函数只能用在计算能力为1.2以上的设备上；另外，aotomicMin函数只支持整数型运算，但现实世界中的问题通常是基于浮点运算的，因此在这种情况下，我们需要寻找新的解决方法。</p><h2 id="2-2-3-并行归约"><a href="#2-2-3-并行归约" class="headerlink" title="2.2.3 并行归约"></a>2.2.3 并行归约</h2><p>​    并行归约适用于许多问题，求最小值只是其中的一种。它使用数据集元素数量一半的线程，每个线程将当前线程对应的元素与另一个元素进行比较，计算两者之间的最小值，并将得到的最小值移到前面。每进行一次比较，线程数减少一半，如此反复直到只剩一个元素为止，这个元素就是需要的最小值。</p><p>​    在选择比较元素的时候，应该尽量避免选择同一个线程束中的元素进行比较，因为这会明显地导致线程束内产生分支，而每个分支都将使SM做双倍的工作，继而影响程序的性能。因此我们选择将线程束中的元素与另一半数据集中的元素进行比较。如下图，阴影部分表示当前活跃的线程。</p><img src="2.png"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array5</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists,</span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_val[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_idx[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_val[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_idx[tid] = <span class="number">0</span>;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">u32 tid_max = num_lists &gt;&gt; <span class="number">1</span>;<span class="comment">//最大线程数为列表总数的一半</span></span><br><span class="line">         u32 data;<span class="comment">//使用寄存器可以提高运行效率，将对共享内存的写操作次数减少为1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当列表中还有未处理完的元素时</span></span><br><span class="line">         <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">         &#123;</span><br><span class="line">             <span class="comment">//计算该元素在原数组中的位置</span></span><br><span class="line">cosnst u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">         &#125;</span><br><span class="line">        <span class="comment">//若当前列表已经处理完，将data赋值最大</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前元素及线程号写入共享内存</span></span><br><span class="line">        reduction_val[tid] = data;</span><br><span class="line">        reduction_idx[tid] = tid;</span><br><span class="line">        __syncthreads;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当前活跃的线程数多于一个时</span></span><br><span class="line">        <span class="keyword">while</span>(tid_max!=<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(tid &lt; tid_max)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//将当前线程中的元素与另一半数据集中的对应元素进行比较</span></span><br><span class="line"><span class="keyword">const</span> u32 val2_idx = tid + tid_max;</span><br><span class="line">                 <span class="keyword">const</span> u32 val2 = reduction_val[val2_idx];</span><br><span class="line">                 </span><br><span class="line">                 <span class="comment">//最后保留较小的那个元素</span></span><br><span class="line">                 <span class="keyword">if</span>(reduction_val[tid] &gt; val2)</span><br><span class="line">                 &#123;</span><br><span class="line">reduction_val[tid] = val2;</span><br><span class="line">                      reduction_idx[tid] = reduction_idx[val_idx];</span><br><span class="line">                 &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//线程数减半，进入下一轮循环</span></span><br><span class="line">            tid_max &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在零号线程中将结果写入结果数组，并将相应线程所指的元素后移一位</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[reduction_idx[<span class="number">0</span>]]++;</span><br><span class="line">             dest_array[i] = reduction_val[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    同样，这种方法也在共享内存中创建了一个临时的列表 list_indexes 用来保存每次循环中从 num_list 个数据集列表中选取出来进行比较的数据。如果进行合并的列表已经为空，那么就将临时列表中的对应数据区赋最大值0xFFFFFFFF。而每轮while循环后，活跃的线程数都将减少一半，直到最后只剩一个活跃的线程，亦即零号线程。最后将结果复制到结果数组中并将最小值所对应的列表索引加一，以确保元素不会被处理两次。</p><h2 id="2-2-4-混合算法"><a href="#2-2-4-混合算法" class="headerlink" title="2.2.4 混合算法"></a>2.2.4 混合算法</h2><p>​    在了解atomicMin函数和并行归约两种方案后，我们可以利用这两种算法各自的优点，创造出一种新的混合方案。</p><p>​    简单的1~N个数据归约的一个主要问题就是当N增大时，程序的速度先变快再变慢，达到最高效的情形时N在8至16左右。混合算法将原数据集划分成诸多个小的数据集，分别寻找每块中的最小值，然后再将每块得到的结果最终归约到一个值中。这种方法和并行归约的思想非常相似，但同时又省略了并行归约中的多次迭代。代码更新如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE 8</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE_BIT_SHIFT 3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ACTIVE_REDUCTIONS ((MAX_NUM_LISTS) / (REDUCTION_SIZE))</span></span><br><span class="line"></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个线程都从原数组中读入一个数据，用作首次比较</span></span><br><span class="line">    u32 data = src_array[tid];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//当前线程所在的数据块编号（8个线程为一组，每个线程处理一个列表）</span></span><br><span class="line">    <span class="keyword">const</span> u32 s_idx = tid &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//首次进行分别归约的数据块总数</span></span><br><span class="line">    <span class="keyword">const</span> u32 num_reductions = num_lists &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    <span class="keyword">const</span> u32 num_elements_per_list = num_elements / num_lists;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在共享内存中创建一个列表，指向每个线程当前所在的元素，并初始化为0</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//遍历所有数据</span></span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//每个数据块在内部归约后都会产生一个相应的最小值</span></span><br><span class="line">        <span class="comment">//在共享内存中开辟一个列表，用来保存每组的最小值</span></span><br><span class="line">__shared__ u32 min_val[MAX_ACTIVE_REDUCTIONS];</span><br><span class="line">         __shared__ u32 min_tid;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//初始化每个数据块的内部最小值</span></span><br><span class="line">        <span class="keyword">if</span>(tid &lt; num_lists)</span><br><span class="line">        &#123;</span><br><span class="line">min_val[s_idx] = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前线程的数据与所处数据块的最小值进行比较，并保留较小的那一个</span></span><br><span class="line">        atomicMin(&amp;min_val[s_idx], data);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//进行归约的数据块总数不为零时</span></span><br><span class="line">        <span class="keyword">if</span>(num_reductions &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//确保每个线程都已经将上一步比较操作完成</span></span><br><span class="line">__syncthreads();</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//将每个数据块产生的最小值与零号数据块的最小值进行比较，保留较小的那一个</span></span><br><span class="line">             <span class="keyword">if</span>(tid &lt; num_reductions)</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_val[<span class="number">0</span>], min_val[tid]);</span><br><span class="line">                  __syncthreads();</span><br><span class="line">             &#125;</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//如果当前线程的数据等于此次比较保留的最小值，记录最小线程号</span></span><br><span class="line">             <span class="keyword">if</span>(data == min_val[<span class="number">0</span>])</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="comment">//确保上一步操作每个线程都已经完成，才能执行下一句</span></span><br><span class="line">             __syncthreads();</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//如果当前线程号恰为记录下的最小线程号</span></span><br><span class="line">            <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//当前所指元素后移一位</span></span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">                </span><br><span class="line">                 <span class="comment">//将结果保存入结果数组</span></span><br><span class="line">                  dest_array[i] = data;</span><br><span class="line">                 </span><br><span class="line">                  <span class="comment">//若该线程对应的列表尚未被处理完</span></span><br><span class="line">                  <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">                      <span class="comment">//更新该线程的data，进行下一轮比较</span></span><br><span class="line">                      data = src_array[tid + (list_indexes[tid] * num_lists)];</span><br><span class="line">                  <span class="keyword">else</span></span><br><span class="line">                      data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    注意到：</p><p>​    1）原来的min_val由单一的数据扩展成为一个共享数据的数组，这是因为每个独立的线程都需要从它对应的数据集中获取当前的最小值来进行内部比较。每个最小值都是一个32位的数值，因此可以存储在独立的共享内存存储体中。</p><p>​    2）内核函数中的REDUCTION_SIZE的值被设置成8，意味着每个数据块中包含8个数据，程序分别找出每个数据块的最小值，然后再在这些最小值中寻找最终的最小值。</p><p>​    3）内核函数中最重要的一个变化是，只有每次比较的最小值所对应的那个线程的data才会更新，其他线程的data都不会更新。而在之前的内核函数中，每轮比较开始，所有线程都会从对应的列表中重新读入data 的值，随着N的增大，这将变得越来越低效。</p><h2 id="2-2-5-总结"><a href="#2-2-5-总结" class="headerlink" title="2.2.5 总结"></a>2.2.5 总结</h2><p>​    1）共享内存允许同一个线程块中的线程读写同一段内存，但线程看不到也无法修改其他线程块的共享内存。</p><p>​    2）共享内存的缓冲区驻留在物理GPU上，所以访问时的延迟远低于访问普通缓冲区的延迟，因此除了使用寄存器，还应更有效地使用共享内存，尤其当数据有重复利用，或全局内存合并，或线程间有共享数据的时候。</p><p>​    3）编写代码时，将关键字_shared__添加到声明中，使得该变量留驻在共享内存中，并且线程块中的每个线程都可以共享这块内存，使得一个线程块中的多个线程能够在计算上进行通信和协作。</p><p>​    4）调用 __syncthreads() 函数来实现线程的同步操作，尤其要注意确保在读取共享内存之前，想要写入的操作都已经完成。另外还需要注意，切不可将这个函数放置在发散分支（某些线程需要执行，而其他线程不需要执行），因为除非线程块中的每个线程都执行了该函数，没有任何线程能够执行之后的指令，从而导致死锁。</p><p>​    5）不妨尝试使用共享内存实现矩阵乘法的优化。</p><blockquote><p>Author: 潘薇鸿<br>PostDate: 2018.11.25</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将探讨CUDA中的内存管理机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>浙江大学超算队博客...活了？</title>
    <link href="https://zjusct.github.io/2018/09/30/first/"/>
    <id>https://zjusct.github.io/2018/09/30/first/</id>
    <published>2018-09-30T13:25:55.000Z</published>
    <updated>2019-05-31T15:04:27.505Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>这里是浙江大学超算队的官方博客</p><a id="more"></a><p>记录一些前沿的Tech姿势、大家平时的DeBug经历、还有ASC世界超算大赛的经验分享！</p><p>希望这个博客不仅仅是给以后的小盆友们提供宝贵的经验 更是诸君技术的记录与提升的大好机会</p><p>平时我们写着代码 总是不愿意写文档 遇到了非常Tricky又神奇的Bug 花了一下午终于解决了 却没有记录下来</p><p>很有可能很多的无谓的时间就会在以后被重复 这样的时间浪费 我们完全可以节省下来！</p><p>希望大家能享受在超算队一起学习的时光~ 从现在开始 把这段美好的时间变成字符 记录下来叭~~</p><img src="ttfish.jpeg"><blockquote><p>Author: TTfish<br>PostDate: 2018.9.30</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;这里是浙江大学超算队的官方博客&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Spc" scheme="https://zjusct.github.io/tags/Spc/"/>
    
      <category term="ZJU" scheme="https://zjusct.github.io/tags/ZJU/"/>
    
  </entry>
  
</feed>
