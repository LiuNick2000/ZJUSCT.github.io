<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浙江大学超算队</title>
  
  <subtitle>Zhejiang University Supercomputer Team</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjusct.github.io/"/>
  <updated>2019-06-14T15:14:23.482Z</updated>
  <id>https://zjusct.github.io/</id>
  
  <author>
    <name>ZJU · SCT</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>生成式对抗网络_理论</title>
    <link href="https://zjusct.github.io/2019/06/14/GAN_theory/"/>
    <id>https://zjusct.github.io/2019/06/14/GAN_theory/</id>
    <published>2019-06-13T16:00:00.000Z</published>
    <updated>2019-06-14T15:14:23.482Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GAN的理论"><a href="#GAN的理论" class="headerlink" title="GAN的理论"></a>GAN的理论</h1><p>在 <a href="http://www.unispac.xyz/?p=1540" target="_blank" rel="noopener">对抗式生成网络-基础</a> 这篇文章中，我们介绍了对抗式生成网络（Generative Adversarial Networks, GAN）的基本思想。我们已经知道，与最经典的Reflection Model不一样，GAN是通过一组用于生成的generator和用于判别的discriminator互相对抗来实现生成能力的自强化。上一篇文章中，我们只是很简单的介绍了它的intuition，但GAN本质上还是一个数学模型，为什么能实现自强化，为什么模型最后会向我们希望的方向收敛，它还需要一些更加严谨的理论推导。</p><p>这篇文章中，我们将根据<a href="https://arxiv.org/pdf/1701.04862.pdf" target="_blank" rel="noopener">《TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS》(Arjovsky et al. 2017)</a> 和 <a href="https://arxiv.org/pdf/1701.07875.pdf" target="_blank" rel="noopener">《Wasserstein GAN》(Arjovsky et al. 2017)</a> 这两篇论文的推导和论述，简单的讨论一下GAN的数学理论。</p><p>在展开GAN的讨论之前，我们将会先简单的介绍几个需要用到的前置概念和知识。在这之后我们会介绍原始GAN的loss function的实际数学意义，并简单讨论其存在的问题，最后讨论解决的方法。</p><h2 id="理论准备"><a href="#理论准备" class="headerlink" title="理论准备"></a>理论准备</h2><h3 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a><font color="green">Jensen不等式</font></h3><p>众所周知，在一个凸函数，其定义域上的任意两点 $x_1,x_2$，在 $0\leq t \leq1$ 时，满足：<br>$$<br>tf(x_1)+(1-t)f(x_2) \geq f[tx_1+(1-t)x_2]<br>$$<br>这就是Jensen不等式最著名的两点形式。Jensen的两点不等式可以被推广到任意多数量的点集${x_i}$上：当$\lambda_i \geq0$且$\Sigma \lambda_i=1$时，凸函数$f(x)$满足 $f(\Sigma\lambda_ix_i)\leq \Sigma\lambda_if(x_i)$。用数学归纳法可以证明这一推广式。</p><p>很容易想到，当上式取极限时，我们则会得到连续域上的Jensen不等式：<br>$$<br>\int p(x)dx=1;=&gt;f[\int p(x)xdx]\leq \int p(x)f(x)dx<br>$$<br>把上式放到概率论中，如果p(x)看作概率密度函数，那么上式显然等价于：<br>$$<br>f[E(x)]\leq E[f(x)]<br>$$</p><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a><font color="green">KL散度</font></h3><p>考虑一个真实的概率分布p(x)和一个对这个概率分布进行拟合的另一个近似分布q(x)，我们如何评估这个近似分布好不好？换句话说，我们如何去量化这个近似评估和真实评估的差异？</p><p>KL散度（Kullback-Leibler divergence）是一个选择。KL散度又常常被称为相对熵（relative entropy）或者信息散度（information divergence），是两个概率分布间差异的非对称性度量。</p><p>对于上面的两个概率分布p和q，KL散度定义如下：<br>$$<br>KL(p||q)<br>=\int p(x)ln[\frac {p(x)}{q(x)}]dx<br>$$<br>直观的来看，对于点x处，KL散度将两个分布的差异损失定义为$p(x)ln[\frac{p(x)}{q(x)}]$，显然只有两个分布都在此处取相同的概率密度，这个损失才为0，并且在真实分布p中出现频率更高的x处的损失所享有的权重也会更大。因此，从直观的角度来看，它确实在某种程度上反映了两个分布之间的拟合程度。</p><p>除了直观意义，它在数学上也有两个很好的性质。</p><h4 id="性质1"><a href="#性质1" class="headerlink" title="性质1"></a><font color="blue">性质1</font></h4><p><strong>KL(p||q) &gt;= 0, 当且仅当p=q时，等号成立.</strong><br>$$<br>KL(p||q)=\int p(x)ln[\frac{p(x)}{q(x)}]dx\<br>令g(x)=\frac{q(x)}{p(x)}\<br>KL(p||q)=\int -p(x)ln[g(x)]dx<br>\-ln(x)为凸函数，由Jensen不等式=&gt;\<br>KL(p||q)=E[-ln(g(x))]\geq -ln[E(g(x))]\<br>=-ln{\int p(x)\frac{q(x)}{p(x)}dx}=-ln(1)=0\<br>得证<br>$$</p><h4 id="性质2"><a href="#性质2" class="headerlink" title="性质2"></a><font color="blue">性质2</font></h4><p><strong>最小化KL散度等价于最大化似然函数</strong><br>$$<br>q(x)是p(x)的近似\<br>=&gt;q(x)=P(x;\theta)，\theta是这个近似器的参数\<br>argmin;KL(p||q)=argmax;\int p(x)ln[q(x)]-\int p(x)ln[p(x)]\<br>=argmax;\int p(x)ln[q(x)]\<br>\approx argmax;\frac{1}{N}\Sigma;ln[q(x_i)]\<br>=argmax; \frac{1}{N};\Pi;q(x_i) \<br>=argmax;\frac{1}{N} ;\Pi;P(x_i;\theta)<br>$$<br>如上所示，对于p(x)做参数估计，我们可以写出似然函数: $\frac{1}{N};\Pi;P(x;\theta)$，最大化这个似然函数的直观意义就是使得这个近似分布中，尽量让实验中出现次数多的样本x取更大的概率值，这是极大似然的基本想法。</p><p>推导发现，最小化KL散度，实际上就等价于最大化这个似然函数。（这可以看作KL散度之所以是一个“良好”测度量的原因。）</p><h3 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a><font color="green">JS散度</font></h3><p>JS散度（Jensen-Shannon divergence），和KL散度一样，也是用来衡量两条曲线的相合性的。</p><p>在KL散度中p和q的地位不等价，交换p和q得到的结果是不一样的。如果把散度当作某种“距离”的衡量，那么在KL散度定义的距离空间中，两个对象之间的距离就是不对称的。但是在一些问题背景下，我们可能更希望使用具有对称性的“距离”。</p><p>JS散度的定义就是为了解决了KL散度的不对称问题：<br>$$<br>JS(P1||P2)=\frac{1}{2}KL(P1||\frac{P1+P2}{2})+\frac{1}{2}KL(P2||\frac{P1+P2}{2})<br>$$<br>如上所示，它直接从KL散度的定义中做了一个简单的变换，直观意义可以理解为P1和P2与这两者的中线”距离”的均值。关于其严格的数学性质，这里就不再赘述，有兴趣可以取查阅相关的资料。</p><h2 id="初代GAN的收敛性"><a href="#初代GAN的收敛性" class="headerlink" title="初代GAN的收敛性"></a>初代GAN的收敛性</h2><p><img src="0.png" alt="Discriminator与Generator之间的Minimax博弈"></p><p>2014年时，Goodfellow在<a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">《Generative Adversarial Nets》</a>中首次提出GAN的时候，使用了上面这个非常优雅的目标损失函数，这是一个非常简练的minimax博弈式，使用这套Loss，训练过程中，Discriminator会尽量的最大化真实样例的评分期望，并最小化生成的样例的评分期望，以筛选出假图片；Generator则尽量最大化自己生成的样例的期望评分，试图骗过Discriminator。其基本的思想我们在上一篇文章中已经介绍过了。</p><p>对于一个generator来说，我们一般是采样一个噪音向量z作为其输入，generator根据这个随机输入生成一个随机特征的图片。当我们使用满足$z\sim p_z$的随机噪音的时候，也隐含的定义了生成的图片的分布$p_g$，理想的情况下，我们希望生成的图片分布尽量收敛于训练集中图像的概率分布$p_{data}$。</p><p>Goodfellow在GAN的开山之作中证明了上面式子中定义的minimax博弈在$p_g=p_{data}$时取得纳什均衡，并且使用其提出的算法（其实就是用梯度下降最优化上述目标函数）可以使得博弈达到这个纳什均衡，因此也就能达到$p_g = p_{data}$这个美好期望。</p><h3 id="纳什均衡点"><a href="#纳什均衡点" class="headerlink" title="纳什均衡点"></a><font color="green">纳什均衡点</font></h3><p>对于任意一个样本x，它既有可能来自训练集，又有可能来自生成器，只不过是概率分布存在差异罢了。这个样本对于上面定义的判别器D损失的贡献为：$-P_{data}(x)logD(x)-P_g(x)log[1-D(x)]$。</p><p>固定generator的参数，上式对D求偏导 :<br>$$<br>Dif=\frac{-P_{data}(x)}{D(x)}+\frac{P_g(x)}{1-D(x)}<br>$$<br>导数为0时，判别器的损失取最优值，不难导出最优判别器：</p><p><img src="1.png" alt="最优判别器"></p><p>这个最优判别器的直观意义很显然，一个数据x在训练集中出现的可能性是a，被生成器生成的概率是b，那么当一个最优的判别器要判断其到底是真图片还是假图片的时候，输出概率$\frac{a}{a+b}$是非常自然的事情。</p><p><img src="2.png" alt="生成器的等价目标函数"></p><p>因此，对于任何一个固定的G，D能取得的最优如上。那么说白了，要求纳什均衡，也就是只需要再调整生成器的生成分布 $p_g(x)$ 的，使得上式取最小。</p><p>原论文中给出了下面的证明过程，证明了$p_g=p_{data}$时，取得纳什均衡：</p><p><img src="3.png" alt="纳什均衡的证明"></p><p>前面我们已经介绍过了KL散度：$KL(p||q)=\int p(x)ln[\frac {p(x)}{q(x)}]dx$. </p><p>原论文中给出的证明其实就是对上面的$C(G)$做了一个变换，将其变换成散度形式：<br>$$<br>\int p_{data}(x)<em>log\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}</em>dx+\int p_g(x)<em>log\frac{p_g(x)}{p_{data}(x)+p_g(x)}</em>dx\<br>=KL（p_{data}||p_{data}+p_g）+KL(p_g||p_{data}(x)+p_g(x))<br>$$<br>log里面分母再除个2，提出来就是它证明中的式子了：<br>$$<br>=KL（p_{data}||\frac{p_{data}+p_g}{2}）+KL(p_g||\frac{p_{data}+p_g}{2})-log(4)<br>$$<br>然后我们就是要最小化它对吧？</p><p>我们在上面介绍KL散度的时候，已经证明了，KL&gt;=0，当且仅当p，q相等时取0，所以上式取最小的条件就是：$p_{data}=\frac{p_{data}+p_g}{2}=p_g&lt;=&gt;p_{data}=p_g$。</p><p>由此得证纳什均衡的取得条件为$p_{data}=p_g$。</p><h3 id="优化算法的收敛性"><a href="#优化算法的收敛性" class="headerlink" title="优化算法的收敛性"></a><font color="green">优化算法的收敛性</font></h3><p><img src="4.png" alt="优化算法"></p><p>在上一篇文章中，我们就已经介绍了这个训练算法，分别让D和G轮流针对Loss做梯度下降而已，直观上很好理解。Goodfellow在原论文中也对其收敛性做了证明：</p><p><img src="5.png" alt="优化算法的收敛性"></p><p>其实感觉证了跟没证差不多吧。。大致就和我们在之前讨论的一样，最后要优化的本质上就是一个关于$p_g$的函数，然后证明了凸性，所以可以用梯度下降保证收敛到最优点。但是实际操作中，我们不能表示为一个$p_g$的函数，而是用多层网络来拟合，虽然缺乏理论保障，但是非常成功，2333。。。</p><p>总的来说，原论文中，这一部分，主要还是证明纳什均衡点的部分最精彩。上一篇文章中，我们用类比的方法解释了GAN的哲学，但是这里我们直接从数学上证明了定义的Loss的合理性：在这个Loss定义下，我们实际上是在让生成器的生成结果的概率分布和数据集分布拟合。</p><h2 id="初代GAN存在的问题"><a href="#初代GAN存在的问题" class="headerlink" title="初代GAN存在的问题"></a>初代GAN存在的问题</h2><p>从14年GAN被提出以来，GAN在实践过程中，就一直存在训练困难等问题。</p><p>“However, they still remain remarkably difficult to train, with most current papers dedicated to heuristically finding stable architectures……Dispite their success, there is little to no theory explaining the unstable behaviour of GAN training.”</p><p>为了解决这个问题，大部分尝试都是通过改进GAN的生成器和判别器的架构，毕竟是CNN，优化优化架构总能让性能提升的。比较成功的如DCGAN，通过不断实验，找到一个比较好的网络架构设置。它们取得了一些成功，但是没有彻底解决GAN存在的问题，并且也一直缺乏对于GAN的表现的理论解释。</p><p>人们发现，在使用上面Goodfellow提出的那套minimax损失函数训练时，判别器训练得越好，生成器梯度消失越严重。也就是说当discriminator太强的时候，generator的能力就训练不上去了。</p><p>Goodfellow当年在写这篇论文的时候也发现了这个问题，于是在介绍完自己最开始提出的那套Loss后，又补充了几句：”In practive, equation 1 may not provide sufficient gradient for G to learn will……Rather than training G to minimize log(1-D(G(z))) we can train G to maxmize log D(G(z)). This objective function results in the same fixed point of the dynamics of G and D but provides much stronger gradients early in learning.”</p><p>其实就是把generator的目标函数简单的从log(1-D)换成了-log(D)，直观来看，意义是一样的，都是在引导generator骗过discriminator，只不过数学形式发生了变化，在训练的时候梯度/收敛等方面发生了变化。但是后来人们发现，这种做法也很一般，首先梯度很不稳定，其次生成的样本多样性不足。</p><p>直到2017年，这位来自 Courant Institute of Mathematical Sciences 的 Martin Arjovsky 先生发表了文章开头提到的<a href="https://arxiv.org/pdf/1701.04862.pdf" target="_blank" rel="noopener">《TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS》(Arjovsky et al. 2017)</a> ，其中存在的理论问题才得到了回答。</p><p>原文中对于这部分问题的归纳得非常精彩：</p><p><img src="6.png" alt="初代GAN存在的问题"></p><p>寥寥几句话，就把前人遇到的问题的要害给抓住了：既然D最优的时候，训练G就能逼近纳什均衡，为啥D越好，G的训练却越糟糕？Goodfellow说这是因为saturation导致的，可以换个loss function，但是为啥换了过后发现更垃圾了。</p><p>于是提出了四个精髓的问题：为啥D越好,G训练越差？为什么GAN的训练不稳定？Goodfellow提出的替代选项也和JSD有关么，性质如何？有什么解决当下问题的办法吗？</p><p>问完后，夸下海口：The fundamental contributions of this paper are the answer to all these questions. 可谓是技惊四座。。。</p><p>这部分，我们主要整理了Arjovsky这篇论文中的关于初代GAN存在的问题的部分分析和推理。我们将分两个部分分别讨论Goodfellow原论文中提出的两种形式的Generator目标函数存在的问题。</p><p>但是我们只会给出一个简单的intuition层次上的讨论，严格的数学推导读者可以自行下载论文查看。因为，这位数学研究所的大哥，写的论文。。。嗯。。。</p><p>非常严谨：</p><p><img src="7.png" alt="初代GAN存在的问题"></p><p>要吃下这篇论文，最好得有一点测度论的基础。（反正我不会）</p><p>至于他在这篇论文中提出的解决方法，我们不再讨论，因为下一个部分，我们会介绍一个更加成熟的方法——WGAN。而提出者。。。还是这位Arjovsky先生。。。</p><h3 id="log-1-D-存在的问题"><a href="#log-1-D-存在的问题" class="headerlink" title="log(1-D)存在的问题"></a><font color="green">log(1-D)存在的问题</font></h3><p>在上面证明GAN的纳什均衡点的时候，我们已经知道当Discriminator最优时，最优化Generator目标函数的等价表达式：<br>$$<br>KL（p_{data}||\frac{p_{data}+p_g}{2}）+KL(p_g||\frac{p_{data}+p_g}{2})-log(4)\<br>=2JSD(p_{data}||p_g)-log(4)<br>$$<br>最小化这个目标函数，也就是最小化$p_{data}$和$p_g$的散度，也就是尽量让两个分布贴合，两者相等时达到纳什均衡，这些我们都已经讨论过了。</p><p>但是问题就出在这个散度上。</p><p>如果在训练过程中$p_{data}$和$p_g$这两个分布完全不重叠，或者重叠部分可忽略，那么这个JS散度会是多少呢？</p><p>（这里的重叠指：两个分布在某个样本点上都有不为0的出现概率。）</p><p>还是先考虑单个x对这个JSD的贡献：<br>$$<br>[p_{data}(x)<em>log\frac{2p_{data}(x)}{p_{data}(x)+p_g(x)}+p_g(x)</em>log\frac{2p_g(x)}{p_{data}(x)+p_g(x)}]dx<br>$$<br>对于每个x来说，存在四种可能性：<br>$$<br>p_{data}(x)=0，p_g(x)=0\<br>p_{data}(x)\neq0，p_g(x)=0\<br>p_{data}(x)=0，p_g(x)\neq0\<br>p_{data}(x)\neq0，p_g(x)\neq0\<br>$$<br>第一种情况，对散度无贡献，第二种情况和第三种情况则会贡献log2*dx。由于完全不重叠或者重叠忽略不计，第四种情况贡献也可算作0。因此，实际上，在$p_{data}和p_g$重叠量可以忽略不计的情况下，这个JS散度始终在log2附近轻微震动。使用这样的loss function来训练generator，其实就意味着：梯度=0。</p><p>也就是说，当discriminator最优的时候，如果生成器产生结果的分布$p_g$与训练集分布$p_{data}$的重叠量可以忽略不计，generator不能获得任何梯度信息（因为梯度=0），无法通过梯度来优化自己。</p><p>而$p_{data}$和$p_d$重叠量可忽略不计的概率是多大呢？几乎是1..</p><p><img src="8.png" alt></p><p><img src="9.png" alt></p><p>原论文用测度论的方法证明了：当$p_{data}和p_g$的支撑集是高维空间中的低维流行时，这两个分布重叠部分的测度为0的概率为1。</p><p>这里简单解释一下概念：</p><ul><li>支撑集：一个函数在其定义域上取值非0的定义点的点集。</li><li>流形：三维空间中我们有曲线，曲面。这些低维曲线/曲面都是这个三维空间中的一个低维流形。比如一三维空间中的曲面是这个空间中的一个二维流形，曲线是一个一维流形。本质上，我们知道，所谓维度就是指需要几个变量才能确定，流形就是高维空间中这些概念的推广。</li><li>测度：可以理解为长度/面积/体积这些经典空间中的概念在高维空间中的推广，可以理解为超体积。（如果对线性代数熟悉的话，我们知道行列式本质上也是在计算一个超体积。）</li></ul><p>为什么要用支撑集这样的概念呢？因为像图片这样的高维向量，以64*64的图片为例（4096维），并不是随便拿一个4096维的向量，它就是一个对人类来说有“意义”的图片。当我们看到一张猫的图片（本质上是一个满足一定特征的向量）我们会说这是猫，当我们看到一个随机生成的4096维向量，我们大多时候看到的就是一堆乱七八糟的像素点。所以对于图片的概率分布来说，这个4096维的向量空间中，大部分的概率值应该都是0，是没有意义的。</p><p>那么，很容易就会想到，对于真实图片的概率分布，其支撑集可能只是这个高维空间中的一个低维流形（真实图片只有可能分布在一个这个空间中的一个低维度超几何面上，其它地方概率=0）。</p><p>更直观一点来说，就是：既然真实图片只是其所在的空间的一个非常小的集合中，那么可能我们根本就不需要这么高维度的向量来表达它，一个更低维度的编码就够了，用像素矩阵来表达一个图片对象是一个非常低效的手段。实际上已经有许多例子能够为此提供支持，其中最有代表性的应该是图像压缩。以JPEG压缩为例，在有些例子中，它可以将一个bmp格式的图片的大小压缩20倍，虽然它是有损压缩，但是解压后肉眼基本上看不出压缩前后的区别！</p><p>而原论文中想要表达的就是：既然图片分布是低位流形，那么两个低维流形的交叠相对于这个低维流形而言，期望测度是0。比如说在二维平面上随便取两条曲线段，这两条曲线段可能会有有限的相交点，但是这些有限的相交点的“长度”相比于这个曲线段微不足道。原论文的证明其实就是把这个直观的感受推广到了高维空间中。</p><p>至此，我们终于知道为啥D训练得太好，G就训练不动了。因为D训练得越好，训练G时的目标函数就越贴近上面我们推导出的JS散度。而这个散度虽然理论上看起来很好，只要不断降低，就能够让两个分布贴在一起，但是实际上由于两个分布的交叠几乎总是可以忽略不计，所以导致这个散度根本降不动。</p><p>因此，初代的GAN训练过程中需要非常小心的平衡D和G的训练，如果以来D就训练得太好，那么G根本训练不动，如果D质量很差，G从D那里得到的梯度指引也会不够准确，导致G的训练效果一般。</p><h3 id="log-D-存在的问题"><a href="#log-D-存在的问题" class="headerlink" title="log(D)存在的问题"></a><font color="green">log(D)存在的问题</font></h3><p>那么Goodfellow提出的备选项 : logD 又如何呢？</p><p>还是按照上面的方法，做一个简单的变形，就可以导出，当D最优的时候，G的目标函数等价于：<br>$$<br>KL(p_g||p_{data})-2JS(p_{data}||p_g)<br>$$<br>最小化这个目标函数会变得很滑稽。</p><p>一方面我们希望最小化KL，另一方面又希望最大化JS。直观上来看，我们又希望将两个分布贴紧，又希望将它们两个拉开。。。</p><p>所以显然当年Goodfellow提出的这个“改进”完全是凭感觉瞎扯的。它避免了原来的那个裸的JS，所以解决了梯度消失的问题，但是这两个互相矛盾的目标也自然最后也会让我们得到不稳定的梯度波动。。</p><h2 id="Wasserstein-GAN"><a href="#Wasserstein-GAN" class="headerlink" title="Wasserstein GAN"></a>Wasserstein GAN</h2><p>Arjovsky在<a href="https://arxiv.org/pdf/1701.04862.pdf" target="_blank" rel="noopener">《TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS》(Arjovsky et al. 2017)</a> 还讨论了一些别的问题，并且也提出了一些解决这些问题的思路和方法。限于笔者实力和精力有限，这里不再赘述，有兴趣可以阅读原论文。</p><p>这一部分，我们将介绍WGAN。WGAN的提出者也是Arjovsky等人，在发了上面这篇paper后，他又立刻发表了 <a href="https://arxiv.org/pdf/1701.07875.pdf" target="_blank" rel="noopener">《Wasserstein GAN》(Arjovsky et al. 2017)</a>。</p><p>Wasserstein GAN，顾名思义，其实就是把概率分布间的距离度量改成使用“Wasserstein Distance”来衡量的GAN。这一部分，我们会简单的介绍Wasserstein Distance及其相对于其它类型距离定义的优势。最后我们会介绍WGAN的基本思路和流程。</p><h3 id="Wasserstein-Distance"><a href="#Wasserstein-Distance" class="headerlink" title="Wasserstein Distance"></a><font color="green">Wasserstein Distance</font></h3><p><img src="14.png" alt="Wasserstein Distance"></p><p>原论文中，作者简单的贴出了Wasserstein Distance的定义，乍一看反正就是一个数学表达式，求的是一个向量间距离的期望下确界。。。</p><p>WD也被称为推土机距离（Earth-Mover Distance），是信息论里面的一个定义。搜一下这个东西，你会发现还有很多应用中都用到了它，它本质上是从最优传输问题导出的。上面我们说了这个式子就是要求一个期望的下确界，对于这个期望来说，其所依赖的联合概率分布是一个参数，这里要求一个下确界，就是要求一个最优联合概率分布来使得整个传输时最优的。而这个最优概率传输代价可以作为两个概率分布之间的差异的某种形式的度量。有兴趣可以参考一下这篇文章：<a href="https://zhuanlan.zhihu.com/p/45980364" target="_blank" rel="noopener">《传说中的推土机距离基础，最优传输理论了解一下》</a>。</p><p>用最优传输代价为什么能衡量分布距离？</p><p>假设我们是一个电商平台，我们运营着自己的仓库，我们当然希望全国各地的仓库中商品的分布尽量和各地的需求贴合。比如说A城对商品需求量为x，我们当然希望A城的仓库正好有x件商品来满足需求。</p><p>如果我们把n个城市的仓库库存表示为一个n维向量X，商品需求也表示为一个n维向量Y，那么最容易想到的简单评估库存分配的方法就是计算$||X-Y||$，这个向量距离范数在一定程度上衡量了库存分布和商品需求分布的贴合程度。</p><p>但是，这个范数可能评估得并不那么精确。</p><p>假设现在n个城市里，有n-2个城市库存和需求都是一样的，但是有两个城市，比如西藏和北京，其中西藏需求量比库存大1，北京的库存比需求量大1。那么我们可以从北京运送1个单位的商品到西藏，此时得到范数值$||X-Y||=1$。</p><p>再考虑另一个例子，如果这两个城市不是北京和西藏，而是北京和天津，那么我们需要把一个单位的商品从北京运送到天津。此时$||X-Y||$依然是1。</p><p>我们很快就会发现一个问题。如果把货物从北京发送到天津所需要的代价肯定显著低于从北京发往西藏，从这个角度来看，后者的分布应该优于前者的分布，但是如果简单的用一个距离范数来表达，无法体现这种差别。</p><p>我们把本地发货的代价设为0，把北京到西藏的代价设为100，北京到天津的代价为10。那么用最优传输代价来评估，前者的代价是100，后者代价是10，我们便能体现出后者由于前者这一特性。</p><p>这就是最优传输代价用来衡量分布距离的直观理解。</p><h3 id="WD的优势"><a href="#WD的优势" class="headerlink" title="WD的优势"></a><font color="green">WD的优势</font></h3><p>原论文在引出WGAN之前，先对比了四种距离定义的表现。考虑到其中一种TVD对于我们理解WGAN并没有什么特别必要，我们这里主要比较一下三种距离：KL Divergence，JS Divergence，Wasserstein Distance (Earth-Mover Distance)。</p><p><img src="12.png" alt="一个距离度量的例子"></p><p><img src="10.png" alt="两个不重叠的分布"></p><p>原作者举了上面这个简单的例子，这个例子中有两个不重叠的概率分布。（原论文没有给上面的示意图，这张示意图源自一篇知乎的文章<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">《令人拍案叫绝的Wasserstein GAN》</a>）</p><p>我们将看到JSD，KLD和WD在这个例子中表现出来的差异：</p><p><img src="11.png" alt="JS散度与KL散度的表现"></p><p><img src="13.png" alt="Wasserstein Distance"></p><p>对于JS我们前面已经分析过了，只要不相交，不管距离是多少，始终等于log2，根本不能体现距离带来的差别，所以一旦分开了，就不能通过梯度拉回来，所以训练G的时候会出现梯度消失。</p><p>对于KL，也很容易验证，当完全不相交时，梯度会趋于无穷，导致梯度爆炸。</p><p>而WD却体现出了非常优秀的性质，它完全是和距离呈正相关的，提供了连续平滑的梯度。</p><p>由此可见，如果我们使用WD来作为目标函数引导$p_g$和$p_{data}$的拟合，就能很好的避免梯度消失或者梯度爆炸。</p><h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a><font color="green">WGAN</font></h3><p>前面我们说了，WGAN就是把初代GAN的loss换掉，不再使用JS散度作为目标，取而代之，采用Wasserstein Distance来作为目标。</p><p>但是并没有这么简单。。。还记得Wasserstein Distance中要求一个下确界吗？这东西并不能直接求解。。</p><p><img src="15.png" alt></p><p><img src="16.png" alt></p><p>如上，虽然WD并不能直接算出来。但是作者在原文中使用了Kantorovich-Rubinstein duality（这个东西有兴趣可以自己去看<a href="https://vincentherrmann.github.io/blog/wasserstein/" target="_blank" rel="noopener">这篇文章</a>，原paper这部分引用了这本书：《Optimal Transport - Old and New》，据说是写个数学系phd看的，998页。”Have fun and good luck!!!”），于是WD距离的计算就归结为另一个等价对偶式的计算：</p><p><img src="17.png" alt="WD的对偶式"></p><p>或者也可以用：</p><p><img src="18.png" alt="1560522899056"></p><p>这里的$f$是一个满足Lipschitz连续的函数。</p><p>所谓的Lipschitz连续，就是对于一个常数K，函数在定义域中任意两个元素$x_1,x_2$上都要满足：<br>$$<br>|f(x_1)-f(x_2)|\leq K|x_1-x_2|<br>$$<br>K被称为Lipschitz常数，上面的表达式中，要求 $f$ 满足对于一个固定大小的Lipschitz常数满足Lipschitz连续。从Lipschitz连续的定义上，可以看到，它限制了函数梯度的上限，使得一个函数的函数值局部变动幅度只能在一个有限的范围内。</p><p>所以，计算WD就成了，在 $f$ 的Lipschitz常数不超过K的条件下，找出那个特殊的满足条件的 $f$ 取到$E_{x\sim p_r}[f(x)]-E_{x\sim p_\theta}[f(x)]$ 的上界。</p><p>似乎找到这个最优 $f$ 也不是一件容易的事情？神经网络最厉害的不就是拟合么。。我们拿一个神经网络来拟合。。但是神经网络在拟合过程中没办法限制f使得它满足Lipschitz条件呀？WGAN原论文中暴力的将拟合f的网络的w参数全部限制在了[-c,c]之间（小于-c就令为-c，大于c就令为c），来强行的满足了Lipschitz条件（f最终肯定是关于w的函数，w的范围只要被限制了，那么f的导数极限也被限制住了）。</p><p>于是在WGAN的设计中，原本的discriminator网络其实被换成了拟合 $f$ 的网络。对于这个 $f$ 网络的训练，它做的事情就是要最大化：<br>$$<br>L=E_{x\sim p_{data}}[f_w(x)]-E_{x\sim p_g}[f_w(x)]<br>$$<br>在训练充足的情况下，这个 $f$ 被期望能逼近使得WD对偶式取上确界时的$f$。</p><p>而generator要干什么呢？它就是负责最小化这个L，因为 $f$ 网络的训练主要是调整 $f$ 取上确界，使得L逼近WD，g网络则负责降低这个WD，使得两个分布被贴合。</p><p>你说巧不巧？明明discriminator已经不再是那个负责判别的discriminator，而是被用来拟合一个不明意义的$f$，两个网络之间竟然还是存在一个minimax博弈！</p><p>最后，附上WGAN的伪代码，其实变化不大：</p><p><img src="19.png" alt="WGAN伪代码"></p><h3 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a><font color="green">WGAN-GP</font></h3><p>WGAN论文中，作者也说了，采用clip来保证Lipschitz条件非常粗暴，简陋，鼓励大家提出一些新的想法。WGAN-GP就是一个替代策略，它在Loss里面添加了一个Gradient penalty项，有点类似正则化项，但是这里的GP项主要是为了限制f网络中的参数，让它满足Lipschitz条件。相对而言，更灵活一点。</p><p>具体的介绍可以参考<a href="https://zhuanlan.zhihu.com/p/52799555" target="_blank" rel="noopener">这篇文章</a>，我们不再展开讨论。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GAN的理论&quot;&gt;&lt;a href=&quot;#GAN的理论&quot; class=&quot;headerlink&quot; title=&quot;GAN的理论&quot;&gt;&lt;/a&gt;GAN的理论&lt;/h1&gt;&lt;p&gt;在 &lt;a href=&quot;http://www.unispac.xyz/?p=1540&quot; target=&quot;_bl
      
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>生成式对抗网络_基础</title>
    <link href="https://zjusct.github.io/2019/05/31/GAN_Introduction/"/>
    <id>https://zjusct.github.io/2019/05/31/GAN_Introduction/</id>
    <published>2019-05-30T16:00:00.000Z</published>
    <updated>2019-05-31T15:25:27.713Z</updated>
    
    <content type="html"><![CDATA[<p>Reference :  <a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Machine Learning and having it deep and structured (2018,Spring)</a> （李宏毅）</p><h1 id="Introduction-of-GAN"><a href="#Introduction-of-GAN" class="headerlink" title="Introduction of GAN"></a>Introduction of GAN</h1><p>生成式对抗网络（Generative Adversarial Network，GAN），是2014年被提出来的一种生成式深度学习网络技术。</p><p>由于在众多应用场合中的出色表现，近年来GAN的应用和研究已经成为一个非常热门的方向。</p><p><img src="0.png" alt="研究热度"></p><p>这篇文章将会对GAN做一个直观的介绍，帮助读者理解GAN的Basic Idea。</p><p>第一节中，我们将简单对机器学习的Basic Ideas做一个回顾。第二节和第三节会简单介绍一下什么是生成（Generation）。第四节和第五节会介绍两种生成模型。第六节我们将会看到GAN是如何结合这两种模型从而达到优秀的表现。</p><h2 id="Review-for-ML"><a href="#Review-for-ML" class="headerlink" title="Review for ML"></a>Review for ML</h2><p>在之前的文章（ <a href="http://zjuqxy.xyz/?p=1081" target="_blank" rel="noopener">机器学习</a> / <a href="http://122.152.198.128/?p=1532>" target="_blank" rel="noopener">卷积神经网络</a>）中，我们介绍了一些机器学习（Machine Learning）的思想和例子。</p><p>简单回顾一下在这些文章中的讨论，我们知道，<font color="red">机器学习本质上就是一个映射拟合 </font>:</p><p>在这个过程中，模型（Model）是一个函数，我们希望对它输入一个自变量x，能从它那里得到一个我们想要的输出y。以房价预测为例，我们希望告诉这个模型一些当前的经济，政治等社会环境，它能反馈一个房价数值。以图像分类为例，我们希望告诉这个模型一张图片，它能反馈给我们这张图片属于什么类别。</p><p>在最简单的形式中，我们用一个简单的线性模型（Linear Model） $w*x=y$ 来建模这个函数。高级一点，选择一个更复杂的核函数（另一种说法也可以称特征函数）的话，这个线性模型可以表达更加复杂的关系。再高级一点，把这些线性模型多层复合起来，形成神经网络（Neural Network），表达力就更强，而这个用深度神经网络来建模的方法自成一派，也就是所谓的深度学习（Deep Learning）。</p><p>而常常所说的训练数据（Training Data）无非就是从真实的世界中采样的一组（x，y）样本。训练（Training）则是让我们的模型从训练数据中学习x和y之间存在的某种规律的模式（Pattern），模型的参数在<font color="red">学习过程中</font>不断调整，最后模型代表的函数尽可能的去模拟了真实世界中的映射关系。</p><p>这个学习过程又是怎样的呢？通常就是定义了一个损失函数（Loss Function），这个损失函数的值是基于模型预测出来的结果和我们采样出来的真实结果的差别。<font color="red">所谓学习其实就是最小化这个差别。因此，所谓的学习，本质上就是这个函数最优化问题的一种拟人化表达而已。</font></p><p>因此机器学习实际上就是给定一个训练数据集，产生一个预测函数。这个预测函数能对输入的x给出期望的y，简单的可以表示为这样：</p><p><img src="2.png" alt="ML示意图"></p><h2 id="What-Is-Generation"><a href="#What-Is-Generation" class="headerlink" title="What Is Generation"></a>What Is Generation</h2><p>机器学习中，有很多种类型的任务，比如最简单的回归（Regression），分类（Classification）等。</p><p>它们是机器学习入门时肯定会接触的例子，常见的回归/分类任务比如：给定一张图片，输出一个标量或者向量描述这个图片的性质。</p><p>而生成（Generation）实际上就可以简单的理解为上述任务的一种逆向过程。给机器一张图片，它给你一个向量描述它的类别或者特征，似乎很无聊？那么如果给机器一个向量后，它给你生成一张图片(<del>老婆</del>)呢？</p><p><img src="1.png" alt="用向量生成图片"></p><p><img src="4.png" alt="生成器根据向量变化产生不同图片"></p><p>这就是生成任务最直观最简单的例子。（除了生成图片以外，当然它也可以用来生成句子/音频等其它任何你能想到的东西，只要能保证可训练。）</p><p>如上图所示，只需要调整对生成器的输入向量，它就会相应的生成不同的图片。P2中增大了第一维的输入，生成的人物头发变长了；P3中改变向量倒数第二维的输入，人物头发变成了蓝色；P4中增大了向量的最后一维，生成的人像就变得笑口常开了。</p><h2 id="Why-Generation"><a href="#Why-Generation" class="headerlink" title="Why Generation"></a>Why Generation</h2><p>一个很自然的问题是：为什么我们需要生成？给我一个向量，我生成一张图片有什么用？</p><p>如果只是给一个很枯燥的向量，让机器生成一张图片确实没啥用。但是如果再组合上其它的功能模块，这个技术可能会非常有用。</p><p><img src="21.png" alt></p><p>比如说，我们可以不从向量开始，而从一个句子开始。训练这样一个模型，自然语言先被转成向量编码，向量编码再被转成这个自然语言句子描述的画面。</p><p><img src="22.png" alt></p><p>又比如说，我们可以从图片中修改一部分区域，画出一个简单的草图，它就自动帮我们生成更加真实的修改内容。</p><p><img src="23.png" alt></p><p>你甚至还可以根据马云的脸的表情信息的编码生成一张小罗伯特·唐尼同样表情的脸，然后替换上去，产生真实的换脸效果。</p><p>生成，在某种意义上来说，是人工智能最有意义的任务。回归和分类这样的任务仅仅是去理解和识别我们熟知的对象，比如识别一句话是不是脏话，识别一张图片是不是猫。但是生成却是化腐朽为神奇，仅仅根据一些非常空洞的描述就自动生成了我们人所熟知的对象，这才是我们真正最想让人工智能为我们做的。仅仅是理解和识别，人工智能终究只是作为一种辅助工具。但是当它能创造的时候，它就翻身做了主人。</p><p><img src="9.png" alt="连续性"></p><p>上图是一个向量生成图片的例子。向量按照一种顺序在逐渐变化，我们而可以看到第四行生成的人脸也在变化。惊人的是，它的生成竟然是一个连续的将脸变换一个朝向的过程。这说明，这个模型确实学到了这个人的脸的深刻特征，能根据向量的微调，对产生的结果也生成连贯一致的微调。</p><p>不只是绘画，当它还能生成语言，生成电影，生成代码，生成电路设计，生成科学理论时，它体现出来的智能性是非常让人震撼的。学习到对象/概念的深层次内涵，并且能够使用这些学到的知识去创造未知的东西，这是人工智能的究极目标之一。</p><h2 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h2><h3 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p><img src="7.png" alt="Generator"></p><p>上图是生成模型的一种基本模式，我们希望输入一个向量得到一个目标输出作为图形。</p><p>一个简单问题是，我们如何训练它？容易想到的是事先准备一堆向量和图片之间的映射对，让这个生成器学得某种映射模式，如下图所示：</p><p><img src="8.png" alt="训练数据样例"></p><p>但面临的一个现实问题是，用于训练的图片很容易得到，但是这个向量code如何得到？需要我们手工设置吗？对于一些大型的项目，这个向量一般维度很高，样例也很多，手工设置这些向量标签可操作性并不强。并且，如果设计的向量不好，也很难保证模型收敛。</p><p>自编码器在一定程度上解决了这个问题：</p><p><img src="5.png" alt></p><p><img src="6.png" alt="Auto-encoder"></p><p>如图，自编码器接收一张图片作为输入，先丢入一个神经网络中产生一个编码作为输出（一个高维向量），这个产生的编码再被丢入另外一个神经网络，恢复成图片。我们希望恢复出来的图片尽可能和输入的图片相近，于是可以定义一个loss来衡量图片在这一次编码和解码过程中的损失，根据这个loss，再使用像梯度下降之类的方法，就可以调整encoder和decoder的参数，使得其尽可能减少损失。</p><p>在这样一个自编码，自解码的过程中，我们就既有了编码器和解码器。我们可以只对解码器输入向量，就能得到一个生成图。需要注意到的是，在这样一个自编码器模型中，code和image的映射完全是由模型生成的。模型会自动帮我们调优这个映射的划分。</p><p><font color="red">PS : </font>这个code的长度是一个超参数，我们需要设定一个适合的编码长度。一方面，它的长度小于原图才能达到我们希望的提取特征的效果。另一方面，如果它的长度太小，可能无法容纳图片中必须的一些关键信息，导致无法还原。</p><h3 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>自动编码器的优点很显然，只需要给模型展示一些正例，它就能学到编码与图片之间的映射，非常易于训练和生成。</p><p>上面我们说了，我们在训练一个自编码器的时候，希望的是输出图片和输入图片尽可能的相近，减少损失。这个相似如何去衡量呢？一般采用的方法都是把图片当作高维向量处理，求两个向量之间距离的L1或者L2范数。这个范数就表征了两张图片之间的差别，而这样的衡量方式局限性很明显：</p><p><img src="10.png" alt="简单范数的局限性"></p><p>如上图所示，如果我们让机器生成了四个版本的“2”的图片。其中v1和v2与原图只差了一个像素点，v3和v4查了足足6个像素点。但是v1和v2破绽很明显，v1在尾巴上出现了一个孤立的橘色块，而v2在中间留了一块白出来，这都是不自然的。而v3和v4虽然像素点差距更大，但是其实只是尾巴和头分别拉长了一点罢了，这都是很正常的情况。所以，用这种标准训练出来的模型，它显然缺少一个大局观。只是机械的分别关注全局对象的每一个小组成部分，很难学到组成部分之间的关联。</p><h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><h3 id="Basic-Idea-1"><a href="#Basic-Idea-1" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p><img src="11.png" alt="评估器"></p><p>评估器接收一个对象输入，产生一个标量输出。（其实就是一个回归问题）如上图所示，以生成图片的任务为例，一个评估器会评估机器生成的图片是否和人类作出来的图有一样的效果，根据生成质量，评估器会给出一个0~1的评分。</p><p>假设我们已经有了一个很好的评估器，我们也可以用拿它来生成图片。给定一个向量，我们通过一些事先构造的假设，将其可能对应的图片锁定在一个集合（可能会很大）中。图片生成就成为了在这个集合中枚举，筛选出一个得分最高的图片：</p><p><img src="12.png" alt="枚举筛选"></p><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>一个实际的问题是，如何训练一个好的评估器？它怎么能分辨出哪些是画得好的，哪些是画得糟糕的？</p><p><img src="13.png" alt></p><p><img src="14.png" alt></p><p>在网上，我们可以下载一大堆艺术家绘制的图片，他们都可以拿来作为正例，用来让模型学习什么是“好”图片。但是如果全都是正例，那么评估器将会收敛为一个恒输出1的常函数。所以，要训练一个评估器，面临的一个实际问题是：如何获得好的反例？</p><p>如果反例的采样分布没有选好，那么模型对反例的识别能力也会很差。就会像上图那样，一个画得很一般的图片却获得了0.9的高分。</p><p>我们可以通过一个迭代算法来不断强化这个评估器：</p><p><img src="15.png" alt="迭代优化"></p><p>如上图所示。一开始，我们可以随机生成一批反例，然后我们用当前的正例和反例训练出来一个评估器。评估器是个什么东西呢？它本质上也是一个定义在图片域上的函数。我们现在就从这个定义域上采集一批评分很高的图片来作为反例，用新的反例结合之前的正例重新训练，得到下一个版本的评估器。最后不断迭代。</p><p><img src="16.png" alt="迭代示意图"></p><p>上图是一个迭代过程的示意图，绿色为真实图片的分布域，蓝色为生成的效果糟糕的图片的分布域，红色曲线是我们的评估函数。在第一轮的训练中，真实域的函数估值会被拉高，我们随机生成那些噪音图片所在的域的股指会被压低。但是这个估值函数可能还不够，它可能没有发现其它很糟糕的图片的分布域，给了它们很高的估值，我们称为“虚高”。所以我们会进行第二轮训练，第二轮中用到的反例是从第一轮训练出来的估值函数中采样的估值高的点，那些“虚高”的点因为被作为反例被压低，而那些本来就该得分高的点即使被采样为反例，但是因为有同区域的正例在支撑，所以这一区域的估值不会被压低。最后，多轮迭代后，那些虚高的点就会逐渐被压平，只有正例分布的区域还依旧坚挺。此时我们就认为得到的评估函数是优秀的。</p><p>这就像是制定一部法律。我们希望人们的行为被约束在一个我们预先定义的可接受的范围内。但是法律有漏洞，总有人钻空子，在制定者期望之外的区域获得很高的收益。法律制定者就会锁定这些突出的领域专门指定新的条令来打击。经过不断的实践和修订，这部法律也就完善起来。</p><h3 id="Pros-and-Cons-1"><a href="#Pros-and-Cons-1" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>估值器的优点正好弥补了自动编码器的缺点。自动编码器是一个Button Up的模型，它根据编码对每一个像素点单独生成，组成一个完整的更高层的结果，这样的Button Up模型很难学到不同组成部分之间的关联。而估值器是一个Top Down模型，它直接获得正常图片作为输入，用全连接网络或者卷积网络很容易学习图片中的一些局部以及全局特征。</p><p>但是缺点也很明显。从上面看到，我们发现评估器很依赖于一个argmax的计算。如果要用它来生成模型，我们需要在一个域上找到得分最高的候选图。如果要用迭代法来训练一个评估器，我们也需要在当前的评估函数域采样估值高的点来作为新的反例。然而这个argmax怎么算呢？很多用discriminator来做生成的，都要事先做一些不准确的假设，根据这些假设对不同的输入向量在图片域中划分一个候选域，在这个可行域上做argmax。并且有的时候需要大量的枚举，非常耗时。并且要训练评估器也不容易，光有正例还不行，还得自己去合理的生成反例。</p><p>所以，显而易见，评估器虽然有了自编码器难以学得的全局观，但是却没了自编码器那种易于生成结果的特性。</p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>讨论了这么多，终于轮到主人公GAN登场了。</p><p>前面我们先讨论了Auto-encoder和Discriminator，分别介绍了用它们来做生成的基本想法。在讨论中，我们已经意识到了：Auto-encoder很容易产生生成图像，但是由于是button-up的模型，根据编码分别对每一个像素点预测，很难学得像素点之间的关系；Discriminator很容易学得图像中一些局部和全局的特征细节，可以很好的处理像素点之间的correlation，但是其主要能力还是在做评估上，在生成方面它还是缺乏一个有效的生成手段。</p><p>GAN集成了这两个方法：一方面，GAN使用一个类似Auto-encoder结构的Generator来生成图像，另一方面它用一个Discriminator来评估生成图片的质量。</p><h3 id="Basic-Idea-2"><a href="#Basic-Idea-2" class="headerlink" title="Basic Idea"></a>Basic Idea</h3><p>考虑这样的一个过程：</p><p>一开始我们已经有了一个generator和discriminator，它们的参数都是随机设置的。generator生成的图片很糟糕，discriminator也无法判别什么图片是好的。</p><p>然后我们可以以一个适当的概率分布随机向这个generator中输入一组向量，然后得到一堆生成的图片，用这些图片作为反例，用艺术家绘制的图片作为正例训练discriminator。这轮训练后，得到的discriminator的能力得到了提升，能够学会给一些好的图片打高分，给一些差的图片打低分。</p><p><img src="18.png" alt="固定Generator，强化Discriminator"></p><p>这之后，我们再固定这个discriminator的参数。此时如果我们给generator输入一个向量，再把它产生的图片送入discriminator中，我们会得到一个反馈的分数。这个反馈分数就可以作为LOSS，我们根据LOSS FUNCTION的梯度调整generator的参数，使得它尽可能产生可以骗过这个版本的discriminator，从它手下得到一个高分。这轮训练后，得到的generator的能力也得到了提升，能够产生一些像样的图片了。</p><p><img src="19.png" alt="固定Discriminator，强化Generator"></p><p>然后我们又重复上面的过程，强化discriminator，discriminator强化后再强化generator。。。可以期望的是，多轮迭代后，我们的generator和discriminator都可以变得很强。</p><p><img src="17.png" alt="GAN的哲学"></p><p>上图很好的从生物进化的角度很好的揭示了GAN的哲学。一开始蝴蝶颜色五颜六色的，停在树上的时候，捕食它的鸟根据颜色是否是棕色来区分它和叶子。在这个过程中，那些五颜六色的蝴蝶被淘汰了，棕色的蝴蝶脱颖而出，成功骗过了初代鸟。但是那些被骗过的初代鸟也会被淘汰，于是鸟也在这个过程中进化，学会了通过判别是否有叶脉来寻找猎物。而蝴蝶在这一过程中再次进化，成为了枯叶蝶。。。这是自然界中经典的良性竞争，互相强化的例子。而Generative Adversarial Network中的“Adversarial”也就是这样来的。在GAN中，我们让一个generator和一个discriminator互相对抗，generator努力的生成逼真的图片试图骗过discriminator，discriminator努力强化自己的辨别能力对抗generator的欺骗。模型的学习就是在两者的对抗之中互相强化而完成的。</p><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="20.png" alt="GAN"></p><p>上图是GAN的基本训练模式。$\theta_d$ 和 $\theta_g$ 分别是Discriminator和Generator的参数，一开始是随意初始化的。</p><p>每一轮中：</p><p>首先从数据库中采样m个真实样本${x_i}$，再让Generator随机生成m个虚假样本${\hat{x_i}}$。</p><p>定义 $V=\frac{1}{m}[;\Sigma_{i=1}^{m}logD(x_i)+\Sigma_{i=1}^{m}log(1-D(\hat{x}_i));]$</p><p>真实样本得分越高，虚假样本得分越低，这个V的值越大。于是我们只需要通过梯度上升法，最大化这个V，我们的Discriminator就能尽可能的对真实样本给出高分，尽可能的给虚假样本给出低分。这实际上就是完成了一个对Discriminator的强化。</p><p>在这之后，我们再取样m个随机向量${z_i}$，丢入Generator，再把生成的结果给Discriminator评估。</p><p>定义$V=\frac{1}{m}\Sigma_{i=1}^mlog(D(G(z_i)))$</p><p>同样的，我们希望最大化这个V值，使得产生的结果尽可能骗过Discriminator。将V值对 $\theta_g $ 求偏导，使用梯度上升来最大化这个V，从而实现对Generator的调优。</p><h3 id="Comparision"><a href="#Comparision" class="headerlink" title="Comparision"></a>Comparision</h3><p>在GAN的设计中，我们可以看到其明显优于Auto-encoder和Discriminator的地方。Auto-encoder生成之后，缺乏一个强大评估反馈，简单的采用原图和恢复生成图的差向量范数来评估，无法考虑到各个component之间的correlation，缺乏一个大局观。Discriminator虽然能对图片产生很深的特征理解，但是缺少一个高效准确的proposal手段，训练时它非常依赖于反例的生成，生成时又需要选择一个候选区域来执行argmax，这都不是容易解决的问题。</p><p>GAN将这两者的特性做了一个结合。虽然还是采用button-up的生成方式，根据向量对每一个像素点单独生成，但是多了一个up-down评估模式的discriminator来对生成内容进行更加细致的评估反馈，从而generator能得到更好的训练指引。另一方面，由于有了一个更加强大的generator，训练discriminator时，能获得质量更高的反例，从而discriminator也能得到更好的训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Reference :  &lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Machine Learning and having
      
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>2019.5.26 会议记录</title>
    <link href="https://zjusct.github.io/2019/05/27/MeetingReport20190526/"/>
    <id>https://zjusct.github.io/2019/05/27/MeetingReport20190526/</id>
    <published>2019-05-27T03:24:31.000Z</published>
    <updated>2019-05-31T15:04:27.471Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="输入密码以查看文章" />    <label for="pass">输入密码以查看文章</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      这是一篇加密文章，仅供超算队内部阅读。
    
    </summary>
    
    
      <category term="MeetingReport" scheme="https://zjusct.github.io/tags/MeetingReport/"/>
    
      <category term="ZJUSCT" scheme="https://zjusct.github.io/tags/ZJUSCT/"/>
    
  </entry>
  
  <entry>
    <title>基于0-1乘性噪声的朴素图片降噪</title>
    <link href="https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/"/>
    <id>https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/</id>
    <published>2019-05-10T16:00:00.000Z</published>
    <updated>2019-05-31T15:04:27.426Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h2><p>给定3张受损图像，尝试恢复他们的原始图像。</p><ol><li>原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。</li><li>受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。</li><li>噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。</li></ol><p><strong>评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。</strong></p><a id="more"></a><br><h2 id="实现介绍"><a href="#实现介绍" class="headerlink" title="实现介绍"></a>实现介绍</h2><br><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>由于图片的像素点在空间上满足局部相似的特征，相邻的像素点通道值变化往往是平滑且有一定规则的。因此，我们可以用一个模型来拟合像素点通道值在空间上的关系。具体实现中，我们将图片切割成若干个小矩形块，然后使用一个二维线性回归模型来回归每个小矩形块中位置和像素通道值的函数关系。为了使结果更加平滑和可靠，我们采用了高斯函数作为基函数。</p><br><h3 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h3><p>当我们对一个 ss * ss 的像素矩阵块做回归的时候，我们要把所有没有被噪音损坏的点都提取出来。点位置用一个高斯核函数处理，这样原来的点坐标(x,y)就被转换成了$(e^{-\frac{(x-mid)^2}{2}},e^{-\frac{(y-mid)^2}{2}})$，在特征空间中用来刻画这个点与矩阵块中心的距离。这样，我们实际要回归的就是点心距与像素通道值的关系。</p><p>这样做，主要是因为我们的局部性原理本身就是不带方向性的，所谓的局部性就是指临近的点存在某种平滑的变化关系。使用这样一个衡量距离的核函数，可以使得我们的回归结果更加平滑：</p><p><img src="1.png" alt="Non-Gaussian"><img src="2.png" alt="Gaussian"></p><p>上图左边是用坐标直接回归，右图是坐标经过高斯核处理后回归的结果。可以看到左边有大量的不平滑的交错的黑白点，看起来很“脏”。右边由于采用了高斯函数处理过的表征距离关系的核函数，结果更加平滑，清晰。</p><p>CODE : </p><p><img src="0.png" alt="CODE OF GAUSSIAN KERNEL"></p><br><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>提取完特征后的数据点拟合，就是一个简单的线性回归任务而已，我们采用最小二乘法回归。<br>$$<br>Loss = \frac {\Sigma_{i=0}^n(y_i-\phi(x_i) * w^T)^2}{n}=\frac{\Sigma_{i=1}^{n}Loss_i}{n}<br>$$<br>使用随机梯度下降来最优化损失函数：<br>$$<br>w\leftarrow w-\eta * \triangledown Loss_i=w+2\eta(y_i-\phi(x_i) * w^T) * \phi(x_i)<br>$$<br>具体实现中，我们取步长=0.005，进行100轮随机梯度下降。</p><p>CODE : </p><p><img src="3.png" alt="CODE OF TRAINING"></p><p>​    <br></p><h3 id="迭代降噪"><a href="#迭代降噪" class="headerlink" title="迭代降噪"></a>迭代降噪</h3><p>我们前面提到了，我们要把图片切成若干个小矩形块，对每个小矩形块分别进行回归，那么这个小矩形块的尺寸取多少比较合适呢？</p><p>我们先取ss=2尝试一下：</p><p><img src="4.png" alt="SS=2"></p><p>噪音为0.8的时候，我们可以看到，如果取一个2*2的块，期望其中没损坏的通道只有0.8个，所以势必有大量的矩阵块里面都是全损坏的，这会使得一些全损坏的块得不到修复，产生大量的黑点。</p><p>直接取ss=5：</p><p><img src="5.png" alt="SS=5"></p><p>显然，黑块的数量变少了，但是实际上图片给人的颗粒赶很明显，更像是一堆模糊的马赛克拼图拼凑而成的。</p><p>我们的解决办法是先取ss=2，对图片做恢复，然后将恢复的图像再用更大的ss来恢复。</p><p>下面是用ss={2，3，4，5}迭代恢复四次的过程：</p><p><img src="6.png" alt="SS-&gt;2"><img src="7.png" alt="SS-&gt;2-&gt;3">    </p><p><img src="8.png" alt="SS-&gt;2-&gt;3-&gt;4"><img src="9.png" alt="SS-&gt;2-&gt;3-&gt;4-&gt;5"></p><p>可以看到黑点逐渐被消除，并且最后经过ss=5的回归后得到的结果在视觉效果上明显优于直接用ss=5进行回归。这种想法本质上是一种<font color="red">贪心算法</font>。先将损失密度小的局部块恢复好，再充分利用之前修复出来的信息将损失密度更大的块修复。</p><br><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>A （noise rate = 0.8）</p><p><img src="10.png" alt="NR=0.8_原图"><img src="9.png" alt="NR=0.8_修复图"></p><p>B（noise rate = 0.4）</p><p><img src="11.png" alt="NR=0.4_原图"><img src="12.png" alt="NR=0.4_修复图"></p><p>C（noise rate = 0.6）</p><p><img src="13.png" alt="NR=0.6_原图"><img src="14.png" alt="NR=0.6_修复图"></p><p>D（根据原图自己生成，测试迭代过程中损失的减少）</p><p><img src="15.png" alt="误差测试"></p><br><h2 id="潜在的优化展望"><a href="#潜在的优化展望" class="headerlink" title="潜在的优化展望"></a>潜在的优化展望</h2><ol><li>由于每个块的修复是独立的，可以考虑使用CPU多线程计算或者在GPU上用CUDA进行并行优化，加速整个修复过程。</li><li>使用更复杂的网络来拟合。</li><li>使用马尔科夫随机场的方法来做图像降噪。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目内容&quot;&gt;&lt;a href=&quot;#项目内容&quot; class=&quot;headerlink&quot; title=&quot;项目内容&quot;&gt;&lt;/a&gt;项目内容&lt;/h2&gt;&lt;p&gt;给定3张受损图像，尝试恢复他们的原始图像。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。&lt;/li&gt;
&lt;li&gt;受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。&lt;/li&gt;
&lt;li&gt;噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Virtual Memory and TLB</title>
    <link href="https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/"/>
    <id>https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/</id>
    <published>2019-03-31T06:05:11.000Z</published>
    <updated>2019-05-31T15:04:27.472Z</updated>
    
    <content type="html"><![CDATA[<h2 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h2><p>x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址</p><ul><li>其中高2G<code>0x80000000 ~ 0xFFFFFFFF</code>为内和空间，由操作系统调用；</li><li>低2G<code>0x00000000 ~ 0x7FFFFFFF</code>为用户空间，由用户使用。</li></ul><p>在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。</p><a id="more"></a><p>虚拟地址空间是一种通过机制映射出来的空间，与实际物理空间大小无必然联系，在x86保护模式下，无论计算及实际主存是512MB还是8GB，虚拟地址空间总是4GB，<strong>这是由CPU和操作系统的宽度决定的</strong>，即：</p><blockquote><p>CPU地址总线宽度 → 物理地址范围<br>CPU的ALU宽度 → 操作系统位数 → 虚拟地址范围</p></blockquote><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>虚拟地址空间 = 主存 + 虚拟内存(交换空间 Swap Space)</p><p>虚拟内存：将硬盘的一部分作为存储器使用，来扩充物理内存。</p><p>利用了自动覆盖、交换技术。内存中存不下、暂时不用的内容会存在硬盘中。</p><blockquote><p>Assume: 32位操作系统，32位寻址总线宽度 → 4G线性空间</p></blockquote><h2 id="保护模式下的进程运行"><a href="#保护模式下的进程运行" class="headerlink" title="保护模式下的进程运行"></a>保护模式下的进程运行</h2><p>虚拟地址空间是硬件行为，CPU自动完成(同时与操作系统协作)虚拟地址到物理地址(可能差熬过实际内存，这样会产生一个异常中断，揭晓来有操作系统处理(如从虚拟内存中调出对应的页框内容))。</p><p>所以，一个程序若运行在保护模式下，其汇编级、机器语言级的寻址都是用的虚拟地址，即在一般的编程中不会接触到物理一层。</p><p>在进程被加载时，系统为进程建立唯一的数据结构<code>进程控制块(PCB = Process Control Block)</code>，直至进程结束。</p><p>PCB中描述了该进程的现状以及控制运行的全部信息，有了PCB，一个进程才可以在保护模式下和其他进程一起被并发地运行起来，操作系统通过PCB对进程进行控制。</p><p>PCB中的程序ID(PID(unix、linux)、句柄(windows))是进程的唯一标识；PCB中的一个指针指向 <strong>页表</strong> ，这些都与地址转化有关。</p><h2 id="地址转化"><a href="#地址转化" class="headerlink" title="地址转化"></a>地址转化</h2><p>地址转化的全过程可以用以下这张图来概括：</p><p><img src="OG.png" alt="OG"></p><p>以下是具体步骤介绍。</p><h3 id="1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留"><a href="#1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留" class="headerlink" title="1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)"></a>1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)</h3><ul><li><p>段内偏移地址(32位)</p></li><li><p>段选择符：16位长的序列，是索引值，定位段描述符；结构：<br><img src="%232.png" alt="#2"></p><ul><li>高13位为表内索引号 —— 但注意由于GDT第一项留空，所以索引要先加1；</li><li>而2位为TI表指示器，0是指GDT，1是指LDT；</li><li>0、1位是RPL请求者特权级，00最高，11最低 —— 在x86保护模式下修改寄存器是系统之灵，必须有对应的权限才能修改(当前执行权限和段寄存器中(被修改的)的RPL均不低于目标段的RPL)</li></ul></li><li><p>段描述符：8x8=64位长的结构，用来描述一个段的各种属性。结构：<br> <img src="%231.png" alt="#1"></p><ul><li>0、1字节+6字节低4位(20位) 段边界/段长度：最大1MB或者4G(看粒度位的单位)</li><li>2、3、4、7字节(32位) 段基址：4G线性地址的任意位置(不一定非要被16整除)</li><li>6、7字节的奇怪设计是为了兼容80286(24位地址总线)</li><li>剩下的那些是段属性，详见<code>20180819143434</code></li></ul></li><li><p>段描述表：多任务操作系统中，含有多个任务，而每个人物都有多个段，其段描述符存于段描述表中。<br>IA-32处理器有3个段描述表：GDT、LDT和IDT。</p><ul><li>GDT(Global Descripter Table) 全局段描述符表：一个系统一般只有一个GDT，含有每一个任务都可以访问的段；通常包含操作系统所使用的代码段、数据段和堆栈段，GDT同时包含各进程LDT数据段项，以及进程间通讯所需要的段。<br>GDTR是CPU提供的寄存器，存储GDT的位置和边界；在32位模式下RGDT有48位长(高32位基地址+低16位边界)，在32e模式下有80位长(高64位基地址+低16位边界)。<br>GDT的第一个表项留空不用，是空描述符，所以索引号要加1。<br>GDT最多128项。</li><li>LDT(Local Descripter Table) 局部段描述符表：16位长，属于某个进程。一个进程一个LDT，对应有RLDT寄存器，进程切换时RLDT改变。<br>RLDT和RGDT不一样，RLDT是一个索引值而不是实际指向，指向GDT中某一个LDT描述项。所以如果要获取LDT中的某一项，先要访问GDT找到对应LDT，再找到LDT中的一项。<br>编译程序时，程序内赋予了虚拟页号。在程序运行时，通过对应LDT转译成物理地址。故虚拟页号是局部性的、不同进程的页号会有冲突。<br>LDT没有空选择子。</li><li>IDT(Interrupt Descripter Table) 中断段描述符表；一个系统一般也只有一个。</li><li>以下这个图能做一点解释：<br><img src="%237.png" alt="#7"></li></ul></li></ul><h3 id="2-线性地址-→-物理地址-页式内存管理"><a href="#2-线性地址-→-物理地址-页式内存管理" class="headerlink" title="2. 线性地址 → 物理地址 (页式内存管理)"></a>2. 线性地址 → 物理地址 (页式内存管理)</h3><p>这一步由CPU的页式管理单元来负责转换。——MMU(内存管理单元)。</p><ul><li><p>线性地址可以拆分为三部分(或者两部分)：<br><img src="%233.png" alt="#3"></p></li><li><p>页(Page)：线性地址被划分为大小一致的若干内存区域，其对应映射到大小相同的与物理空间区域页框(Frame)上。这个映射不一定是连贯而有序的。</p></li><li><p>CR3：页目录基址寄存器。对于每一个进程，CR3的内容不同(有点像RLDT)，页目录基址也不同，线性地址-物理地址的映射也不同。</p></li><li><p>页目录：占用一个4kb的内存页，最多存储1024个页目录表项(PDE)，一个PDE有4字节。在没启用PAE时，有两种PDE，规格不同。</p></li><li><p>页目录表项(PDE)：每个程序有多个页表，即拥有多个PDE。PDE的结构如下：<br><img src="%234.png" alt="#4"><br>12~31位(20位)表示页表起始物理地址的高20位(页表基址低12位为0，即一定以4kb对齐)。</p></li><li><p>页表：一个页表占4kb的内存页，最多存储1024个页表项(PTE)，一个PTE是4字节。页表的基址是4kb对齐的，低12位是0。</p></li></ul><p>采用对页表项的二级管理模式(也目录→页表→页)能够节约空间。因为不存在的页表就可以不分配空间，并且对于Windows来说只有一级页表才会存在主存中，二级可以存在辅存中——不过Linux中它们都常驻主存。</p><p>一些CPU会提供更多级的架构，如三级、四级。Linux中，有对应的高层次抽象，提供了一个四层页管理架构：<br><img src="%236.png" alt="#6"><br>把中间的某几个定为长度为0，就可以调整架构级数。如“四化二”：某地址0x08147258，对应的PUD、PMD里只有一个表项为PUD→PMD，PMD→PT；划分的时候，PGD=0000100000，PUD=PMD=0，PT=0101000111.</p><h3 id="3-TLB-转换检测缓冲区、快表、转译后被缓冲区"><a href="#3-TLB-转换检测缓冲区、快表、转译后被缓冲区" class="headerlink" title="3. TLB (转换检测缓冲区、快表、转译后被缓冲区)"></a>3. TLB (转换检测缓冲区、快表、转译后被缓冲区)</h3><p>处理器中，一个具有并行朝赵能力的特殊高速缓存器，存储最近访问过的一些页表项(时空局部性原理，减少页映射的内存访问次数)。</p><p>TLB较贵，通常能够存放16~512个页表项。</p><ul><li><p>TLB命中：直接取出对应的页表项</p></li><li><p>TLB缺失：先淘汰TLB中的某一项(TLB替换策略，一些算法，可以由硬件或软件来实现)</p><ul><li>硬件处理TLB Miss：CPU会遍历页表，找到正确的PTE；如果没有找到，CPU就会发起一个页错误并将控制权交给操作系统。</li><li>软件处理TLB Miss：CPU直接发出未命中错误，让操作系统来处理。</li></ul></li><li><p>脏记录：当TLB中某个PTE项失效(如切换进程、进程退出、虚拟页换出到磁盘)，PTE标记为不存在，此时映射已经不成立了。<br>操作系统要保证即时刷新掉这些脏记录，不同的CPU有不同的刷新TLB方法，但每次都完全刷新TLB会很慢，所以现在有一些策略，扩展对一个PTE的描述(如针对某个进程、空间的标识，如果目前进程与PTE相关，就会忽略掉)，这样可以让多个进程同时共存TLB</p></li></ul><h2 id="Linux-段式管理"><a href="#Linux-段式管理" class="headerlink" title="Linux 段式管理"></a>Linux 段式管理</h2><p>Linux似乎没有理会Intel的那一套段的机制，而是做了一个高级的抽象。<br>Linux对所有的进程使用了相同的段来对指令和数据寻址，让每个段寄存器都指向同一个段描述符，让这个段描述符的基址为0，长度为4G。即用这种方式略去了段式内存管理。<br>对应多有用户代码段、用户数据段、内核代码段和内核数据段。可以在<code>segment.h</code>中看到，四种段对应的段基址都是0，这就是“平坦内存模型”，这样就有<code>段内偏移地址=逻辑地址</code></p><p>且，四种段对应的都为GDT。即Linux大多数情况都不使用LDT，除非使用wine等Windows防真程序。</p><p>Linux 0.11中每个进程划分64MB的虚拟内存空间。故逻辑地址范围为0~0x4000000</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;虚拟地址空间&quot;&gt;&lt;a href=&quot;#虚拟地址空间&quot; class=&quot;headerlink&quot; title=&quot;虚拟地址空间&quot;&gt;&lt;/a&gt;虚拟地址空间&lt;/h2&gt;&lt;p&gt;x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中高2G&lt;code&gt;0x80000000 ~ 0xFFFFFFFF&lt;/code&gt;为内和空间，由操作系统调用；&lt;/li&gt;
&lt;li&gt;低2G&lt;code&gt;0x00000000 ~ 0x7FFFFFFF&lt;/code&gt;为用户空间，由用户使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Operating System" scheme="https://zjusct.github.io/tags/Operating-System/"/>
    
  </entry>
  
  <entry>
    <title>如何在墙内快速部署CentOS 7的MySQL</title>
    <link href="https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/"/>
    <id>https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/</id>
    <published>2019-03-31T02:45:19.000Z</published>
    <updated>2019-05-31T15:04:27.470Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。</p><p>其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。</p><p>本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。</p><a id="more"></a><h2 id="传统方案……慢到怀疑人生"><a href="#传统方案……慢到怀疑人生" class="headerlink" title="传统方案……慢到怀疑人生"></a>传统方案……慢到怀疑人生</h2><p>根据<a href="https://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="noopener">官方指南</a>，我们执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源</span></span><br><span class="line">wget <span class="string">"https://dev.mysql.com/get/mysql80-community-release-el7-2.noarch.rpm"</span></span><br><span class="line"><span class="comment"># 安装源</span></span><br><span class="line">sudo rpm -ivh mysql80-community-release-el7-2.noarch.rpm</span><br><span class="line"><span class="comment"># 检查源是否成功安装</span></span><br><span class="line">sudo yum repolist enabled | grep <span class="string">"mysql80-community*"</span></span><br></pre></td></tr></table></figure><p>接下来就是正常的安装步骤：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install mysql-community-server mysql</span><br></pre></td></tr></table></figure><p>但是由于一些原因，下载速度基本是几Byte/s，MySQL 服务器的大小(加上依赖服务)差不多有600MB，这种方法基本不可取。手头没有特别好的而且很新的软件源，就打算手动安装。</p><h2 id="手动安装法"><a href="#手动安装法" class="headerlink" title="手动安装法"></a>手动安装法</h2><p>首先依然需要下载并安装官方源。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql-community-server</span><br></pre></td></tr></table></figure><p>利用该命令我们可以获取一些 MySQl Server 以来安装顺序及其版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">=================================================================================================</span><br><span class="line"> Package                       Arch          Version              Repository                Size</span><br><span class="line">=================================================================================================</span><br><span class="line">Reinstalling:</span><br><span class="line"> mysql-community-client        x86_64        8.0.15-1.el7         mysql80-community         25 M</span><br><span class="line"></span><br><span class="line"> mysql-community-libs          x86_64        8.0.15-1.el7         mysql80-community          2 M</span><br><span class="line"></span><br><span class="line"> mysql-community-common        x86_64        8.0.15-1.el7         mysql80-community        570 K</span><br><span class="line"></span><br><span class="line"> mysql-community-server        x86_64        8.0.15-1.el7         mysql80-community        360 M</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">=================================================================================================</span><br></pre></td></tr></table></figure><p>解压并分析rpm源包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm2cpio mysql80-community-release-el7-2.noarch.rpm | cpio -div</span><br><span class="line">vim /etc/yum.repos.d/mysql-community.repo</span><br></pre></td></tr></table></figure><p>从中我们可以找到对应版本的网络路径为<code>http://repo.mysql.com/yum/mysql-8.0-community/el/7/x86_64/</code>。</p><p>打开该地址，找到对应的几个安装包：</p><ul><li>mysql-community-client-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-libs-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-common-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-server-8.0.15-1.el7.x86_64.rpm</li></ul><p>使用某种下载工具(我使用的是迅雷)下载，然后使用<code>scp</code>指令上传到服务器上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp mysql-community-client-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-libs-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-common-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-server-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>按照先后顺序依次执行<code>yum</code>本地安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo yum localinstall mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">sudo yum -y install mysql</span><br></pre></td></tr></table></figure><p>安装成功，启动并测试服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br><span class="line">systemctl status mysqld.service</span><br></pre></td></tr></table></figure><p>找出默认密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep &quot;password&quot; /var/log/mysqld.log &gt;&gt; defalut_mysql_passwd.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。&lt;/p&gt;
&lt;p&gt;其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。&lt;/p&gt;
&lt;p&gt;本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="MySQL" scheme="https://zjusct.github.io/tags/MySQL/"/>
    
      <category term="CentOS" scheme="https://zjusct.github.io/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>BP算法</title>
    <link href="https://zjusct.github.io/2018/12/24/bp/"/>
    <id>https://zjusct.github.io/2018/12/24/bp/</id>
    <published>2018-12-24T13:03:59.000Z</published>
    <updated>2019-05-31T15:04:27.488Z</updated>
    
    <content type="html"><![CDATA[<p>本文探讨BP算法。</p><a id="more"></a><h1 id="Feed-forward-neural-network-and-back-propagation"><a href="#Feed-forward-neural-network-and-back-propagation" class="headerlink" title="Feed forward neural network and back propagation"></a>Feed forward neural network and back propagation</h1><h2 id="1-Neuron-structure"><a href="#1-Neuron-structure" class="headerlink" title="1. Neuron structure"></a>1. Neuron structure</h2><p><img src="neuron.png" alt="Neuron structure"></p><p>上图是一种典型的神经元结构，$x_n$是神经元的输入，将输入加权求和后再通过激活函数即可得到此神经元的输出：<br>$$t = \sum_{i=1}^{n}{w_ix_i} + b$$<br>$$a = f(t)$$</p><p>为计算方便，可将偏置$b$提到求和符号里面，相当于加入一个恒为1的输入值，对应的权重为$b$：<br>$$t = \sum_{i=0}^{n}{w_ix_i},(x_0 = 1, w_0 = b)$$<br>$$a = f(t)$$<br>此即为上图神经元结构对应的表达式</p><p>常用的激活函数有sigmoid, ReLU, tanh等。</p><h2 id="2-Network-structure"><a href="#2-Network-structure" class="headerlink" title="2. Network structure"></a>2. Network structure</h2><p><img src="bp_net.jpg" alt="Network Structure"></p><p>这是一个简单的3层网络，输入层有3个输入值，隐藏层包含3个隐藏神经元，最后是两个输出值<br>隐藏层神经元的前向计算过程：</p><p>$$z_i^{l} = \sum_{i=0}^{n}w_{ij}^{l}x_j, (x_0 = 1, w_0 = b)$$</p><p>$$a_i^l = f(z_i^l)$$</p><p>$l$表示第几层。</p><p>这个网络的抽象数学表达式为：<br>$$F(x) = f_3(f_2(x * W_2 + b_2) * W_3 + b_3)$$</p><p>事实上，深度神经网络一般都能够抽象为一个复合的非线性多元函数，有多少隐藏层就有多少层复合函数：<br>$$F(x) = f_n\left(\dots f_3(f_2(f_1(x) * w_1 + b_1) * w_2 + b_2)\dots\right)$$</p><h2 id="3-Loss"><a href="#3-Loss" class="headerlink" title="3. Loss"></a>3. Loss</h2><p>Loss，即损失，用来衡量神经网络的输出值与实际值的误差，对于不同的问题，通常会定义不同的loss函数</p><p>回归问题常用的均方误差：<br>$$MSE = \frac{1}{n}\sum_{i=1}^{n}(Y - f(x))^2$$<br>$Y$为实际值，$f(x)$为网络预测值</p><p>分类问题常用的交叉熵(m类)：<br>$$L = \sum_{k=1}^{n}\sum_{i=1}^{m}l_{ki}log(p_{ki})$$<br>$l_{ki}$表示第k个样本实际是否属于第i类（0，1编码），$p_{ki}$表示第k个样本属于第i类的概率值</p><p>特别地，二分类问题的交叉熵损失函数形式为：<br>$$L = \sum_{i=1}^{n}[y_ilog(p_i) + (1 - y_i)log(1 - p_i)]$$<br>$y_i$为第i个样本所属类别，$p_i$为第i个样本属于$y_i$类的概率</p><h2 id="4-Back-propagation"><a href="#4-Back-propagation" class="headerlink" title="4. Back propagation"></a>4. Back propagation</h2><p>BP 是用来将loss反向传播的算法，用来调整网络中神经元间连接的权重和偏置，整个训练的过程就是：前向计算网络输出<br>-&gt;;根据当前网络输出计算loss-&gt;BP算法反向传播loss调整网络参数，不断循环这样的三步直到loss达到最小或达到指定停止条件</p><p>BP算法的本质是求导的链式法则，对于上面的三层网络，假设其损失函数为$C$，激活函数为$\sigma$，第$l$第$i$个神经元的输入为$z_i^{(l)}$，输出为$a_i^{(l)}$</p><p>则通过梯度下降来更新权值和偏置的公式如下：<br>$$W_{ij}^{(l)} = W_{ij}^{(l)} - \eta\frac{\partial}{\partial W_{ij}^{(l)}}C\tag1$$<br>$$b_{i}^{(l)} = b_{i}^{(l)} - \eta\frac{\partial}{\partial b_{i}^{(l)}}C\tag2$$</p><p>$W_{ij}^{(l)}$表示第$l$层第$i$个神经元与第$l - 1$层第$j$个神经元连接的权值，$b_i^{(l)}$表示第$l$层第$i$个神经元的偏置</p><p>$\eta$表示学习率</p><p>由更新公式可见主要问题在于求解损失函数关于权值和偏置的偏导数</p><p>第$l$层第$i$个神经元的输入$z_i^{(l)}$为：<br>$$z_i^{(l)} = \sum_{j=1}^{n^{(l-1)}}{W_{ij}^{(l)}a_j^{(l-1)}} + b_i^{l}\tag3$$</p><p>则更新公式中偏导项可化为:</p><p>$$\frac{\partial}{\partial W_{ij}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial W_{ij}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}} \bullet a_i^{(l-1)}\tag4$$</p><p>$$\frac{\partial}{\partial b_{i}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial b_{i}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}}\tag5$$</p><p>定义</p><p>$$\delta_i^{(l)} = \frac{\partial}{\partial z_i^{(l)}}C\tag6$$</p><p>现在问题转化为求解$\delta_i^{(l)}$，对第$l$层第$j$个神经元有：<br>$$<br>\delta_j^{(l)} = \frac{\partial C}{\partial z_j^{(l)}} = \sum_{i=1}^{n^{(l+1)}}\frac{\partial C}{\partial z_i^{(l+1)}} \bullet \frac{\partial z_i^{(l+1)}}{\partial a_j^{(l)}} \bullet \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet \frac{\partial(W_{ij}^{l+1} + b_i^{(l+1)})}{\partial a_j^{(l)}} \bullet \sigma^\prime(z_j^{(l)})\<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet W_{ij}^{(l+1)} \bullet \sigma^\prime(z_j^{(l)})\tag7<br>$$</p><p>则：<br>$$\delta^{(l)} = ((W^{(l+1)})^T\delta^{(l+1)})\odot\sigma^\prime(z^{(l)})\tag8$$</p><p>损失函数关于权重和偏置的偏导分别为：<br>$$\frac{\partial C}{\partial W_{ij}^{(l)}} = a_i^{(l-1)}\delta_i^{(l)}\tag9$$<br>$$\frac{\partial C}{\partial b_{i}^{(l)}} =\delta_i^{(l)}\tag{10}$$</p><p>误差根据8式由输出层向后传播，再结合1，2，9，10四式对权重和偏置进行更新</p><h2 id="5-实现"><a href="#5-实现" class="headerlink" title="5.实现"></a>5.实现</h2><p>下面是一个简单3隐层神经网络的实现</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum((pred - y) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_prime</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pred - y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">network</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_layers, output_size, loss = loss, loss_prime = loss_prime)</span>:</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation function</span></span><br><span class="line">        self.activation = self.sigmoid</span><br><span class="line">        <span class="comment"># derivative of activation function</span></span><br><span class="line">        self.activation_prime = self.sigmoid_prime</span><br><span class="line">        <span class="comment"># loss funciton</span></span><br><span class="line">        self.loss = loss</span><br><span class="line">        <span class="comment"># derivative of loss function</span></span><br><span class="line">        self.loss_prime = loss_prime</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input-&gt;hidden</span></span><br><span class="line">        self.w_ih = np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.b_ih = np.random.randn(<span class="number">1</span>, hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        self.W_hh = [np.random.randn(hidden_size, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line">        self.B_hh = [np.random.randn(<span class="number">1</span>, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden-&gt;output</span></span><br><span class="line">        self.w_ho = np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.b_ho = np.random.randn(<span class="number">1</span>, output_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assemble w and b</span></span><br><span class="line">        self.W = [self.w_ih]</span><br><span class="line">        self.W.extend(self.W_hh)</span><br><span class="line">        self.W.append(self.w_ho)</span><br><span class="line"></span><br><span class="line">        self.B = [self.b_ih]</span><br><span class="line">        self.B.extend(self.B_hh)</span><br><span class="line">        self.B.append(self.b_ho)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># activation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(x) * (<span class="number">1</span> - self.sigmoid(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass, calculate the output of the network</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            a = self.activation(np.dot(a, w) + b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backpropagate error</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get output of each layer in forward pass</span></span><br><span class="line">        out = x</span><br><span class="line">        outs = []</span><br><span class="line">        zs = []</span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            z = np.dot(out, w) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            out = self.activation(z)</span><br><span class="line">            outs.append(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># δ of last layer</span></span><br><span class="line">        delta = self.loss_prime(outs[<span class="number">-1</span>], y) * self.activation_prime(zs[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        delta_b[<span class="number">-1</span>] = delta</span><br><span class="line">        delta_w[<span class="number">-1</span>] = np.dot(outs[<span class="number">-2</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(delta_w)):</span><br><span class="line">            delta = np.dot(delta, self.W[-i+<span class="number">1</span>].transpose()) * self.activation_prime(zs[-i])</span><br><span class="line">            delta_b[-i] = delta</span><br><span class="line">            delta_w[-i] = np.dot(outs[-i<span class="number">-1</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> delta_w, delta_b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update w and b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, batch, lr)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> batch:</span><br><span class="line">            d_w, d_b = self.backward(x, y)</span><br><span class="line">            delta_w = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_w, d_w)]</span><br><span class="line">            delta_b = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_b, d_b)]</span><br><span class="line"></span><br><span class="line">        self.W = [w - lr * t <span class="keyword">for</span> w, t <span class="keyword">in</span> zip(self.W, delta_w)]</span><br><span class="line">        self.B = [b - lr * t <span class="keyword">for</span> b, t <span class="keyword">in</span> zip(self.B, delta_b)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SGD training</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, epochs, batch_size, lr)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">            np.random.shuffle(train_data)</span><br><span class="line">            batches = [train_data[t : t + batch_size] <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, len(train_data), batch_size)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">                self.update(batch, lr)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> train_data:</span><br><span class="line">                loss += self.loss(self.forward(x), y)</span><br><span class="line">            loss /= len(train_data)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"Epoch %d done, loss: %f"</span> % (i + <span class="number">1</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.forward(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># use it for handwriting digits classification</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onehot</span><span class="params">(y)</span>:</span></span><br><span class="line">    arr = np.zeros([y.shape[<span class="number">0</span>], <span class="number">10</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        arr[i][y[i]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line">x_train = x_train.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">x_test = x_test.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">y_train = onehot(y_train)</span><br><span class="line">y_test = onehot(y_test)</span><br><span class="line"></span><br><span class="line">train_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_train, y_train)]</span><br><span class="line">test_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_test, y_test)]</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">3</span></span><br><span class="line">output_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">net = network(input_size, hidden_size, num_layers, output_size)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.005</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">net.train(train_data, epochs, batch_size, lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    exp = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> exp / np.sum(exp)</span><br><span class="line"></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> test_data:</span><br><span class="line">    ret = net.forward(x)</span><br><span class="line">    pred = softmax(ret)</span><br><span class="line">    <span class="keyword">if</span> np.argmax(pred) == np.argmax(y):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">acc = float(correct) / len(test_data)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, acc)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文探讨BP算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Machine Learning" scheme="https://zjusct.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Tensorflow</title>
    <link href="https://zjusct.github.io/2018/12/23/tensorflow/"/>
    <id>https://zjusct.github.io/2018/12/23/tensorflow/</id>
    <published>2018-12-23T11:57:25.000Z</published>
    <updated>2019-05-31T15:04:27.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-单机"><a href="#1-单机" class="headerlink" title="1.单机"></a>1.单机</h2><h3 id="log-device-placement"><a href="#log-device-placement" class="headerlink" title="log_device_placement"></a>log_device_placement</h3><p>单机情况比较简单，不需要特殊配置，TensorFlow会自动将计算任务分配到可用的GPU上，在定义session时，可以通过<em>log_device_placement</em>参数来打印具体的计算任务分配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="1.png"><h3 id="指定设备"><a href="#指定设备" class="headerlink" title="指定设备"></a>指定设备</h3><p>如果需要让一些运算在特定的设备上执行，可以使用tf.device:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="2.png"><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>尽管上面一个例子中我们只给CPU和GPU0指定了计算任务，但是两块显卡的显存都被占满了：</p><img src="3.png"><p>因为TensorFlow会默认占满所有可见GPU的显存，对于简单的计算任务，这样显然非常浪费，我们可以通过修改环境变量<em>CUDA_VISIBLE_DEVICES</em>解决这个问题:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 运行时指定环境变量</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python demo.py</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python 代码中修改环境变量</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>]=<span class="string">'0'</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="2-多机"><a href="#2-多机" class="headerlink" title="2.多机"></a>2.多机</h2><h3 id="In-graph-amp-Between-graph"><a href="#In-graph-amp-Between-graph" class="headerlink" title="In-graph &amp; Between-graph"></a>In-graph &amp; Between-graph</h3><p>TensorFlow的分布式训练有两种模式：In-graph和Between-graph</p><p>In-graph: 不同的机器执行计算图的不同部分，和单机多GPU模式类似，一个节点负责模型数据分发，其他节点等待接受任务，通过<em>tf.device(“/job:worker/task:n”)</em>来指定计算运行的节点</p><img src="5.png"><p>Between-graph:每台机器执行相同的计算图</p><blockquote><p>Author: 陈岩<br>PostDate: 2018.12.21</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-单机&quot;&gt;&lt;a href=&quot;#1-单机&quot; class=&quot;headerlink&quot; title=&quot;1.单机&quot;&gt;&lt;/a&gt;1.单机&lt;/h2&gt;&lt;h3 id=&quot;log-device-placement&quot;&gt;&lt;a href=&quot;#log-device-placement&quot; cla
      
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Tensorflow" scheme="https://zjusct.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Quick Guide to CUDA Profiling</title>
    <link href="https://zjusct.github.io/2018/12/07/cuprof/"/>
    <id>https://zjusct.github.io/2018/12/07/cuprof/</id>
    <published>2018-12-07T15:48:33.000Z</published>
    <updated>2019-05-31T15:04:27.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Brief-Introduction"><a href="#1-Brief-Introduction" class="headerlink" title="1. Brief Introduction"></a>1. Brief Introduction</h2><p>在并行计算领域，很难通过纯理论的分析来确定程序的性能，<code>GPGPU</code>这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。</p><p><code>Nvidia</code>提供了<code>nvprof</code>，<code>nvvp</code>，<code>Nsight</code>三种cuda可用的性能分析工具，本文将简述配合使用<code>nvprof</code>与<code>nvvp</code>的cuda程序性能分析方法。</p><a id="more"></a><h2 id="2-Check-Out-Device-Properties"><a href="#2-Check-Out-Device-Properties" class="headerlink" title="2. Check Out Device Properties"></a>2. Check Out Device Properties</h2><p>由于cuda程序的线程/块分配方案与程序运行的的硬件高度相关，故对目标平台的硬件参数有一定程度的了解是相当有必要的。我们可以使用<code>cudaGetDeviceProperties()</code>函数获取设备的各项属性，下述代码可以结合<code>cuda_runtime_api.h#1218</code>处<code>struct cudaDeviceProp</code>的定义和各属性的相应注解自行理解。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nDevices;</span><br><span class="line">cudaDeviceProp prop;</span><br><span class="line">cudaGetDeviceCount( &amp;nDevices );</span><br><span class="line"><span class="keyword">for</span> ( <span class="keyword">auto</span> i = <span class="number">0</span>; i != nDevices; ++i )</span><br><span class="line">&#123;</span><br><span class="line">cudaGetDeviceProperties( &amp;prop, i );</span><br><span class="line"><span class="comment">// check out interesting property</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-Profile-Using-Nvprof"><a href="#3-Profile-Using-Nvprof" class="headerlink" title="3. Profile Using Nvprof"></a>3. Profile Using Nvprof</h2><h3 id="3-1-Quick-Start"><a href="#3-1-Quick-Start" class="headerlink" title="3.1. Quick Start"></a>3.1. Quick Start</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="3-2-Metrics"><a href="#3-2-Metrics" class="headerlink" title="3.2. Metrics"></a>3.2. Metrics</h3><ul><li>使用<code>--query-metrics</code>列出所有可测试的性能指标。</li><li>使用<code>--metrics sm_efficiency,warp_execution_efficiency,...</code>指定要测试的性能指标。</li></ul><h3 id="3-3-PC-Sampling"><a href="#3-3-PC-Sampling" class="headerlink" title="3.3. PC Sampling"></a>3.3. PC Sampling</h3><p>在CC5.2或更高的设备上支持使用PC采样(PC sampling)技术。</p><p>PC采样技术通过<code>Round-Robin</code>方法对SM中所有活动线程束的PC状态进行采样，采样结果包含如下两种可能：</p><ul><li>线程束完成了当前指令。</li><li>线程束被<code>stall</code>，不能完成当前指令，并可以给出<code>stall</code>的原因。</li></ul><p>事实上线程束被<code>stall</code>并不代表指令流水线处于<code>stall</code>状态，因为其他正常运行的线程束可以利用计算资源。</p><p>CC6.0以上的设备对PC采样方法进行了改进，通过检查线程束调度器是否执行指令来确定指令流水线是否真正处于<code>stall</code>状态，从而能正确指示指令<code>stall</code>的原因。</p><h2 id="4-Data-Visualize-Using-Nvvp"><a href="#4-Data-Visualize-Using-Nvvp" class="headerlink" title="4. Data Visualize Using Nvvp"></a>4. Data Visualize Using Nvvp</h2><p><code>nvvp</code>可以导入<code>nvprof</code>的分析结果，可视化显示统计图表，并且建议性地指出程序可能存在的瓶颈。</p><p><em>以饼状图显示各类stall比重</em></p><img src="a.jpg"><p><em>以频谱显示各类指令比例</em></p><img src="b.jpg"><p><em>通过source file mapping可视化指令stall状态，需要在编译选项中指定<code>-lineinfo</code></em></p><img src="d.jpg"><h3 id="4-1-Usage"><a href="#4-1-Usage" class="headerlink" title="4.1. Usage"></a>4.1. Usage</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvprof -f --kernels <span class="string">"kernelName"</span> --analysis-metrics -o a.nvvp &lt;task&gt; &lt;args&gt;</span><br><span class="line">nvvp a.nvvp</span><br></pre></td></tr></table></figure><p>这里我使用的方法是在集群上用<code>nvprof</code>做性能测试，之后将分析结果<code>*.nvvp</code>传回本地用<code>nvvp</code>做可视化。</p><h2 id="Ext-Remarks"><a href="#Ext-Remarks" class="headerlink" title="Ext. Remarks"></a>Ext. Remarks</h2><h3 id="Tradeoff-Between-Registers-and-Threads"><a href="#Tradeoff-Between-Registers-and-Threads" class="headerlink" title="Tradeoff Between Registers and Threads"></a>Tradeoff Between Registers and Threads</h3><p>在实际Profiling中重新认识了这个问题。</p><p>在默认情况下，<code>nvcc</code>为每个线程分配<code>maxRegsPerThread</code>个数的寄存器，在<em>Tesla K40</em>上，这个值为64。同时，每个SM持有为65536个寄存器，这意味着单个SM中的线程数最多不超过1024。通过检查参数表，我们发现该设备单个SM可容纳线程数为2048。这意味着我们计算任务的GPU利用率最大只有50%（所有SM均满载的状态下）。</p><p>在这种情况下，如果我们将分配给单个线程的寄存器数目减半，则最大GPU利用率可以达到100%。但若发生寄存器溢出（register spilling），溢出的存储空间被放到片外的local memory，访问速度在（同在片外的）global memory级别。</p><p>在实际的CUDA核函数中，能全部利用64个寄存器的情况很少。寄存器的使用情况可以在nvvp中检查，如果发现有大量寄存器浪费，可以立即减少寄存器数量。在大多数情况下，可以结合计算任务的量级和性质来调节线程最大寄存器数，从而达到有针对性的性能调优。</p><p>在<code>nvcc</code>中指定单个线程最大寄存器数，可以添加编译选项<code>-maxrregcount=N</code>。如果限定不修改编译选项或需要逐核函数指定，则需要使用<code>__launch_bounds__</code>限定符，如下（隐式地指定了最大寄存器个数）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)</span><br><span class="line">MyKernel(...)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在我的<code>path tracer</code>中对上述方法进行测试，将每线程的寄存器数减半为32，SM线程数加倍并满载，GPU利用率由30+提升到70+，执行速度有1.5倍左右的提升。</p><h3 id="Tradeoff-Between-BlockDim-and-BlockPerSM"><a href="#Tradeoff-Between-BlockDim-and-BlockPerSM" class="headerlink" title="Tradeoff Between BlockDim and BlockPerSM"></a>Tradeoff Between BlockDim and BlockPerSM</h3><p>当一个块（block）中的所有线程束（warp）全部完成时，这个块才可以被SM调度。如果块的大小过大，则块的运行速度受单个线程束约束的开销就越大（如果算法并行度很高，增大块的大小不失为一个好选择）；如果块的大小过小，则一方面SM可能无法达到其最大利用率（受<code>maxBlocksPerSM</code>的限制），另一方面SM调度块的额外开销也会增大。尤其是针对不同特点的计算任务有不同的更优选择，如<code>divergency</code>较高的任务更适合较小的BlockDim。所以在选择BlockDim时不仅要在算法的适应性上做考虑，还要通过多次性能测试来进行针对性的优化。</p><h3 id="Beware-of-Ladder-Effects"><a href="#Beware-of-Ladder-Effects" class="headerlink" title="Beware of Ladder Effects"></a>Beware of Ladder Effects</h3><p>注意计算资源分配时要注意分配的资源量要能够被组别整除，否则会出现断层状的资源浪费现象。</p><p><em>每块线程数与SM中最大线程束数的关系</em></p><img src="c.jpg">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Brief-Introduction&quot;&gt;&lt;a href=&quot;#1-Brief-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Brief Introduction&quot;&gt;&lt;/a&gt;1. Brief Introduction&lt;/h2&gt;&lt;p&gt;在并行计算领域，很难通过纯理论的分析来确定程序的性能，&lt;code&gt;GPGPU&lt;/code&gt;这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Nvidia&lt;/code&gt;提供了&lt;code&gt;nvprof&lt;/code&gt;，&lt;code&gt;nvvp&lt;/code&gt;，&lt;code&gt;Nsight&lt;/code&gt;三种cuda可用的性能分析工具，本文将简述配合使用&lt;code&gt;nvprof&lt;/code&gt;与&lt;code&gt;nvvp&lt;/code&gt;的cuda程序性能分析方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
      <category term="Profile" scheme="https://zjusct.github.io/tags/Profile/"/>
    
  </entry>
  
  <entry>
    <title>CUDA内存管理总结(一)</title>
    <link href="https://zjusct.github.io/2018/11/25/cuda/"/>
    <id>https://zjusct.github.io/2018/11/25/cuda/</id>
    <published>2018-11-24T16:46:08.000Z</published>
    <updated>2019-05-31T15:04:27.493Z</updated>
    
    <content type="html"><![CDATA[<p>本文将探讨CUDA中的内存管理机制。</p><a id="more"></a><h2 id="一、寄存器"><a href="#一、寄存器" class="headerlink" title="一、寄存器"></a>一、寄存器</h2><p>​    GPU的每个SM（流多处理器）都有上千个寄存器，每个SM都可以看作是一个多线程的CPU核，但与一般的CPU拥有二、四、六或八个核不同，一个GPU可以有<strong>N个SM核</strong>；同样，与一般的CPU核支持一到两个硬件线程不同，每个SM核可能有<strong>8~192个SP</strong>（流处理器），亦即每个SM能同时支持这么多个硬件线程。事实上，一台GPU设备的所有SM中活跃的线程数目通常数以万计。</p><h3 id="1-1-寄存器映射方式"><a href="#1-1-寄存器映射方式" class="headerlink" title="1.1 寄存器映射方式"></a>1.1 寄存器映射方式</h3><p>​    <strong>CPU处理多线程</strong>：进行上下文切换，使用寄存器重命名机制，将当前所有寄存器的状态保存到栈（系统内存），再从栈中恢复当前需要执行的新线程在上一次的执行状态。这些操作通常花费上百个CPU时钟周期，有效工作吞吐量低。</p><p>​    <strong>GPU处理多线程</strong>：与CPU相反，GPU利用多线程隐藏了内存获取与指令执行带来的延迟；此外，GPU不再使用寄存器重命名机制，而是尽可能为每个线程分配寄存器，从而上下文切换就变成了寄存器组选择器（或指针）的更新，几乎是零开销。</p><h3 id="1-2-寄存器空间大小"><a href="#1-2-寄存器空间大小" class="headerlink" title="1.2 寄存器空间大小"></a>1.2 寄存器空间大小</h3><p>​    每个SM可提供的寄存器空间大小分别有8KB、16KB、32KB和64KB，每个线程中的每个变量占用一个寄存器，因而总共会占用N个寄存器，N代表调度的线程数量。当线程块上的寄存器数目是允许的最大值时，每个SM会只处理一个线程块。</p><h3 id="1-3-SM调度线程、线程块"><a href="#1-3-SM调度线程、线程块" class="headerlink" title="1.3 SM调度线程、线程块"></a>1.3 SM调度线程、线程块</h3><p>​    由于大多数内核对寄存器的需求量很低，所以可以通过降低寄存器的需求量来增加SM上线程块的调度数量，从而提高运行的线程总数，根据线程级并行<strong>“占用率越高，程序运行越快”</strong>，可以实现运行效率的优化。当线程级并行（<em>Thread-Level Parallelism</em>，TLP）足以隐藏存储延迟时会达到一个临界点，此后想要继续提高程序性能，可以在单个线程中实现指令级的并行（<em>Instruction-Level Parallelism</em>，ILP），即单线程处理多数据。</p><p>​    但在另一方面，每个SM所能调度的线程总量是有限制的，因此当线程总量达到最大时，再减少寄存器的使用量就无法达到提高占有率的目的（如下表中寄存器数目由20减小为16，线程块调度数量不变），所以在这种情况下，应增加寄存器的使用量到临界值。</p><img src="1.png"><h3 id="1-4-寄存器优化方式"><a href="#1-4-寄存器优化方式" class="headerlink" title="1.4 寄存器优化方式"></a>1.4 寄存器优化方式</h3><p>​    1）将中间结果累积在寄存器而非全局内存中。尽量避免全局内存的写操作，因为如果操作聚集到同一块内存上，就会强制硬件对内存的操作序列化，导致严重的性能降低；</p><p>​    2）循环展开。循环一般非常低效，因为它们会产生分支，造成流水线停滞。</p><h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h3><p>​    使用寄存器可以有效消除内存访问，或提供额外的ILP，以此实现GPU内核函数的加速，这是最为有效的方法之一。</p><h2 id="二、共享内存"><a href="#二、共享内存" class="headerlink" title="二、共享内存"></a>二、共享内存</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p>​    1、共享内存实际上是可以受用户控制的一级缓存，每个SM中的一级缓存和共享内存共用一个64KB的内存段。</p><p>​    2、共享内存的延迟很低，大约有1.5TB/s的带宽，而全局内存仅为160GB/s，换言之，有效利用共享内存有可能获得7倍的加速比。但它的速度依然只有寄存器的十分之一，并且共享内存的速度几乎在所有GPU中都相同，因为它由核时钟频率驱动。</p><p>​    3、只有当数据重复利用、全局内存合并，或者线程之间有共享数据（例如同时访问相同地址的存储体）的时候使用共享内存才更合适，否则将数据直接从全局内存加载到寄存器性能会更好。</p><p>​    4、共享内存是基于存储体切换的架构（<em>bank-switched architecture</em>），费米架构的设备上有32个存储体。无论有多少线程发起操作，<strong>每个</strong>存储体<strong>每个</strong>周期只执行<strong>一次</strong>操作。因此，如果线程束中的每个线程各访问一个存储体，那么所有线程的操作都可以在一个周期内同时执行，且所有操作都是独立互不影响的。此外，如果所有线程同时访问同一地址的存储体，会触发一个广播机制到线程束中的每个线程中。但是，如果是其他的访问方式，线程访问共享内存就需要排队，即一个线程访问时，其他线程将阻塞闲置。因此很重要的一点时，应该尽可能地获得<strong>零存储体冲突</strong>的共享内存访问。</p><h3 id="2-2-Example：使用共享内存排序"><a href="#2-2-Example：使用共享内存排序" class="headerlink" title="2.2 Example：使用共享内存排序"></a>2.2 Example：使用共享内存排序</h3><h2 id="2-2-1-归并排序"><a href="#2-2-1-归并排序" class="headerlink" title="2.2.1 归并排序"></a>2.2.1 归并排序</h2><p>​    假设待排序的数据集大小为N，现将数据集进行划分。根据归并排序的划分原则，最后每个数据包中只有两个数值需要排序，因此，在这一阶段，最大并行度可达到 $N \over 2$ 个独立线程。例如，处理一个大小为512KB的数据集，共有128K个32位的元素，那么最多可以使用的线程个数为64K个（N=128K，N/2=64K），假设GPU上有16个SM，每个SM最多支持1536个线程，那么每个GPU上最多可以支持24K个线程，因此，按照这样划分，64K的数据对只需要2.5次迭代即可完成排序操作。</p><p>​    但是，如果采用上述划分排序方式再进行合并，我们需要从每个排好序的数据集中读出元素，对于一个64K的集合，需要64K次读操作，即从内存中获取256MB的数据，显然当数据集很大的时候不合适。</p><p>​    因此，我们采用通过限制对原始问题的迭代次数，通过基于共享内存的分解方式来获得更好的合并方案。因为在费米架构的设备上有32个存储体，即对应32个线程，所以当需要的线程数量减少为32（一个线程束）时，停止迭代，于是共需要线程束4K个（128K/32=4K），又因为GPU上有16个SM，所以这将为每个SM分配到256个线程束。然而由于费米架构设备上的每个SM最多只能同时执行48个线程束，因此多个块将被循环访问。</p><p>​    通过将数据集以每行32个元素的方式在共享内存中进行分布，每列为一个存储体，即可得到零存储体冲突的内存访问，然后对每一列实施相同的排序算法。（或者也可以理解为桶排序呀）</p><p>​    然后再进行列表的合并。</p><h2 id="2-2-2-合并列表"><a href="#2-2-2-合并列表" class="headerlink" title="2.2.2 合并列表"></a>2.2.2 合并列表</h2><p>​    先从串行合并任意数目的有序列表看起：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, <span class="comment">//待排序数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 u32 *<span class="keyword">const</span> dest_array, <span class="comment">//排序后的数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_lists, <span class="comment">//列表总数</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_elements)</span> <span class="comment">//数据总数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);<span class="comment">//每个列表中的数据个数</span></span><br><span class="line">    u32 list_indexes[MAX_NUM_LISTS]; <span class="comment">//所有列表当前所在的元素下标</span></span><br><span class="line">    <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">    &#123;</span><br><span class="line">list_indexes[<span class="built_in">list</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">dest_array[i] = find_min(scr_array, </span><br><span class="line">                                 list_indexes, </span><br><span class="line">                                 num_lists, </span><br><span class="line">                                 num_elements_per_list);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">u32 <span class="title">find_min</span><span class="params">(<span class="keyword">const</span> u32*cosnt src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">             u32 *<span class="keyword">const</span> list_indexes, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_elements_per_list)</span><span class="comment">//寻找num_lists个元素中的最小值</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">    u32 min_idx = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_lists; i++)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[i] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line"><span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                min_idx = i;</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    list_indexes[min_idx]++;</span><br><span class="line">    <span class="keyword">return</span> min_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    将上述算法用GPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">gpu_sort_array_array</span><span class="params">(u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_elements)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 tid = (blockIdx.x * blockDim.x) + threadIdx.x;</span><br><span class="line">    __shared__ u32 sort_tmp[NUM_ELEM];</span><br><span class="line">    __shared__ u32 sort_tmp_1[NUM_ELEM];</span><br><span class="line">    </span><br><span class="line">    copy_data_to_shared(data, sort_tmp, num_lists, num_elements, tid);</span><br><span class="line">    radix_sort2(sort_tmp, num_lists, num_elements, tid, sort_tmp_1);</span><br><span class="line">    merge_array6(sort_tmp, data, num_lists, num_elements, tid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    第一个函数的实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">copy_data_to_shared</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    u32 *sort_tmp, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">sort_tmp[i+tid] = data[i+tid]; </span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    该函数中，程序按行将数据从全局内存读入共享内存。当函数调用一个子函数并传入参数时，这些参数必须以某种方式提供给被调用的函数，有两种方法可以采用。一种是通过寄存器传递所需的值，另一种方法是创建一个名为“栈帧”的内存区，但这种方法非常地不高效。出于这一原因，我们需要重新修改合并的程序(merge_array)，以避免函数调用，修改后程序如下（单线程）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array1</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    lists_indexes[tid] = <span class="number">0</span>;<span class="comment">//从每个列表的第一个元素开始</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//单线程</span></span><br><span class="line">    <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">        <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">        &#123;</span><br><span class="line">u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             u32 min_idx = <span class="number">0</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">      &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[<span class="built_in">list</span>] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line">   <span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                 min_idx = i;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          list_indexes[min_idx]++;</span><br><span class="line">          dest_array[i]=min_val;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    这里只用一个线程进行合并，但显然，为了获得更好的性能，一个线程是远远不够的。因为数据被写到一个单一的列表中，所以多个线程必须进行某种形式的合作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array6</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个列表分到的元素个数</span></span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建一个共享列表数组，用来储存当前线程所访问的列表元素下标</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建所有线程共享的最小值与最小值线程号</span></span><br><span class="line">    __shared__ u32 min_val;</span><br><span class="line">    __shared__ u32 min_tid;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        u32 data;</span><br><span class="line">        <span class="comment">//如果当前列表还未被读完，则从中读取数据</span></span><br><span class="line">        <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list);</span><br><span class="line">        &#123;</span><br><span class="line">             <span class="comment">//计算出当前元素在原数组中的下标</span></span><br><span class="line"><span class="keyword">const</span> u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//用零号线程来初始化最小值与最小值线程号</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//让所有线程都尝试将它们现在手上有的值写入min_val，但只有最小的数据会被保留</span></span><br><span class="line">        <span class="comment">//利用__syncthreads()确保每个线程都执行了该操作</span></span><br><span class="line">        atomicMin(&amp;min_val, data);</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在所有data==min_val的线程中，选取最小线程号写入min_tid</span></span><br><span class="line">        <span class="keyword">if</span>(min_val == data)</span><br><span class="line">        &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将满足要求的线程所在列表的当前元素往后移一位，进行下一轮比较</span></span><br><span class="line">        <span class="comment">//并将筛选结果存入结果数组dest_array</span></span><br><span class="line">        <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">             dest_array[i] = data;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    上面的函数中将num_lists个线程进行合并操作，但只用了一个线程一次将结果写入结果数据数组中，保证了结果的正确性，不会引起线程间的冲突。</p><p>​    其中使用到了 atomicMin 函数。每个线程以从列表中获取的数据作为入参调用该函数，取代了原先单线程访问列表中所有元素并找出最小值的操作。当每个线程调用 atomicMin 函数时，线程读取保存在共享内存中的最小值并于当前线程中的值进行比较，然后把比较结果重新写回最小值对应的共享内存中，同时更新最小值对应的线程号。然而，由于列表中的数据可能会重复，因此可能出现多个线程的值均为最小值的情况，保留的线程号却各不相同。因此需要执行第二步操作，保证保留的线程号为最小线程号。</p><p>​    虽然这种方法的优化效果很显著，但它也有一定的劣势。例如，atomicMin函数只能用在计算能力为1.2以上的设备上；另外，aotomicMin函数只支持整数型运算，但现实世界中的问题通常是基于浮点运算的，因此在这种情况下，我们需要寻找新的解决方法。</p><h2 id="2-2-3-并行归约"><a href="#2-2-3-并行归约" class="headerlink" title="2.2.3 并行归约"></a>2.2.3 并行归约</h2><p>​    并行归约适用于许多问题，求最小值只是其中的一种。它使用数据集元素数量一半的线程，每个线程将当前线程对应的元素与另一个元素进行比较，计算两者之间的最小值，并将得到的最小值移到前面。每进行一次比较，线程数减少一半，如此反复直到只剩一个元素为止，这个元素就是需要的最小值。</p><p>​    在选择比较元素的时候，应该尽量避免选择同一个线程束中的元素进行比较，因为这会明显地导致线程束内产生分支，而每个分支都将使SM做双倍的工作，继而影响程序的性能。因此我们选择将线程束中的元素与另一半数据集中的元素进行比较。如下图，阴影部分表示当前活跃的线程。</p><img src="2.png"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array5</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists,</span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_val[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_idx[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_val[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_idx[tid] = <span class="number">0</span>;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">u32 tid_max = num_lists &gt;&gt; <span class="number">1</span>;<span class="comment">//最大线程数为列表总数的一半</span></span><br><span class="line">         u32 data;<span class="comment">//使用寄存器可以提高运行效率，将对共享内存的写操作次数减少为1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当列表中还有未处理完的元素时</span></span><br><span class="line">         <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">         &#123;</span><br><span class="line">             <span class="comment">//计算该元素在原数组中的位置</span></span><br><span class="line">cosnst u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">         &#125;</span><br><span class="line">        <span class="comment">//若当前列表已经处理完，将data赋值最大</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前元素及线程号写入共享内存</span></span><br><span class="line">        reduction_val[tid] = data;</span><br><span class="line">        reduction_idx[tid] = tid;</span><br><span class="line">        __syncthreads;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当前活跃的线程数多于一个时</span></span><br><span class="line">        <span class="keyword">while</span>(tid_max!=<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(tid &lt; tid_max)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//将当前线程中的元素与另一半数据集中的对应元素进行比较</span></span><br><span class="line"><span class="keyword">const</span> u32 val2_idx = tid + tid_max;</span><br><span class="line">                 <span class="keyword">const</span> u32 val2 = reduction_val[val2_idx];</span><br><span class="line">                 </span><br><span class="line">                 <span class="comment">//最后保留较小的那个元素</span></span><br><span class="line">                 <span class="keyword">if</span>(reduction_val[tid] &gt; val2)</span><br><span class="line">                 &#123;</span><br><span class="line">reduction_val[tid] = val2;</span><br><span class="line">                      reduction_idx[tid] = reduction_idx[val_idx];</span><br><span class="line">                 &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//线程数减半，进入下一轮循环</span></span><br><span class="line">            tid_max &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在零号线程中将结果写入结果数组，并将相应线程所指的元素后移一位</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[reduction_idx[<span class="number">0</span>]]++;</span><br><span class="line">             dest_array[i] = reduction_val[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    同样，这种方法也在共享内存中创建了一个临时的列表 list_indexes 用来保存每次循环中从 num_list 个数据集列表中选取出来进行比较的数据。如果进行合并的列表已经为空，那么就将临时列表中的对应数据区赋最大值0xFFFFFFFF。而每轮while循环后，活跃的线程数都将减少一半，直到最后只剩一个活跃的线程，亦即零号线程。最后将结果复制到结果数组中并将最小值所对应的列表索引加一，以确保元素不会被处理两次。</p><h2 id="2-2-4-混合算法"><a href="#2-2-4-混合算法" class="headerlink" title="2.2.4 混合算法"></a>2.2.4 混合算法</h2><p>​    在了解atomicMin函数和并行归约两种方案后，我们可以利用这两种算法各自的优点，创造出一种新的混合方案。</p><p>​    简单的1~N个数据归约的一个主要问题就是当N增大时，程序的速度先变快再变慢，达到最高效的情形时N在8至16左右。混合算法将原数据集划分成诸多个小的数据集，分别寻找每块中的最小值，然后再将每块得到的结果最终归约到一个值中。这种方法和并行归约的思想非常相似，但同时又省略了并行归约中的多次迭代。代码更新如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE 8</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE_BIT_SHIFT 3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ACTIVE_REDUCTIONS ((MAX_NUM_LISTS) / (REDUCTION_SIZE))</span></span><br><span class="line"></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个线程都从原数组中读入一个数据，用作首次比较</span></span><br><span class="line">    u32 data = src_array[tid];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//当前线程所在的数据块编号（8个线程为一组，每个线程处理一个列表）</span></span><br><span class="line">    <span class="keyword">const</span> u32 s_idx = tid &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//首次进行分别归约的数据块总数</span></span><br><span class="line">    <span class="keyword">const</span> u32 num_reductions = num_lists &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    <span class="keyword">const</span> u32 num_elements_per_list = num_elements / num_lists;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在共享内存中创建一个列表，指向每个线程当前所在的元素，并初始化为0</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//遍历所有数据</span></span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//每个数据块在内部归约后都会产生一个相应的最小值</span></span><br><span class="line">        <span class="comment">//在共享内存中开辟一个列表，用来保存每组的最小值</span></span><br><span class="line">__shared__ u32 min_val[MAX_ACTIVE_REDUCTIONS];</span><br><span class="line">         __shared__ u32 min_tid;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//初始化每个数据块的内部最小值</span></span><br><span class="line">        <span class="keyword">if</span>(tid &lt; num_lists)</span><br><span class="line">        &#123;</span><br><span class="line">min_val[s_idx] = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前线程的数据与所处数据块的最小值进行比较，并保留较小的那一个</span></span><br><span class="line">        atomicMin(&amp;min_val[s_idx], data);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//进行归约的数据块总数不为零时</span></span><br><span class="line">        <span class="keyword">if</span>(num_reductions &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//确保每个线程都已经将上一步比较操作完成</span></span><br><span class="line">__syncthreads();</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//将每个数据块产生的最小值与零号数据块的最小值进行比较，保留较小的那一个</span></span><br><span class="line">             <span class="keyword">if</span>(tid &lt; num_reductions)</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_val[<span class="number">0</span>], min_val[tid]);</span><br><span class="line">                  __syncthreads();</span><br><span class="line">             &#125;</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//如果当前线程的数据等于此次比较保留的最小值，记录最小线程号</span></span><br><span class="line">             <span class="keyword">if</span>(data == min_val[<span class="number">0</span>])</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="comment">//确保上一步操作每个线程都已经完成，才能执行下一句</span></span><br><span class="line">             __syncthreads();</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//如果当前线程号恰为记录下的最小线程号</span></span><br><span class="line">            <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//当前所指元素后移一位</span></span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">                </span><br><span class="line">                 <span class="comment">//将结果保存入结果数组</span></span><br><span class="line">                  dest_array[i] = data;</span><br><span class="line">                 </span><br><span class="line">                  <span class="comment">//若该线程对应的列表尚未被处理完</span></span><br><span class="line">                  <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">                      <span class="comment">//更新该线程的data，进行下一轮比较</span></span><br><span class="line">                      data = src_array[tid + (list_indexes[tid] * num_lists)];</span><br><span class="line">                  <span class="keyword">else</span></span><br><span class="line">                      data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    注意到：</p><p>​    1）原来的min_val由单一的数据扩展成为一个共享数据的数组，这是因为每个独立的线程都需要从它对应的数据集中获取当前的最小值来进行内部比较。每个最小值都是一个32位的数值，因此可以存储在独立的共享内存存储体中。</p><p>​    2）内核函数中的REDUCTION_SIZE的值被设置成8，意味着每个数据块中包含8个数据，程序分别找出每个数据块的最小值，然后再在这些最小值中寻找最终的最小值。</p><p>​    3）内核函数中最重要的一个变化是，只有每次比较的最小值所对应的那个线程的data才会更新，其他线程的data都不会更新。而在之前的内核函数中，每轮比较开始，所有线程都会从对应的列表中重新读入data 的值，随着N的增大，这将变得越来越低效。</p><h2 id="2-2-5-总结"><a href="#2-2-5-总结" class="headerlink" title="2.2.5 总结"></a>2.2.5 总结</h2><p>​    1）共享内存允许同一个线程块中的线程读写同一段内存，但线程看不到也无法修改其他线程块的共享内存。</p><p>​    2）共享内存的缓冲区驻留在物理GPU上，所以访问时的延迟远低于访问普通缓冲区的延迟，因此除了使用寄存器，还应更有效地使用共享内存，尤其当数据有重复利用，或全局内存合并，或线程间有共享数据的时候。</p><p>​    3）编写代码时，将关键字_shared__添加到声明中，使得该变量留驻在共享内存中，并且线程块中的每个线程都可以共享这块内存，使得一个线程块中的多个线程能够在计算上进行通信和协作。</p><p>​    4）调用 __syncthreads() 函数来实现线程的同步操作，尤其要注意确保在读取共享内存之前，想要写入的操作都已经完成。另外还需要注意，切不可将这个函数放置在发散分支（某些线程需要执行，而其他线程不需要执行），因为除非线程块中的每个线程都执行了该函数，没有任何线程能够执行之后的指令，从而导致死锁。</p><p>​    5）不妨尝试使用共享内存实现矩阵乘法的优化。</p><blockquote><p>Author: 潘薇鸿<br>PostDate: 2018.11.25</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将探讨CUDA中的内存管理机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>浙江大学超算队博客...活了？</title>
    <link href="https://zjusct.github.io/2018/09/30/first/"/>
    <id>https://zjusct.github.io/2018/09/30/first/</id>
    <published>2018-09-30T13:25:55.000Z</published>
    <updated>2019-05-31T15:04:27.505Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>这里是浙江大学超算队的官方博客</p><a id="more"></a><p>记录一些前沿的Tech姿势、大家平时的DeBug经历、还有ASC世界超算大赛的经验分享！</p><p>希望这个博客不仅仅是给以后的小盆友们提供宝贵的经验 更是诸君技术的记录与提升的大好机会</p><p>平时我们写着代码 总是不愿意写文档 遇到了非常Tricky又神奇的Bug 花了一下午终于解决了 却没有记录下来</p><p>很有可能很多的无谓的时间就会在以后被重复 这样的时间浪费 我们完全可以节省下来！</p><p>希望大家能享受在超算队一起学习的时光~ 从现在开始 把这段美好的时间变成字符 记录下来叭~~</p><img src="ttfish.jpeg"><blockquote><p>Author: TTfish<br>PostDate: 2018.9.30</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;这里是浙江大学超算队的官方博客&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Spc" scheme="https://zjusct.github.io/tags/Spc/"/>
    
      <category term="ZJU" scheme="https://zjusct.github.io/tags/ZJU/"/>
    
  </entry>
  
</feed>
